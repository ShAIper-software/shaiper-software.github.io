{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"SHAIPER - AI for Surrogate Modelling"},{"location":"#about","title":"About","text":"<p>Welcome to the <code>SHAIPER - AI for Surrogate Modelling</code> community!</p> <p><code>SHAIPER</code> is currently being developed at the IT4Innovations National Supercomputing Center. Its purpose is to offer a comprehensive toolbox for experimentation with and the utilization of various models for surrogate modelling. The goal is to implement a state of the art software with respect to parallel scalability and efficiency while letting the researchers focus on more interesting aspects of their work, i.e. experiment with novel architectures, optimization methods etc.</p> <p><code>SHAIPER</code> is designed to offer comprehensive toolbox for:</p> <ul> <li>Efficient massivelly parallel dataset manipulation, analysis and information exports to standalone files.</li> <li>Modular AI models assembly.</li> <li>Hyperparameter space exploration for finding optimal model structures and learning behaviour.</li> <li>Model training monitoring and reporting.</li> <li>Feature optimizations based on gradient/genetic and mixed algorithms using the trained models.</li> </ul>"},{"location":"#current-capabilities","title":"Current Capabilities","text":"<p>In the current state, the software is usable for the following tasks:</p> <ul> <li>Tabular Database creation, augmentation, transformation, utilization in training and statistics calculation + report exporting.</li> <li>Modular model assembly and utilization in training and parameter optimization</li> <li>Arbitrary Optimizer and Scheduler utilization</li> <li>Basic Loss function utilization</li> <li>Training progress reporting</li> <li>Parameter search via first order gradient methods</li> </ul>"},{"location":"#current-status-of-the-development-with-respect-to-ecosystem-around-the-source-codes","title":"Current status of the development with respect to ecosystem around the source codes","text":""},{"location":"#data-management-design-intention-and-development-status","title":"Data management Design Intention and Development Status","text":""},{"location":"#training-workflow-and-development-status","title":"Training Workflow and Development Status","text":""},{"location":"#utilization-development-status","title":"Utilization Development Status","text":""},{"location":"#design-pillars-of-required-uix","title":"Design Pillars of required UIX","text":""},{"location":"#installation","title":"Installation","text":"<p>For the installation steps, refer to the Installation Tutorial.</p>"},{"location":"#how-to-run-the-software","title":"How to run the software","text":"<p>To learn about how to run &amp; utilize the software, refer to the Guide for Users.</p>"},{"location":"#want-to-participate-in-development","title":"Want to participate in development?","text":"<p>If you are a contributing developer, please refer to the Note for Developers.</p>"},{"location":"api/overview.hidden/","title":"API Overview","text":""},{"location":"api/overview.hidden/#AI4SurrogateModelling","title":"AI4SurrogateModelling","text":""},{"location":"api/API%20Reference/entry_point/","title":"Module <code>entry_point</code>","text":""},{"location":"api/API%20Reference/entry_point/#AI4SurrogateModelling.entry_point","title":"AI4SurrogateModelling.entry_point","text":"<p>A main entry point of the program. Parses the configuration file and serves as a hub for all functionalities.</p>"},{"location":"api/API%20Reference/entry_point/#AI4SurrogateModelling.entry_point.main","title":"main","text":"<pre><code>main()\n</code></pre> Source code in <code>AI4SurrogateModelling/entry_point.py</code> <pre><code>def main():\n    parser = argparse.ArgumentParser(\n        prog='ai4sm',\n        # description='...',\n        # epilog='Help!',\n    )\n\n    parser.add_argument('-c', '--config', default = 'run_config.yml')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.config):\n        raise RuntimeError(f'Configuration file \"{args.config}\"\\\n            does not exist.')\n    mpi.sync()\n\n    try:\n        with open(args.config, 'r') as f:\n            yaml = YAML(typ=\"safe\")\n            yaml.allow_duplicate_keys = False\n\n            config = yaml.load(f)\n        cfg = Config(**config)\n        if mpi.get_rank() == 0:\n            print(f\"\u2705 Main configuration: '{args.config}' is valid!\")\n        Logger.target_dir = config['main']['logging_directory']\n\n\n        if config['main']['logging'] == 'info':\n            main_info(cfg)\n        elif config['main']['logging'] == 'debug':\n            main_debug(cfg)\n\n    except ValidationError as e:\n        if mpi.get_rank() == 0:\n            print(f\"\u274c Main configuration: '{args.config}' validation failed\")\n            print(e)\n\n    except DuplicateKeyError as e:\n        if mpi.get_rank() == 0:\n            print(f\"\u274c Main configuration: '{args.config}' validation failed\")\n            print(e)\n</code></pre>"},{"location":"api/API%20Reference/entry_point/#AI4SurrogateModelling.entry_point.main_debug","title":"main_debug","text":"<pre><code>main_debug(config)\n</code></pre> Source code in <code>AI4SurrogateModelling/entry_point.py</code> <pre><code>@main_loop(logger_level = LoggerInfoLevel.DEBUG)\ndef main_debug(config):\n    main_function(config)\n</code></pre>"},{"location":"api/API%20Reference/entry_point/#AI4SurrogateModelling.entry_point.main_function","title":"main_function","text":"<pre><code>main_function(config: dict)\n</code></pre> <p>Takes the provided configuration dictionary and performs the specified  operations in order.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>a configuration dictionary</p> required Source code in <code>AI4SurrogateModelling/entry_point.py</code> <pre><code>def main_function(\n    config: dict\n):\n    \"\"\"Takes the provided configuration dictionary and performs the specified \n    operations in order.\n\n    Args:\n        config (dict): a configuration dictionary\n    \"\"\"\n\n    try:\n        seed = config.main.seed\n        Logger.info(f'Setting seed for random generators: {seed}')\n        torch.manual_seed(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.use_deterministic_algorithms(True)\n\n\n        config = Configuration(config)\n\n        for action_index in range(runtime_state.get_n_actions()):\n            runtime_state.perform_action(\n                action_idx = action_index,\n                reinit = False,\n            )\n    except Exception as e:\n        if mpi.get_rank() == 0:\n            print(\"\u274c Encountered an error! Check the logs.\")\n            print(f'\u274c Error message: {e}')\n        error_msg = traceback.format_exc()\n        msg = f'Encountered an exception: {e}\\nStack Trace:\\n{error_msg}'\n        Logger.warning(msg)\n        mpi.abort(0, '')\n        return\n</code></pre>"},{"location":"api/API%20Reference/entry_point/#AI4SurrogateModelling.entry_point.main_info","title":"main_info","text":"<pre><code>main_info(config)\n</code></pre> Source code in <code>AI4SurrogateModelling/entry_point.py</code> <pre><code>@main_loop(logger_level = LoggerInfoLevel.INFO)\ndef main_info(config):\n    main_function(config)\n</code></pre>"},{"location":"api/API%20Reference/src/aux/","title":"Module <code>src.aux</code>","text":""},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux","title":"AI4SurrogateModelling.src.aux","text":"<p>Module containing uncategorized auxilliary functions</p>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator","title":"ndarray_comparator","text":"<pre><code>ndarray_comparator(value: ndarray, eps: float)\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator--a-basic-comparator-for-1d-numpyndarray-objects-a-and-b","title":"A basic comparator for 1D numpy.ndarray objects A and B.","text":"<p>1) len(A) &lt; len(B) =&gt; A &lt; B 2) A[j] == B[j] and A[i] &lt; B[i] =&gt; A &lt; B for j &lt; i</p> <p>Flattens the input array and calculates relevant metrics for future comparison.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>input numpy.ndarray to be compared in the future</p> required <code>eps</code> <code>float</code> <p>tolerance parameter, two numbers a and b are            considered the same when |a - b| &lt;= eps</p> required Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __init__(self, value: np.ndarray, eps: float) -&gt; None:\n    \"\"\"Flattens the input array and calculates relevant metrics for future\n    comparison.\n\n    Args:\n        value (np.ndarray): input numpy.ndarray to be compared in the future\n        eps (float): tolerance parameter, two numbers a and b are\\\n        considered the same when |a - b| &lt;= eps\n    \"\"\"\n    self.value = value.flatten()\n    self.eps = eps\n    self.n = len(self.value)\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n = len(value)\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value = flatten()\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: ndarray_comparator) -&gt; bool\n</code></pre> <p>Implements the equal (==) operator.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ndarray_comparator</code> <p>Value to compare against</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when self == other, otherwise False</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __eq__(\n    self,\n    other: \"ndarray_comparator\",\n) -&gt; bool:\n    \"\"\"Implements the equal (==) operator.\n\n    Args:\n        other (ndarray_comparator): Value to compare against\n\n    Returns:\n        bool: True when self == other, otherwise False\n    \"\"\"\n    if self.n != other.n:\n        return False\n\n    for i in range(self.n):\n        if np.abs(self.value[i] - other.value[i]) &gt; self.eps:\n            return False\n\n    return True\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: ndarray_comparator) -&gt; bool\n</code></pre> <p>Implements the greater than or equal (&gt;=) operator</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ndarray_comparator</code> <p>Value to compare against</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when self &gt;= other, otherwise False</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __ge__(\n    self,\n    other: \"ndarray_comparator\",\n) -&gt; bool:\n    \"\"\"Implements the greater than or equal (&gt;=) operator\n\n    Args:\n        other (ndarray_comparator): Value to compare against\n\n    Returns:\n        bool: True when self &gt;= other, otherwise False\n    \"\"\"\n\n    if self &gt; other:\n        return True\n\n    if self == other:\n        return True\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: ndarray_comparator) -&gt; bool\n</code></pre> <p>Implements the greater than (&gt;) operator</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ndarray_comparator</code> <p>Value to compare against</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when self &gt; other, otherwise False</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __gt__(\n    self,\n    other: \"ndarray_comparator\",\n) -&gt; bool:\n    \"\"\"Implements the greater than (&gt;) operator\n\n    Args:\n        other (ndarray_comparator): Value to compare against\n\n    Returns:\n        bool: True when self &gt; other, otherwise False\n    \"\"\"\n\n    if self.n &gt; other.n:\n        return True\n\n    if self.n &lt; other.n:\n        return False\n\n    for i in range(self.n):\n        if self.value[-1 - i] &gt; self.eps + other.value[-1 - i]:\n            return True\n\n        if self.value[-1 - i] + self.eps &lt; other.value[-1 - i]:\n            return False\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__le__","title":"__le__","text":"<pre><code>__le__(other: ndarray_comparator) -&gt; bool\n</code></pre> <p>Implements the lower than or equal (&lt;=) operator</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ndarray_comparator</code> <p>Value to compare against</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when self &lt;= other, otherwise False</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __le__(\n    self,\n    other: \"ndarray_comparator\",\n) -&gt; bool:\n    \"\"\"Implements the lower than or equal (&lt;=) operator\n\n    Args:\n        other (ndarray_comparator): Value to compare against\n\n    Returns:\n        bool: True when self &lt;= other, otherwise False\n    \"\"\"\n\n    if self &lt; other:\n        return True\n\n    if self == other:\n        return True\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: ndarray_comparator) -&gt; bool\n</code></pre> <p>Implements the lower than (&lt;) operator</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ndarray_comparator</code> <p>Value to compare against</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when self &lt; other, otherwise False</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __lt__(\n    self,\n    other: \"ndarray_comparator\",\n) -&gt; bool:\n    \"\"\"Implements the lower than (&lt;) operator\n\n    Args:\n        other (ndarray_comparator): Value to compare against\n\n    Returns:\n        bool: True when self &lt; other, otherwise False\n    \"\"\"\n\n    if self.n &lt; other.n:\n        return True\n\n    if self.n &gt; other.n:\n        return False\n\n    for i in range(self.n):\n        if self.value[-1 - i] &gt; self.eps + other.value[-1 - i]:\n            return False\n\n        if self.value[-1 - i] + self.eps &lt; other.value[-1 - i]:\n            return True\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.ndarray_comparator.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns the string representation of this array.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns the native string representation of the flattened            numpy array</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string representation of this array.\n\n    Returns:\n        str: Returns the native string representation of the flattened\\\n        numpy array\n    \"\"\"\n\n    return f\"{self.value}\"\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.f_not_implemented","title":"f_not_implemented","text":"<pre><code>f_not_implemented(func) -&gt; Callable\n</code></pre> <p>A decorator to be used on functions which are not implemented yet. Useful when the codebase is large and you are unsure what functions are called and where. Aborts the program on all MPI processes and produces a call trace for debugging.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function to be decorated</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>the wrapped function</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def f_not_implemented(func) -&gt; Callable:\n    \"\"\"A decorator to be used on functions which are not implemented yet. Useful\n    when the codebase is large and you are unsure what functions are called and\n    where. Aborts the program on all MPI processes and produces a call trace for\n    debugging.\n\n    Args:\n        func (Callable): A function to be decorated\n\n    Returns:\n        Callable: the wrapped function\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper():\n        history = []\n        frame = inspect.currentframe().f_back\n        while frame is not None:\n            if (\n                \"/logging.py\" not in frame.f_code.co_filename\n                and \"/mpi.py\" not in frame.f_code.co_filename\n            ):\n                history.append((frame.f_code.co_filename, frame.f_lineno))\n            frame = frame.f_back\n        Logger.warning(\n            f\"Function: {func.__name__} is not implemented yet, call history:\"\n        )\n        for history_entry in history:\n            Logger.warning(f\"{history_entry}\")\n\n        MPI.COMM_WORLD.Abort(0)\n\n    return wrapper\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.f_should_remove","title":"f_should_remove","text":"<pre><code>f_should_remove(func: Callable) -&gt; Callable\n</code></pre> <p>A decorator to be used on functions which are planned to be removed or made obsolete.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>the wrapped function</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def f_should_remove(func: Callable) -&gt; Callable:\n    \"\"\"A decorator to be used on functions which are planned to be removed or\n    made obsolete.\n\n    Args:\n        func (Callable): A function to be decorated.\n\n    Returns:\n        Callable: the wrapped function\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper():\n        Logger.warning(f\"Function: {func.__name__} should be removed\")\n\n    return wrapper\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.get_index_eq_min","title":"get_index_eq_min","text":"<pre><code>get_index_eq_min(\n    u: ndarray_comparator, V: ndarray, V_ordering: list[int]\n) -&gt; int\n</code></pre> <p>Provided the comparator u and the associated array, find and retrieve the highest row index i such that V[i] &gt; u.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray_comparator</code> <p>referential row to compare against</p> required <code>V</code> <code>ndarray</code> <p>row space in which to look for the rows</p> required <code>V_ordering</code> <code>list[int]</code> <p>An ordering of rows in V such that        V[V_ordering[i]] &lt;= V[V_ordering[i + 1]]</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the highest row index i such that V[i] &gt; u</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def get_index_eq_min(\n    u: ndarray_comparator,\n    V: np.ndarray,\n    V_ordering: list[int],\n) -&gt; int:\n    \"\"\"Provided the comparator u and the associated array, find and retrieve the\n    highest row index i such that V[i] &gt; u.\n\n    Args:\n        u (ndarray_comparator): referential row to compare against\n        V (np.ndarray): row space in which to look for the rows\n        V_ordering (list[int]): An ordering of rows in V such that\\\n        V[V_ordering[i]] &lt;= V[V_ordering[i + 1]]\n\n    Returns:\n        int: the highest row index i such that V[i] &gt; u\n    \"\"\"\n\n    v_idx_start = 0\n    v_idx_end = len(V_ordering) - 1\n    while v_idx_end - v_idx_start &gt; 0:\n        v_idx_mid = (v_idx_end + v_idx_start) // 2\n        # Logger.warning(f'get_index_eq_min: {v_idx_start} - {v_idx_end}')\n\n        Vpointer_ordered = V_ordering[v_idx_mid]\n        v = ndarray_comparator(V[Vpointer_ordered], u.eps)\n        if v &lt; u:\n            v_idx_start = v_idx_mid + 1\n        else:\n            v_idx_end = v_idx_mid\n\n    return v_idx_start\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.get_maximal_matching","title":"get_maximal_matching","text":"<pre><code>get_maximal_matching(\n    graph_edges: set[tuple[int, int]],\n) -&gt; set[tuple[int, int]]\n</code></pre> <p>A greedy algorithm finding a maximal matching from the provided edges.</p> <p>Parameters:</p> Name Type Description Default <code>graph_edges</code> <code>set[tuple[int, int]]</code> <p>Each entry is a tuple (u, v),        meaning that vertex u is connected to vertex v</p> required <p>Returns:</p> Type Description <code>set[tuple[int, int]]</code> <p>set[tuple[int, int]]: Subset of the input graph_edges such that adding        any edge from: graph_edges - output will connect to an edge already in        the output.</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def get_maximal_matching(\n    graph_edges: set[tuple[int, int]],\n) -&gt; set[tuple[int, int]]:\n    \"\"\"A greedy algorithm finding a maximal matching from the provided edges.\n\n    Args:\n        graph_edges (set[tuple[int, int]]): Each entry is a tuple (u, v),\\\n        meaning that vertex u is connected to vertex v\n\n    Returns:\n        set[tuple[int, int]]: Subset of the input graph_edges such that adding\\\n        any edge from: graph_edges - output will connect to an edge already in\\\n        the output.\n    \"\"\"\n\n    output = set()\n    vertices_in_output = set()\n\n    for edge in graph_edges:\n        u = edge[0]\n        v = edge[1]\n\n        if u not in vertices_in_output and v not in vertices_in_output:\n            output.add(edge)\n            vertices_in_output.add(u)\n            vertices_in_output.add(v)\n\n    return output\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.get_similar_row_pairs","title":"get_similar_row_pairs","text":"<pre><code>get_similar_row_pairs(\n    U: ndarray,\n    U_ordering: list[int],\n    U_indices: list[int],\n    V: ndarray,\n    V_ordering: list[int],\n    V_indices: list[int],\n    eps: float,\n) -&gt; set[tuple[int, int]]\n</code></pre> <p>Takes two arrays U with shape (nU, k) and V with shape (nV, k) and finds rows of U which are also in V given the provided tolerance eps &gt; 0. Outputs the global row indices of corresponding pairs.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>First array containing the first set of rows.</p> required <code>V</code> <code>ndarray</code> <p>Second array containing the second set of rows.</p> required <code>U_ordering</code> <code>list[int]</code> <p>An ordering of rows in U such that        U[U_ordering[i]] &lt;= U[U_ordering[i + 1]]</p> required <code>U_indices</code> <code>list[int]</code> <p>The array U is possibly distributed among        multiple MPI processes. This list keeps track of the global row        indices for array U.</p> required <code>V_ordering</code> <code>list[int]</code> <p>An ordering of rows in V such that        V[V_ordering[i]] &lt;= V[V_ordering[i + 1]]</p> required <code>V_indices</code> <code>list[int]</code> <p>The array V is possibly distributed among        multiple MPI processes. This list keeps track of the global row        indices for array V.</p> required <code>eps</code> <code>float</code> <p>tolerance parameter to be used in the comparator</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set[tuple[int, int]]</code> <p>Outputs a set of corresponding pairs, i.e. each (u, v) in the        output means that U[u] == V[v]. Outputs an empty set when no matches        are found.</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def get_similar_row_pairs(\n    U: np.ndarray,\n    U_ordering: list[int],\n    U_indices: list[int],\n    V: np.ndarray,\n    V_ordering: list[int],\n    V_indices: list[int],\n    eps: float,\n) -&gt; set[tuple[int, int]]:\n    \"\"\"Takes two arrays U with shape (nU, k) and V with shape (nV, k) and finds\n    rows of U which are also in V given the provided tolerance eps &gt; 0. Outputs\n    the global row indices of corresponding pairs.\n\n    Args:\n        U (np.ndarray): First array containing the first set of rows.\n        V (np.ndarray): Second array containing the second set of rows.\n        U_ordering (list[int]): An ordering of rows in U such that\\\n        U[U_ordering[i]] &lt;= U[U_ordering[i + 1]]\n        U_indices (list[int]): The array U is possibly distributed among\\\n        multiple MPI processes. This list keeps track of the global row\\\n        indices for array U.\n        V_ordering (list[int]): An ordering of rows in V such that\\\n        V[V_ordering[i]] &lt;= V[V_ordering[i + 1]]\n        V_indices (list[int]): The array V is possibly distributed among\\\n        multiple MPI processes. This list keeps track of the global row\\\n        indices for array V.\n        eps (float): tolerance parameter to be used in the comparator\n\n    Returns:\n        set: Outputs a set of corresponding pairs, i.e. each (u, v) in the\\\n        output means that U[u] == V[v]. Outputs an empty set when no matches\\\n        are found.\n    \"\"\"\n\n    out = set()\n\n    nV = len(V_indices)\n\n    for Upointer in range(len(U_indices)):\n        Upointer_ordered = U_ordering[Upointer]\n        Uglobal_idx = U_indices[Upointer_ordered]\n\n        u = ndarray_comparator(U[Upointer_ordered], eps)\n\n        # first determine whether it makes sense to look\n        v0 = ndarray_comparator(V[V_ordering[0]], eps)\n        v1 = ndarray_comparator(V[V_ordering[-1]], eps)\n        has_solution = u &lt;= v1 and u &gt;= v0\n        if not has_solution:\n            continue\n\n        # look for the lowest element in set V greater than u to speed-up the\n        # search\n        v_from_idx = get_index_eq_min(u, V, V_ordering)\n\n        for Vpointer in range(v_from_idx, nV):\n            Vpointer_ordered = V_ordering[Vpointer]\n            Vglobal_idx = V_indices[Vpointer_ordered]\n\n            # when global indices are the same, the rows are trivially equal,\n            # we are not interested in such cases\n            if Vglobal_idx == Uglobal_idx:\n                continue\n            if u == ndarray_comparator(V[Vpointer_ordered], eps):\n                min_val = min(Uglobal_idx, Vglobal_idx)\n                max_val = max(Uglobal_idx, Vglobal_idx)\n                out.add((min_val, max_val))\n            else:\n                break\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.get_simplex_distance","title":"get_simplex_distance","text":"<pre><code>get_simplex_distance(\n    *,\n    q: ndarray,\n    points: ndarray,\n    simplices: list[tuple[int, ...]]\n) -&gt; float\n</code></pre> <p>Calculates the shortest distance from point q to any point lying in the provided simplex set.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>ndarray</code> <p>Coordinates of the point to calculate for.</p> required <code>points</code> <code>ndarray</code> <p>Coordinates of the simplex vertices</p> required <code>simplices</code> <code>list[tuple[int, ...]]</code> <p>Simplex adjacency</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>min ||q - x|| over x, where x is any point in the provided        simplex set.</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def get_simplex_distance(\n    *,\n    q: np.ndarray,\n    points: np.ndarray,\n    simplices: list[tuple[int, ...]],\n) -&gt; float:\n    \"\"\"Calculates the shortest distance from point q to any point lying in the\n    provided simplex set.\n\n    Args:\n        q (np.ndarray): Coordinates of the point to calculate for.\n        points (np.ndarray): Coordinates of the simplex vertices\n        simplices (list[tuple[int, ...]]): Simplex adjacency\n\n    Returns:\n        float: min ||q - x|| over x, where x is any point in the provided\\\n        simplex set.\n    \"\"\"\n    qw_distance = None\n    for S in simplices:\n\n        # for each simplex S, find w in S such that ||q-w||=min||q-x||\n        # over x from S\n\n        # the algorithm used is a genral purpose least square solution\n        # applicable to simplices of any dimension\n\n        u = S[0]\n        b = (q - points[u]).reshape(-1, 1)\n        A = []\n        for i in range(1, len(S)):\n            v = S[i]\n            A.append((points[v] - points[u]).reshape(-1, 1))\n\n            u = v\n\n        A = np.concatenate(A, axis=1)\n        (sol, res, rank, singular_values) = np.linalg.lstsq(A, b, rcond=None)\n\n        sol = sol.flatten()\n        for i in range(len(sol)):\n            if sol[i] &lt; 0:\n                sol[i] = 0\n\n            if sol[i] &gt; 1:\n                sol[i] = 1\n\n        u = S[0]\n        w = points[u].copy()\n        for i in range(len(sol)):\n            v = S[i + 1]\n            w += sol[i] * (points[v] - points[u])\n            u = v\n\n        if qw_distance is None:\n            qw_distance = np.linalg.norm(q - w)\n        else:\n            qw_distance = min(qw_distance, np.linalg.norm(q - w))\n\n    return qw_distance\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.lies_near_simplex_set","title":"lies_near_simplex_set","text":"<pre><code>lies_near_simplex_set(\n    q: ndarray,\n    points: ndarray,\n    simplices: list[tuple[int, int]],\n    tol: float,\n) -&gt; bool\n</code></pre> <p>Determines whether the provided point q lies near any of the provided simplices.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>ndarray</code> <p>The point for which to determine whether it lies near a        set of simplices of not.</p> required <code>points</code> <code>ndarray</code> <p>Coordinates of the simplices to test against</p> required <code>simplices</code> <code>list[tuple[int, ...]]</code> <p>Vertex adjacency of the simplices.</p> required <code>tol</code> <code>float</code> <p>Measure of what 'near' and 'not near' mean.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if q is at most tol distance (Euclidean) from any point in        the provided simplices. False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def lies_near_simplex_set(\n    q: np.ndarray,\n    points: np.ndarray,\n    simplices: list[tuple[int, int]],\n    tol: float,\n) -&gt; bool:\n    \"\"\"Determines whether the provided point q lies near any of the provided\n    simplices.\n\n    Args:\n        q (np.ndarray): The point for which to determine whether it lies near a\\\n        set of simplices of not.\n        points (np.ndarray): Coordinates of the simplices to test against\n        simplices (list[tuple[int, ...]]): Vertex adjacency of the simplices.\n        tol (float): Measure of what 'near' and 'not near' mean.\n\n    Returns:\n        bool: True if q is at most tol distance (Euclidean) from any point in\\\n        the provided simplices. False otherwise.\n    \"\"\"\n    if (\n        get_simplex_distance(\n            q=q,\n            points=points,\n            simplices=simplices,\n        )\n        &lt; tol\n    ):\n        return True\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.permute_columns","title":"permute_columns","text":"<pre><code>permute_columns(\n    data: list[ndarray],\n    keys_current: list[str],\n    keys_new: list[str],\n) -&gt; list[ndarray]\n</code></pre> <p>Given the list of numpy arrays data and its column labeling keys_new, find and apply a column permutation such that the data and its column labeling corresponds to the labeling keys_current.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[ndarray]</code> <p>List of arrays to permute</p> required <code>keys_current</code> <code>list[str]</code> <p>Target column ordering</p> required <code>keys_new</code> <code>list[str]</code> <p>Current column ordering</p> required <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: a list of elemenets in data (in the same order) with        permuted columns.</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def permute_columns(\n    data: list[np.ndarray],\n    keys_current: list[str],\n    keys_new: list[str],\n) -&gt; list[np.ndarray]:\n    \"\"\"Given the list of numpy arrays data and its column labeling keys_new,\n    find and apply a column permutation such that the data and its column\n    labeling corresponds to the labeling keys_current.\n\n    Args:\n        data (list[np.ndarray]): List of arrays to permute\n        keys_current (list[str]): Target column ordering\n        keys_new (list[str]): Current column ordering\n\n    Returns:\n        list[np.ndarray]: a list of elemenets in data (in the same order) with\\\n        permuted columns.\n    \"\"\"\n    keys_indexing_new = {}\n    incorrect_labels = []\n    for i, key_new in enumerate(keys_new):\n        keys_indexing_new[key_new] = i\n\n        if key_new not in keys_current:\n            incorrect_labels.append(key_new)\n\n    current2new_key_map = []\n    for key_current in keys_current:\n        if key_current in keys_indexing_new:\n            current2new_key_map.append(keys_indexing_new[key_current])\n\n    if len(current2new_key_map) != len(keys_new):\n        msg = f\"Incorrect column labeling encountered!. Incorrect labels: {incorrect_labels}.\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    data_new = [np.array([D[i] for i in current2new_key_map]) for D in data]\n\n    return data_new\n</code></pre>"},{"location":"api/API%20Reference/src/aux/#AI4SurrogateModelling.src.aux.transform_function","title":"transform_function","text":"<pre><code>transform_function(func) -&gt; Callable\n</code></pre> <p>A decorator to be used around functions producing data transformations. The decorator checks whether the input contains the required argument and whether the output contains the required arguments.</p> <p>Checks for missing arguments. In case of a failure aborts the program on all MPI processes. Checks and informs the user if any error in input/output definition is present.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function to be decorated</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>the wrapped function</p> Source code in <code>AI4SurrogateModelling/src/aux.py</code> <pre><code>def transform_function(\n    func,\n) -&gt; Callable:\n    \"\"\"A decorator to be used around functions producing data transformations.\n    The decorator checks whether the input contains the required argument and\n    whether the output contains the required arguments.\n\n    Checks for missing arguments. In case of a failure aborts the program on\n    all MPI processes.\n    Checks and informs the user if any error in input/output definition is\n    present.\n\n    Args:\n        func (Callable): A function to be decorated\n\n    Returns:\n        Callable: the wrapped function\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if \"data\" not in kwargs:\n            Logger.warning(\n                f\"Function: {func.__name__} is missing a required\\\n                argument: data\"\n            )\n            MPI.COMM_WORLD.Abort(0)\n\n        result = func(*args, **kwargs)\n\n        if not isinstance(result, dict):\n            Logger.warning(\n                f\"Function: {func.__name__} has incorrect output type: requires\\\n                a dictionary\"\n            )\n            MPI.COMM_WORLD.Abort(0)\n\n        if \"data\" not in result:\n            Logger.warning(\n                f\"Function: {func.__name__} is missing a required output field:\\\n                data\"\n            )\n            MPI.COMM_WORLD.Abort(0)\n\n        return result\n\n    return wrapper\n</code></pre>"},{"location":"api/API%20Reference/src/config/","title":"Module <code>src.config</code>","text":""},{"location":"api/API%20Reference/src/config/#AI4SurrogateModelling.src.config","title":"AI4SurrogateModelling.src.config","text":"<p>Contains modules and functions related to parsing, processing and verification of user supplied configurations.</p>"},{"location":"api/API%20Reference/src/config/#AI4SurrogateModelling.src.config.Configuration","title":"Configuration","text":"<pre><code>Configuration(config: Config)\n</code></pre> <p>Validates the user supplied configuration and performs basic pre-processing steps:     - evaluates aliases such that no aliases contain variables     - uses the evaluated aliases to replace all variables in the                configuration tree.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Contains the user supplied configuration.</p> required Source code in <code>AI4SurrogateModelling/src/config.py</code> <pre><code>def __init__(self, config: Config):\n    \"\"\"Validates the user supplied configuration and performs basic\n    pre-processing steps:\n        - evaluates aliases such that no aliases contain variables\n        - uses the evaluated aliases to replace all variables in the\\\n            configuration tree.\n\n    Args:\n        config (dict): Contains the user supplied configuration.\n    \"\"\"\n\n    cfg = config.model_dump()\n    noperations = len(cfg[\"operations\"])\n    naliases = len(cfg[\"aliases\"])\n\n    # processing aliases\n    Logger.info(f\"Number of aliases in the configuration file: {naliases}\")\n    self.alias_evaluation_map = self._get_alias_evaluation_map(\n        cfg[\"aliases\"],\n    )\n    for v, w in self.alias_evaluation_map.items():\n        Logger.info(f\"Replacing '{v}' with '{w}'\")\n\n    # parsing operations and actions in each operation\n    Logger.info(\n        f\"Number of operations in the configuration file: {noperations}\"\n    )\n    self.operations = [{}] * len(cfg[\"operations\"])\n    operation_keys = list(cfg[\"operations\"].keys())\n    operation_indices = np.sort(\n        [int(op_key.split(\"_\")[-1]) for op_key in operation_keys]\n    )\n\n    for operation_index_local, operation_index_global in enumerate(\n        operation_indices\n    ):\n        op_uid = f\"operation_{operation_index_global}\"\n        self.operations[operation_index_local] = self._read_operation(\n            cfg[\"operations\"][op_uid]\n        )\n\n    # check if the user-provided action sequences do not contain dependency\n    # cycles\n    runtime_state.check_object_dependencies()\n</code></pre>"},{"location":"api/API%20Reference/src/config/#AI4SurrogateModelling.src.config.Configuration.alias_evaluation_map","title":"alias_evaluation_map  <code>instance-attribute</code>","text":"<pre><code>alias_evaluation_map = _get_alias_evaluation_map(\n    cfg[\"aliases\"]\n)\n</code></pre>"},{"location":"api/API%20Reference/src/config/#AI4SurrogateModelling.src.config.Configuration.operations","title":"operations  <code>instance-attribute</code>","text":"<pre><code>operations = [{}] * len(cfg['operations'])\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/","title":"Module <code>src.history_progress</code>","text":""},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress","title":"AI4SurrogateModelling.src.history_progress","text":""},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress","title":"HistoryProgress","text":"<pre><code>HistoryProgress()\n</code></pre> <p>Keeps track of various metrics obtained during the training process.</p> Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def __init__(self):\n    self.measurements = {}\n    self.measurements_parsed = {}\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.measurements","title":"measurements  <code>instance-attribute</code>","text":"<pre><code>measurements = {}\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.measurements_parsed","title":"measurements_parsed  <code>instance-attribute</code>","text":"<pre><code>measurements_parsed = {}\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.add","title":"add","text":"<pre><code>add(key: str, value: Any, log: bool = False)\n</code></pre> <p>Adds a new measurement at the current internal index position.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path to the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>Value to be added</p> required Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def add(\n    self,\n    key: str,\n    value: Any,\n    log: bool = False,\n):\n    \"\"\"Adds a new measurement at the current internal index position.\n\n    Args:\n        key (str): Path to the nested dictionary.\n        value (Any): Value to be added\n    \"\"\"\n    if key not in self.measurements:\n        self.measurements[key] = []\n\n    M = self.measurements[key]\n    value = float(value)\n    if log:\n        value = np.log10(value)\n    M.append(value)\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.add_transpose_last","title":"add_transpose_last","text":"<pre><code>add_transpose_last(key: str, value: Any, log: bool = False)\n</code></pre> <p>Adds a new measurement at the current internal index position. Also exchanges the last entry in the key with the data key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Path to the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>Value to be added</p> required Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def add_transpose_last(\n    self,\n    key: str,\n    value: Any,\n    log: bool = False,\n):\n    \"\"\"Adds a new measurement at the current internal index position.\n    Also exchanges the last entry in the key with the data key.\n\n    Args:\n        key (str): Path to the nested dictionary.\n        value (Any): Value to be added\n    \"\"\"\n    key_tokens = key.split(\"/\")\n    key_last = key_tokens[-1]\n    key_prefix = \"/\".join(key_tokens[:-1])\n\n    for k, v in value.items():\n        if \"model_parameters\" in k:\n            key = f\"{key_prefix}/{k}\"\n        else:\n            key = f\"{key_prefix}/{k}/{key_last}\"\n        if key not in self.measurements:\n            self.measurements[key] = []\n        M = self.measurements[key]\n        v = float(v)\n        if log:\n            v = np.log10(v)\n        M.append(v)\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.get_last","title":"get_last","text":"<pre><code>get_last() -&gt; dict\n</code></pre> Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def get_last(self) -&gt; dict:\n    return {key: v[-1] for key, v in self.measurements.items()}\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.parse","title":"parse","text":"<pre><code>parse()\n</code></pre> <p>Converts the raw lists of measurements as dictionaries into a nested dictionary of lists.</p> Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def parse(self):\n    \"\"\"Converts the raw lists of measurements as dictionaries into a nested\n    dictionary of lists.\n    \"\"\"\n    self.measurements_parsed = {}\n\n    for key, v in self.measurements.items():\n        dictionary_path = key.split(\"/\")\n\n        M = self.measurements_parsed\n        for k in dictionary_path[:-1]:\n            if k not in M:\n                M[k] = {}\n            M = M[k]\n\n        last_key = dictionary_path[-1]\n        if isinstance(v[0], dict):\n            M[last_key] = {}\n            for k in v[0]:\n                M[last_key][k] = [\n                    v[idx][k] if v[idx] is not None else None\n                    for idx in range(len(v))\n                ]\n        else:\n            M[last_key] = v\n</code></pre>"},{"location":"api/API%20Reference/src/history_progress/#AI4SurrogateModelling.src.history_progress.HistoryProgress.should_terminate","title":"should_terminate","text":"<pre><code>should_terminate(\n    key: str, niterations: int = 10, treshold: float = 1e-06\n) -&gt; bool\n</code></pre> Source code in <code>AI4SurrogateModelling/src/history_progress.py</code> <pre><code>def should_terminate(\n    self, key: str, niterations: int = 10, treshold: float = 1e-6\n) -&gt; bool:\n    if key not in self.measurements:\n        return False\n    M = self.measurements[key]\n    if len(M) &lt;= niterations:\n        return False\n\n    subM = np.array(M[-niterations:])\n    # Logger.warning(subM)\n    mean = np.mean(subM)\n    diff = np.abs((subM - mean)).max()\n    if diff &lt; treshold:\n        msg = f'Terminating training proceduce. Values with key: \"{key}\" did not change significantly during the last {niterations} iterations.'\n        Logger.info(msg)\n        return True\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/io/","title":"Module <code>src.io</code>","text":""},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io","title":"AI4SurrogateModelling.src.io","text":"<p>Module conatining methods and classes utilized in efficient parallel access to a filesystem.</p>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile","title":"ParallelFile","text":"<pre><code>ParallelFile(fn: str, initial_file_size: int = 10 ** 6)\n</code></pre> <p>Wrapper for easy parallel file manipulation of an LMDB file.</p> <p>Sets all internal state variables to default values. Initializes the database file. - filename to the database - initial size of the database file - load queue - save queue - remove queue - move queue</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>Path to the directory storign the related database files.</p> required <code>initial_file_size</code> <code>int</code> <p>Initial size of the database file.            Defaults to 10**6.</p> <code>10 ** 6</code> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    fn: str,\n    initial_file_size: int = 10**6,\n) -&gt; None:\n    \"\"\"Sets all internal state variables to default values. Initializes the\n    database file.\n    - filename to the database\n    - initial size of the database file\n    - load queue\n    - save queue\n    - remove queue\n    - move queue\n\n    Args:\n        fn (str): Path to the directory storign the related database files.\n        initial_file_size (int): Initial size of the database file.\\\n        Defaults to 10**6.\n    \"\"\"\n    self.fn = fn\n    self.initial_size = initial_file_size\n\n    self.queue_load = {}\n    self.queue_save = {}\n    self.queue_remove = {}\n    self.queue_move = {}\n\n    if not os.path.exists(self.fn):\n        env = lmdb.open(self.fn, map_size=initial_file_size, subdir=True)\n        env.close()\n\n    if mpi.is_master():\n        if not os.path.isdir(self.fn):\n            Logger.info(\n                f'Creating directory \"{self.fn}\" containing\\\n                the data file'\n            )\n            os.makedirs(self.fn)\n    mpi.sync()\n\n    self._load_metadata()\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn = fn\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.initial_size","title":"initial_size  <code>instance-attribute</code>","text":"<pre><code>initial_size = initial_file_size\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.queue_load","title":"queue_load  <code>instance-attribute</code>","text":"<pre><code>queue_load = {}\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.queue_move","title":"queue_move  <code>instance-attribute</code>","text":"<pre><code>queue_move = {}\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.queue_remove","title":"queue_remove  <code>instance-attribute</code>","text":"<pre><code>queue_remove = {}\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.queue_save","title":"queue_save  <code>instance-attribute</code>","text":"<pre><code>queue_save = {}\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.append","title":"append","text":"<pre><code>append(*, data: dict[str], msg: str) -&gt; None\n</code></pre> <p>Takes the supplied data and inserts them at the end of this database. Must be called by ALL mpi processes (even if some processes might have empty data).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str]</code> <p>A dictionary containing a prefix as a            key and the associated array of objects. The controller will            store the objects under the path starting with the prefix            key.</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef append(\n    self,\n    *,\n    data: dict[str],\n    msg: str,\n) -&gt; None:\n    \"\"\"Takes the supplied data and inserts them at the end of this database.\n    Must be called by ALL mpi processes (even if some processes might have\n    empty data).\n\n    Args:\n        data (dict[str]): A dictionary containing a prefix as a\\\n        key and the associated array of objects. The controller will\\\n        store the objects under the path starting with the prefix\\\n        key.\n        msg (str): An informative message for the user's benefit.\n    \"\"\"\n    keys = data.keys()\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f\"The queue is not empty, cannot save the files with prefix:\\\n            {keys}\",\n        )\n\n    # retrieve the global number of new entries for each prefix\n    ncurrent = self._get_ncurrently_stored_files(keys)\n    nnew_local = {k: len(data[k]) for k in keys}\n    nnew = mpi.allgather(data=nnew_local)\n\n    # calculate the appropriate index shifting so no files are overwritten\n    # and no gaps in indexing are present\n    shift = {k: 0 for k in keys}\n    for rank in range(mpi.get_rank() - 1):\n        for k in keys:\n            shift[k] += nnew[rank][k]\n\n    indices_global = {}\n    for k in keys:\n        indices_global[k] = [\n            ncurrent[k] + shift[k] + i for i in range(nnew_local[k])\n        ]\n\n    # finally construct the local save queues\n    self.queue_save = {}\n    nops = 0\n    for k in keys:\n        self.queue_save[k] = [\n            (self._get_fn_key(k, indices_global[k][i]), data[k][i])\n            for i in range(len(indices_global[k]))\n        ]\n        nops += len(self.queue_save[k])\n\n    Logger.info(\n        f'Appending {nops} entries with keys: {keys} into database:\\\n        \"{self.fn}\"'\n    )\n\n    self._save_files(\n        msg,\n        synchronous_access=True,\n        synchronize=True,\n        lmdb_environment_fn=self.fn,\n    )\n\n    self._increase_ncurrently_stored_files(\n        values={\n            k: np.sum([nnew[i][k] for i in range(len(nnew))]) for k in keys\n        },\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.get_default_metadata","title":"get_default_metadata","text":"<pre><code>get_default_metadata() -&gt; dict\n</code></pre> <p>Constructs and returns the default metadata structure. Currently, it contains the number of stored files for each prefix in the database.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Metadata</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef get_default_metadata(\n    self,\n) -&gt; dict:\n    \"\"\"Constructs and returns the default metadata structure. Currently, it\n    contains the number of stored files for each prefix in the database.\n\n    Returns:\n        dict: Metadata\n    \"\"\"\n    return {\"prefix_nfiles\": {}}\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.load","title":"load","text":"<pre><code>load(\n    *, keys: list[str], msg: str\n) -&gt; tuple[dict[str : (list[Any])], list[int]]\n</code></pre> <p>Loads all the files with the specified prefix. Each MPI process holds a portion of the files. Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of prefixes for which to perform the            loading operation</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str:(list[Any])], list[int]]</code> <p>tuple[dict[str: list[Any]], list[int]]: A list of loaded objects            for each supplied prefix and the associated list of global file            indices.</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef load(\n    self,\n    *,\n    keys: list[str],\n    msg: str,\n) -&gt; tuple[dict[str : list[Any]], list[int]]:\n    \"\"\"Loads all the files with the specified prefix. Each MPI process holds\n    a portion of the files. Must be called by ALL MPI processes.\n\n    Args:\n        keys (list[str]): A list of prefixes for which to perform the\\\n        loading operation\n        msg (str): An informative message for the user's benefit.\n\n    Returns:\n        tuple[dict[str: list[Any]], list[int]]: A list of loaded objects\\\n        for each supplied prefix and the associated list of global file\\\n        indices.\n    \"\"\"\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f\"The queue is not empty, cannot load the files with prefixes:\\\n            {keys}\",\n        )\n\n    nfiles_total = self._get_ncurrently_stored_files(keys)\n    self.queue_load = {}\n    self.load_buffer = {}\n    indices_global = {}\n    for key in keys:\n        indices_global[key] = [\n            i for i in mpi.iterator(range(nfiles_total[key]))\n        ]\n\n        self.queue_load[key] = [\n            self._get_fn_key(key, i) for i in indices_global[key]\n        ]\n\n        self.load_buffer[key] = [\n            None for _ in range(len(indices_global[key]))\n        ]\n\n    self._load_files(\n        msg,\n        synchronous_access=False,\n        synchronize=False,\n        lmdb_environment_fn=self.fn,\n    )\n\n    return self.load_buffer, indices_global\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.load_single_process","title":"load_single_process","text":"<pre><code>load_single_process(\n    *, keys: list[str], msg: str, rank: int = 0\n) -&gt; tuple[dict[str : (list[Any])], list[int]]\n</code></pre> <p>Loads all the files with the specified prefix, can be called by any MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of prefixes for which to perform the            loading operation</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required <code>rank</code> <code>int</code> <p>Rank of the process to load data for. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[dict[str:(list[Any])], list[int]]</code> <p>tuple[dict[str: list[Any]], list[int]]: A list of loaded objects            for each supplied prefix and the associated list of global file            indices.</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef load_single_process(\n    self,\n    *,\n    keys: list[str],\n    msg: str,\n    rank: int = 0,\n) -&gt; tuple[dict[str : list[Any]], list[int]]:\n    \"\"\"Loads all the files with the specified prefix, can be called by any\n    MPI processes.\n\n    Args:\n        keys (list[str]): A list of prefixes for which to perform the\\\n        loading operation\n        msg (str): An informative message for the user's benefit.\n        rank (int): Rank of the process to load data for. Defaults to 0.\n\n    Returns:\n        tuple[dict[str: list[Any]], list[int]]: A list of loaded objects\\\n        for each supplied prefix and the associated list of global file\\\n        indices.\n    \"\"\"\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f\"The queue is not empty, cannot load the files with prefix:\\\n            {keys}\",\n        )\n\n    self.queue_load = {k: [] for k in keys}\n    self.load_buffer = {}\n\n    if mpi.get_rank() == rank:\n        nfiles_total = self._get_ncurrently_stored_files(keys)\n        indices_global = list(np.arange(start=0, stop=nfiles_total, step=1))\n        for key in keys:\n            self.queue_load[key] = [\n                self._get_fn_key(key, i) for i in indices_global\n            ]\n\n            self.load_buffer[key] = [\n                None for _ in range(len(indices_global))\n            ]\n    else:\n        self.load_buffer = {}\n        indices_global = []\n\n    self._load_files(\n        msg,\n        synchronous_access=False,\n        synchronize=False,\n        lmdb_environment_fn=self.fn,\n    )\n\n    return self.load_buffer, indices_global\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.load_subset","title":"load_subset","text":"<pre><code>load_subset(\n    *, indices_global: list[int], keys: list[str], msg: str\n) -&gt; dict[str : (list[Any])]\n</code></pre> <p>For each of the supplied prefix, loads all files corresponding to the provided global indices.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of prefixes for which to perform the            loading operation</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required <code>indices_global</code> <code>list[int]</code> <p>A list of file indices to load.</p> required <p>Returns:</p> Type Description <code>dict[str:(list[Any])]</code> <p>dict[str: list[Any]]: A list of loaded objects for each supplied            prefix.</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef load_subset(\n    self,\n    *,\n    indices_global: list[int],\n    keys: list[str],\n    msg: str,\n) -&gt; dict[str : list[Any]]:\n    \"\"\"For each of the supplied prefix, loads all files corresponding to the\n    provided global indices.\n\n    Args:\n        keys (list[str]): A list of prefixes for which to perform the\\\n        loading operation\n        msg (str): An informative message for the user's benefit.\n        indices_global (list[int]): A list of file indices to load.\n\n    Returns:\n        dict[str: list[Any]]: A list of loaded objects for each supplied\\\n        prefix.\n    \"\"\"\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f\"The queue is not empty, cannot load the files with prefix:\\\n            {keys}\",\n        )\n\n    self.queue_load = {}\n    self.load_buffer = {}\n    for key in keys:\n        self.queue_load[key] = [\n            self._get_fn_key(key, i) for i in indices_global\n        ]\n        self.load_buffer[key] = [\n            None for _ in range(len(self.queue_load[key]))\n        ]\n\n    self._load_files(\n        msg,\n        synchronous_access=False,\n        synchronize=False,\n        lmdb_environment_fn=self.fn,\n    )\n\n    return self.load_buffer\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.remove","title":"remove","text":"<pre><code>remove(\n    *, keys: list[str], indices_global: list[int], msg: str\n) -&gt; None\n</code></pre> <p>Given the list of prefixes and global file indices for each prefix, removes the corresponding files. Automatically recalculates which files need to be moved so that the resulting files for each prefix use a continuous indexation. The number of file io operations is minimized.</p> <p>Must be called by ALL mpi processes and assumes ONLY mpi process with rank 0 has the relevant data.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of prefixes for which to perform the            removal operation</p> required <code>indices_global</code> <code>list[int]</code> <p>Indices of files to be removed. One            list for all prefixes. All processes have the same list, no need            for synchronization.</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef remove(\n    self,\n    *,\n    keys: list[str],\n    indices_global: list[int],\n    msg: str,\n) -&gt; None:\n    \"\"\"Given the list of prefixes and global file indices for each prefix,\n    removes the corresponding files. Automatically recalculates which files\n    need to be moved so that the resulting files for each prefix use a\n    continuous indexation. The number of file io operations is minimized.\n\n    Must be called by ALL mpi processes and assumes ONLY mpi process with\n    rank 0 has the relevant data.\n\n    Args:\n        keys (list[str]): A list of prefixes for which to perform the\\\n        removal operation\n        indices_global (list[int]): Indices of files to be removed. One\\\n        list for all prefixes. All processes have the same list, no need\\\n        for synchronization.\n        msg (str): An informative message for the user's benefit.\n    \"\"\"\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f\"The queue is not empty, cannot remove the files with prefix:\\\n            {keys}\",\n        )\n\n    nremoved = 0\n    nops_move = 0\n    nops_remove = 0\n    # only the master rank will perform this operation\n    if mpi.get_rank() == 0:\n        self.queue_move = {k: [] for k in keys}\n        self.queue_remove = {k: [] for k in keys}\n\n        # sort the global indices for more robust processing\n        removed_indices = np.sort(indices_global)\n        nremoved = len(removed_indices)\n        ntotal = self._get_ncurrently_stored_files(keys)\n        assert np.min(ntotal) == np.max(ntotal)\n\n        # keep the indices of kept files in reverse order for more\n        # efficient approach\n        kept_indices = np.sort(\n            list(\n                set(range(ntotal[keys[0]])).difference(set(indices_global))\n            )\n        )[::-1]\n\n        # essentially a tracker of kept/removed file indices\n        test_list = [i for i in range(ntotal[keys[0]])]\n        for i in removed_indices:\n            test_list[i] = None\n\n        for step_idx, kept_idx in enumerate(kept_indices):\n\n            # once the list of files is reduced by 'nremoved' elements,\n            # we are done\n            if kept_idx &lt; len(test_list) - nremoved:\n                break\n\n            # idx_t &lt; kept_idx, we move a file index\n            idx_to = removed_indices[step_idx]\n            test_list[idx_to] = kept_idx\n\n            for key in keys:\n                self.queue_move[key].append(\n                    (\n                        self._get_fn_key(key, kept_idx),\n                        self._get_fn_key(key, idx_to),\n                    )\n                )\n\n        # now we are in a state when we moved some of the kept files into\n        # removed files with lower indices\n        # we need to check whether the remaining files to be removed still\n        # need removing\n        for idx_remove in range(len(test_list) - nremoved, len(test_list)):\n\n            # the index is supposed to be removed, but is not part of the\n            # movement logic\n            # place it to the remoive queue\n            if test_list[idx_remove] is not None:\n                for key in keys:\n                    self.queue_remove[key].append(\n                        self._get_fn_key(key, idx_remove)\n                    )\n                test_list[idx_remove] = None\n\n        for key in keys:\n            nops_remove += len(self.queue_remove[key])\n            nops_move += len(self.queue_move[key])\n\n        for i in range(ntotal[keys[0]] - nremoved):\n            if test_list[i] is None:\n                mpi.abort(\n                    0,\n                    \"Incorrect reindexing map after file removal\\\n                    (Encountered a deleted element)\",\n                )\n            if test_list[i] in removed_indices:\n                mpi.abort(\n                    0,\n                    \"Incorrect reindexing map after file removal\\\n                    (index should be removed but remained)\",\n                )\n\n        interr = False\n        for i in range(ntotal[keys[0]] - nremoved, ntotal[keys[0]]):\n            if test_list[i] is not None:\n                Logger.warning(\n                    f\"Incorrect reindexing map after file removal\\\n                    (Encountered a valid element[{test_list[i]}])\"\n                )\n                interr = True\n        if interr:\n            mpi.abort(0, \"invalid move/del combinations\")\n\n    self._move_files(\n        f\"Moving {nops_move} files\",\n        synchronous_access=False,\n        synchronize=True,\n        lmdb_environment_fn=self.fn,\n    )\n    self._remove_files(\n        f\"Removing {nops_remove} files\",\n        synchronous_access=False,\n        synchronize=True,\n        lmdb_environment_fn=self.fn,\n    )\n\n    nremoved = mpi.broadcast(nremoved, root=0)\n    self._increase_ncurrently_stored_files(\n        values={k: -nremoved for k in keys},\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Sets the internal state to initial values. Empties all queues, sets its own metadata to default values and stores them in the database file.</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef reset(self) -&gt; None:\n    \"\"\"Sets the internal state to initial values. Empties all queues, sets\n    its own metadata to default values and stores them in the database file.\n    \"\"\"\n    self.queue_load = {}\n    self.queue_save = {}\n    self.queue_remove = {}\n    self.queue_move = {}\n    self.metadata = self.get_default_metadata()\n    self._save_metadata()\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.ParallelFile.update","title":"update","text":"<pre><code>update(\n    *,\n    indices_global: dict[list[int]],\n    data: dict[str],\n    msg: str\n) -&gt; None\n</code></pre> <p>Takes the supplied data and the corresponding global indices and updates the corresponding EXISTING content of this database. Must be called by ALL mpi processes (even if some processes might have empty data).Must be called by ALL mpi processes.</p> <p>Parameters:</p> Name Type Description Default <code>indices_global</code> <code>dict[list[int]]</code> <p>description</p> required <code>data</code> <code>dict[str]</code> <p>A dictionary containing a prefix as a            key and the associated array of objects. The controller will store            the objects under the path starting with the prefix key.</p> required <code>msg</code> <code>str</code> <p>An informative message for the user's benefit.</p> required Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef update(\n    self,\n    *,\n    indices_global: dict[list[int]],\n    data: dict[str],\n    msg: str,\n) -&gt; None:\n    \"\"\"Takes the supplied data and the corresponding global indices and\n    updates the corresponding EXISTING content of this database. Must be\n    called by ALL mpi processes (even if some processes might have empty\n    data).Must be called by ALL mpi processes.\n\n    Args:\n        indices_global (dict[list[int]]): _description_\n        data (dict[str]): A dictionary containing a prefix as a\\\n        key and the associated array of objects. The controller will store\\\n        the objects under the path starting with the prefix key.\n        msg (str): An informative message for the user's benefit.\n    \"\"\"\n    if self._queue_size() &gt; 0:\n        mpi.abort(\n            0,\n            f'The queue is not empty, cannot update the files with prefix:\\\n            \"{data.keys()}\"',\n        )\n\n    self.queue_save = {}\n    nops = 0\n    for key in data.keys():\n        self.queue_save[key] = [\n            (self._get_fn_key(key, indices_global[key][i]), data[key][i])\n            for i in range(len(indices_global[key]))\n        ]\n        nops += len(self.queue_save[key])\n\n    Logger.info(f'Updating {nops} rows in database: \"{self.fn}\"')\n\n    self._save_files(\n        msg,\n        synchronous_access=True,\n        synchronize=True,\n        lmdb_environment_fn=self.fn,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.get_first_active_element","title":"get_first_active_element","text":"<pre><code>get_first_active_element(\n    arr: dict[str : (list[str])],\n) -&gt; dict[str:int]\n</code></pre> <p>Determines where to start the processing of the supplemented files. This is necessary, because the queue is processed in predetrmined chunks, the processing of which can fail and need to be restarted.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>dict</code> <p>A dictionary of (key -&gt; filenames) pairs.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str:int]</code> <p>For each key in arr, contains the lowest index of the filename        not yet processed</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>@Logger.logged()\ndef get_first_active_element(\n    arr: dict[str : list[str]],\n) -&gt; dict[str:int]:\n    \"\"\"Determines where to start the processing of the supplemented files. This\n    is necessary, because the queue is processed in predetrmined chunks, the\n    processing of which can fail and need to be restarted.\n\n    Args:\n        arr (dict): A dictionary of (key -&gt; filenames) pairs.\n\n    Returns:\n        dict: For each key in arr, contains the lowest index of the filename\\\n        not yet processed\n    \"\"\"\n    first_active_index = {k: len(v) for k, v in arr.items()}\n\n    # trivial approach, implement binary search in the future or something more\n    # clever with O(1) complexity\n    for k, v in arr.items():\n        for i in range(len(v)):\n            if v[i] is not None:\n                first_active_index[k] = i\n                break\n\n    return first_active_index\n</code></pre>"},{"location":"api/API%20Reference/src/io/#AI4SurrogateModelling.src.io.lmdb_controller","title":"lmdb_controller","text":"<pre><code>lmdb_controller(func) -&gt; Callable\n</code></pre> <p>A decorator function used to wrap parallel io methods. It splits the file operation into chunks of size 1000. When the database does not have enough storage, it increases it by a predefined factor and restarts the file processing from the last unfinished chunk.</p> The decorated function requires three arguments <p>lmdb_environment_fn (str): path to an LMDB file synchronous_access (bool): if true, processes do not access the files        simultaneously, but go in a round-robin fashion. When one process        finishes all its work, another one line continues with its work. synchronize (bool): if true, processes access the files simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function working with files in parallel.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>the wrapped function</p> Source code in <code>AI4SurrogateModelling/src/io.py</code> <pre><code>def lmdb_controller(func)-&gt; Callable:\n    \"\"\"A decorator function used to wrap parallel io methods. It splits the file\n    operation into chunks of size 1000. When the database does not have enough\n    storage, it increases it by a predefined factor and restarts the file\n    processing from the last unfinished chunk.\n\n    The decorated function requires three arguments:\n        lmdb_environment_fn (str): path to an LMDB file\n        synchronous_access (bool): if true, processes do not access the files\\\n        simultaneously, but go in a round-robin fashion. When one process\\\n        finishes all its work, another one line continues with its work.\n        synchronize (bool): if true, processes access the files simultaneously.\n\n    Args:\n        func (Callable): A function working with files in parallel.\n\n    Returns:\n        Callable: the wrapped function\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if \"lmdb_environment_fn\" not in kwargs:\n            mpi.abort(\n                0,\n                f'The function: \"{func.__name__}\" requires the following\\\n                argument: \"lmdb_environment_fn\"',\n            )\n        if \"synchronous_access\" not in kwargs:\n            mpi.abort(\n                0,\n                f'The function: \"{func.__name__}\" requires the following\\\n                argument: \"synchronous_access\"',\n            )\n        if \"synchronize\" not in kwargs:\n            mpi.abort(\n                0,\n                f'The function: \"{func.__name__}\" requires the following\\\n                argument: \"synchronize\"',\n            )\n\n        env_fn = kwargs[\"lmdb_environment_fn\"]\n        synchronous_access = kwargs[\"synchronous_access\"]\n        synchronize = kwargs[\"synchronize\"]\n        kwargs[\"commit_interval\"] = 1000\n\n        env = lmdb.open(env_fn, subdir=True)\n\n        current_env_size = env.info()[\"map_size\"]\n        while True:\n\n            kwargs[\"env\"] = env\n            try:\n                if synchronous_access:\n                    for rank in range(mpi.get_world_size()):\n                        if rank == mpi.get_rank():\n                            func(*args, **kwargs)\n                        res = mpi.allreduce(data=0, op=mpi.OP.MAX)\n                        if res &gt; 0.5:\n                            break\n                elif synchronize:\n                    func(*args, **kwargs)\n                    res = mpi.allreduce(data=0, op=mpi.OP.MAX)\n                else:\n                    func(*args, **kwargs)\n                    res = 0\n\n            except lmdb.MapFullError:\n                if synchronous_access or synchronize:\n                    res = mpi.allreduce(data=1, op=mpi.OP.MAX)\n                else:\n                    res = 1\n\n            if res &gt; 0.5:\n                new_env_size = int(1.5 * current_env_size)\n                Logger.info(\n                    f\"Setting a new map size:\\\n                    {_convert_bytes_to_str(current_env_size)} -&gt;\\\n                    {_convert_bytes_to_str(new_env_size)}\"\n                )\n                current_env_size = new_env_size\n                env.set_mapsize(new_env_size)\n            else:\n                break\n\n        env.close()\n\n    return wrapper\n</code></pre>"},{"location":"api/API%20Reference/src/logging/","title":"Module <code>src.logging</code>","text":""},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging","title":"AI4SurrogateModelling.src.logging","text":"<p>Module containing classes for logging functionality.</p>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger","title":"Logger","text":"<p>A static class containing methods and decorators for comprehensive logging functionality.</p>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.level","title":"level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>level = 0\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.logger_level","title":"logger_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logger_level = NONE\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.mpi_is_master","title":"mpi_is_master  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mpi_is_master = True\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.mpi_rank","title":"mpi_rank  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mpi_rank = 0\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.mpi_size","title":"mpi_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mpi_size = 1\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.target_dir","title":"target_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_dir = 'logs'\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.debug","title":"debug  <code>staticmethod</code>","text":"<pre><code>debug(msg: str = '') -&gt; None\n</code></pre> <p>Outputs DEBUG level logging information to the MPI specific files.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to be logged. Default ''.</p> <code>''</code> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef debug(msg: str = \"\") -&gt; None:\n    \"\"\"Outputs DEBUG level logging information to the MPI specific files.\n\n    Args:\n        msg (str): Message to be logged. Default ''.\n    \"\"\"\n    if Logger.logger_level.value &gt;= LoggerInfoLevel.DEBUG.value:\n        logger.debug(f\"{Logger.indent()}{msg}\")\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.get_result_log_string","title":"get_result_log_string  <code>staticmethod</code>","text":"<pre><code>get_result_log_string(obj: Any) -&gt; str\n</code></pre> <p>A recursive function representing any object as a string. Used for logging purpose where some object can be extremely large or contain many elements and implicit string conversion is infeasible.</p> <p>Based on the type of the object, these conversion rules apply:     str: returns the string as is, unless its longer than 30            characters, in which case it returns 'string, length[n]',            where n = len(obj)</p> <pre><code>bytes: returns 'binary data, length[n]', where n = len(obj)\n\nnumber: returns the string representation of the number.\n\niterable: other objects which can be iterated over, like lists,            dictionaries, tuples and sets, are parsed recursively.\n\nnumpy array: Numpy arrays result in 'numpy.ndarray(shape = (n)',            where n is the shape of obj.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to be represented as a string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the object.</p> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef get_result_log_string(obj: Any) -&gt; str:\n    \"\"\"A recursive function representing any object as a string.\n    Used for logging purpose where some object can be extremely large or\n    contain many elements and implicit string conversion is infeasible.\n\n    Based on the type of the object, these conversion rules apply:\n        str: returns the string as is, unless its longer than 30\\\n        characters, in which case it returns 'string, length[n]',\\\n        where n = len(obj)\n\n        bytes: returns 'binary data, length[n]', where n = len(obj)\n\n        number: returns the string representation of the number.\n\n        iterable: other objects which can be iterated over, like lists,\\\n        dictionaries, tuples and sets, are parsed recursively.\n\n        numpy array: Numpy arrays result in 'numpy.ndarray(shape = (n)',\\\n        where n is the shape of obj.\n\n    Args:\n        obj (Any): Object to be represented as a string.\n\n    Returns:\n        str: String representation of the object.\n    \"\"\"\n    if obj is not None:\n        len_limit = 2\n\n        if Logger.is_iterable(obj):\n            if isinstance(obj, str):\n                if len(obj) &lt; 30:\n                    return f\"{obj}\"\n                return f\"string, length[{len(obj)}]\"\n\n            if isinstance(obj, bytes):\n                return f\"binary data, length[{len(obj)}]\"\n\n            if isinstance(obj, dict):\n                return (\n                    \"{\"\n                    + f'{\", \".join([f\"{Logger.get_result_log_string(k)}: {Logger.get_result_log_string(v)}\" for k, v in obj.items()])}'\n                    + \"}\"\n                )\n\n            if isinstance(obj, list):\n                len_ = len(obj)\n                if len_ &gt; len_limit:\n                    out = (\n                        \"[\"\n                        + f'{\", \".join([f\"{Logger.get_result_log_string(v)}\" for v in obj[:len_limit]])}'\n                        + f\", and {len_ - len_limit} more ... ]\"\n                    )\n                else:\n                    out = (\n                        \"[\"\n                        + f'{\", \".join([f\"{Logger.get_result_log_string(v)}\" for v in obj])}'\n                        + \"]\"\n                    )\n                return out\n\n            if isinstance(obj, tuple):\n                return (\n                    \"(\"\n                    + f'{\", \".join([f\"{Logger.get_result_log_string(v)}\"\n                    for v in obj])}'\n                    + \")\"\n                )\n\n            if isinstance(obj, set):\n                return (\n                    \"(\"\n                    + f'{\", \".join([f\"{Logger.get_result_log_string(v)}\"\n                    for v in obj])}'\n                    + \")\"\n                )\n\n            if isinstance(obj, np.ndarray):\n                return f\"numpy.ndarray(shape = ({obj.shape})\"\n\n        return f\"{obj}\"\n\n    return \"None\"\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.indent","title":"indent  <code>staticmethod</code>","text":"<pre><code>indent() -&gt; str\n</code></pre> <p>Returns a whitespace string. The length depends on the depth of the logged function in the stack.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef indent() -&gt; str:\n    \"\"\"Returns a whitespace string. The length depends on the depth of the\n    logged function in the stack.\n\n    Returns:\n        str: # of white spaces lineary proportional to the stack depth\n    \"\"\"\n    return \"  \" * Logger.level\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.indent--of-white-spaces-lineary-proportional-to-the-stack-depth","title":"of white spaces lineary proportional to the stack depth","text":""},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.info","title":"info  <code>staticmethod</code>","text":"<pre><code>info(msg: str = '') -&gt; None\n</code></pre> <p>Outputs INFO level logging information to the MPI specific files.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to be logged. Default ''.</p> <code>''</code> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef info(msg: str = \"\") -&gt; None:\n    \"\"\"Outputs INFO level logging information to the MPI specific files.\n\n    Args:\n        msg (str): Message to be logged. Default ''.\n    \"\"\"\n    if Logger.logger_level.value &gt;= LoggerInfoLevel.INFO.value:\n        logger.info(f\"{Logger.indent()}{msg}\")\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.init","title":"init  <code>staticmethod</code>","text":"<pre><code>init(\n    logger_level: LoggerInfoLevel,\n    mpi_rank: int,\n    mpi_size: int,\n    mpi_is_master: bool,\n) -&gt; None\n</code></pre> <p>Initializes the static logging environment. Each MPI process has its own logging file. Logging should not be conditioned based on the MPI process rank, this may result in program hanging.</p> <p>Parameters:</p> Name Type Description Default <code>logger_level</code> <code>LoggerInfoLevel</code> <p>The amount of logged information.</p> required <code>mpi_rank</code> <code>int</code> <p>The global MPI rank.</p> required <code>mpi_size</code> <code>int</code> <p>The total number of MPI processes</p> required <code>mpi_is_master</code> <code>bool</code> <p>Is this MPI process a master?</p> required Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef init(\n    logger_level: LoggerInfoLevel,\n    mpi_rank: int,\n    mpi_size: int,\n    mpi_is_master: bool,\n) -&gt; None:\n    \"\"\"Initializes the static logging environment. Each MPI process has its\n    own logging file. Logging should not be conditioned based on the MPI\n    process rank, this may result in program hanging.\n\n    Args:\n        logger_level (LoggerInfoLevel): The amount of logged information.\n        mpi_rank (int): The global MPI rank.\n        mpi_size (int): The total number of MPI processes\n        mpi_is_master (bool): Is this MPI process a master?\n    \"\"\"\n\n    Logger.mpi_rank = mpi_rank\n    Logger.mpi_size = mpi_size\n    Logger.mpi_is_master = mpi_is_master\n\n    Logger.logger_level = logger_level\n\n    # Remove the default logging sink (which logs to terminal)\n    logger.remove()\n\n    if logger_level != LoggerInfoLevel.NONE:\n        level_str = \"INFO\"\n        if logger_level.value &gt;= LoggerInfoLevel.DEBUG.value:\n            level_str = \"DEBUG\"\n\n        logger.add(\n            f\"{Logger.target_dir}/app_{Logger.mpi_rank:04d}.log\",\n            format=\"{time} {level} {message}\",\n            level=level_str,\n            backtrace=True,\n            serialize=False,\n            mode=\"w\",\n        )\n        logger.add(\n            f\"{Logger.target_dir}/app_{Logger.mpi_rank:04d}.json\",\n            format=\"{time} {level} {message}\",\n            level=level_str,\n            backtrace=True,\n            serialize=True,\n            mode=\"w\",\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.is_iterable","title":"is_iterable  <code>staticmethod</code>","text":"<pre><code>is_iterable(obj: Any) -&gt; bool\n</code></pre> <p>Determines if the object can be iterated over.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to be analyzed.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if it can be iterated over, False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef is_iterable(obj: Any) -&gt; bool:\n    \"\"\"Determines if the object can be iterated over.\n\n    Args:\n        obj (Any): Object to be analyzed.\n\n    Returns:\n        bool: True if it can be iterated over, False otherwise.\n    \"\"\"\n    return isinstance(obj, Iterable)\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.logged","title":"logged  <code>staticmethod</code>","text":"<pre><code>logged(commentary='', show_debug=False) -&gt; Any\n</code></pre> <p>Decorator to log function calls, arguments, return values, and optional commentary.</p> <p>Parameters:</p> Name Type Description Default <code>commentary</code> <code>str</code> <p>Supplementary debug information. Defaults to ''.</p> <code>''</code> <code>show_debug</code> <code>bool</code> <p>Optional flag. When the logging level includes            DEBUG information, this flag can enable/disable it for select            functions. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>decorator</p> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef logged(commentary=\"\", show_debug=False) -&gt; Any:\n    \"\"\"Decorator to log function calls, arguments, return values, and\n    optional commentary.\n\n    Args:\n        commentary (str): Supplementary debug information. Defaults to ''.\n        show_debug (bool): Optional flag. When the logging level includes\\\n        DEBUG information, this flag can enable/disable it for select\\\n        functions. Defaults to False.\n\n    Returns:\n        Any: decorator\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            Logger.level += 1\n\n            if (\n                Logger.logger_level.value &lt; LoggerInfoLevel.DEBUG.value\n                or not show_debug\n            ):\n                result = func(*args, **kwargs)\n\n            else:\n                arg_values = \", \".join(\n                    Logger.get_result_log_string(a) for a in args\n                )\n                kwarg_values = Logger.get_result_log_string(kwargs)\n                params = (\n                    f\"args = {arg_values}, kwarg = {kwarg_values}\".strip(\n                        \", \"\n                    )\n                )\n\n                Logger.debug(\n                    f\"Calling {func.__name__}({params}) {commentary}\"\n                )\n\n                mem_ = MEM.get_memory_usage_MB()\n                time_ = time.time()\n                result = func(*args, **kwargs)\n                mem_ = MEM.get_memory_usage_MB() - mem_\n                time_ = time.time() - time_\n\n                if result is not None:\n                    Logger.debug(\n                        f\"{func.__name__} returned\\\n                        {Logger.get_result_log_string(result)!r}\\\n                        (Time: {time_: 6.3f} [s] Memory: {mem_: 6.3f} [MB])\"\n                    )\n                else:\n                    Logger.debug(\n                        f\"{func.__name__} finished (Time: {time_: 6.3f} [s]\\\n                        Memory: {mem_: 6.3f} [MB])\"\n                    )\n\n            Logger.level -= 1\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.Logger.warning","title":"warning  <code>staticmethod</code>","text":"<pre><code>warning(msg: str = '') -&gt; None\n</code></pre> <p>Outputs WARNING level logging information to the MPI specific files.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to be logged. Default ''.</p> <code>''</code> Source code in <code>AI4SurrogateModelling/src/logging.py</code> <pre><code>@staticmethod\ndef warning(msg: str = \"\") -&gt; None:\n    \"\"\"Outputs WARNING level logging information to the MPI specific files.\n\n    Args:\n        msg (str): Message to be logged. Default ''.\n    \"\"\"\n    if Logger.logger_level.value &gt;= LoggerInfoLevel.INFO.value:\n        logger.warning(f\"{Logger.indent()}{msg}\")\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.LoggerInfoLevel","title":"LoggerInfoLevel","text":"<p>               Bases: <code>Enum</code></p> <p>Enum class determining the level of logging to be used in the program</p>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.LoggerInfoLevel.DEBUG","title":"DEBUG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEBUG = 2\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.LoggerInfoLevel.INFO","title":"INFO  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INFO = 1\n</code></pre>"},{"location":"api/API%20Reference/src/logging/#AI4SurrogateModelling.src.logging.LoggerInfoLevel.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre>"},{"location":"api/API%20Reference/src/memory/","title":"Module <code>src.memory</code>","text":""},{"location":"api/API%20Reference/src/memory/#AI4SurrogateModelling.src.memory","title":"AI4SurrogateModelling.src.memory","text":"<p>Module containing classes used to monitor various metrics related to the usage of memory.</p>"},{"location":"api/API%20Reference/src/memory/#AI4SurrogateModelling.src.memory.MEM","title":"MEM","text":"<p>A static class for simple memory information monitoring.</p>"},{"location":"api/API%20Reference/src/memory/#AI4SurrogateModelling.src.memory.MEM.get_memory_usage_MB","title":"get_memory_usage_MB  <code>staticmethod</code>","text":"<pre><code>get_memory_usage_MB() -&gt; float\n</code></pre> <p>Retrieves the current RAM usage</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>returns the memory usage in MBs.</p> Source code in <code>AI4SurrogateModelling/src/memory.py</code> <pre><code>@staticmethod\ndef get_memory_usage_MB() -&gt; float:\n    \"\"\"Retrieves the current RAM usage\n\n    Returns:\n        float: returns the memory usage in MBs.\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    mem_info = process.memory_info()\n    mem = mem_info.rss / (1024 * 1024)\n\n    return mem\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/","title":"Module <code>src.mpi</code>","text":""},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi","title":"AI4SurrogateModelling.src.mpi","text":"<p>Module containing classes, functions and decorators related to the management and utilization of MPI resources.</p>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi","title":"mpi","text":"<p>A static class containing various quality of life improvements w.r.t. MPI utilization.</p>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.OP","title":"OP  <code>staticmethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>An internal enumerator class for reduction operation specification.</p>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.OP.MAX","title":"MAX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX = MAX\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.OP.MIN","title":"MIN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN = MIN\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.OP.SUM","title":"SUM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUM = SUM\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.abort","title":"abort  <code>staticmethod</code>","text":"<pre><code>abort(abort_code: int, abort_message: str) -&gt; None\n</code></pre> <p>Terminates all MPI processes with the user provided abortion code and message..</p> <p>Parameters:</p> Name Type Description Default <code>abort_code</code> <code>int</code> <p>Code for the reason for the abortion. Defaults to            1.</p> required <code>abort_message</code> <code>str</code> <p>Message to display to the user. Defaults to \"\".</p> required Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef abort(\n    abort_code: int,\n    abort_message: str,\n) -&gt; None:\n    \"\"\"Terminates all MPI processes with the user provided abortion code\n    and message..\n\n    Args:\n        abort_code (int): Code for the reason for the abortion. Defaults to\\\n        1.\n        abort_message (str): Message to display to the user. Defaults to \"\".\n    \"\"\"\n    Logger.info(f\"[{abort_code:04d}]{abort_message}\")\n    history = []\n    frame = inspect.currentframe().f_back\n    while frame is not None:\n        if (\n            \"/logging.py\" not in frame.f_code.co_filename\n            and \"/mpi.py\" not in frame.f_code.co_filename\n        ):\n            history.append((frame.f_code.co_filename, frame.f_lineno))\n        frame = frame.f_back\n    for H in history:\n        Logger.warning(f\"{H}\")\n    MPI.COMM_WORLD.Abort(abort_code)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.allgather","title":"allgather  <code>staticmethod</code>","text":"<pre><code>allgather(data: Any) -&gt; list[Any]\n</code></pre> <p>Gathers data from all MPI processes and creates a list out of them.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Single value</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of values, where the i-th value corresponds to            the value of data on the i-th MPI process.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef allgather(data: Any) -&gt; list[Any]:\n    \"\"\"Gathers data from all MPI processes and creates a list out of\n    them.\n\n    Args:\n        data (Any): Single value\n\n    Returns:\n        list[Any]: A list of values, where the i-th value corresponds to\\\n        the value of data on the i-th MPI process.\n    \"\"\"\n    return MPI.COMM_WORLD.allgather(data)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.allreduce","title":"allreduce  <code>staticmethod</code>","text":"<pre><code>allreduce(data: Any, op: OP) -&gt; Any\n</code></pre> <p>Performs the reduction operation and stores the result on ALL the MPI processes. Works differently based on the data type:</p> <ul> <li>numpy arrays: returns a numpy array of the same shape as the input        array</li> <li>torch tensors: returns a torch tensors of the same shape, dtype and        on the same device as the input array</li> <li>other types with length: returns the same type with the same length,        each entry is the product of the reduction operation specified with        other entries at the same index.</li> <li>other: standard reduction implementation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Data to be reduced.</p> required <code>op</code> <code>OP</code> <p>Enumerator specifying the reduction operation.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Data of the same type and the same shape, where each entry is            the product of the reduction operation specified.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef allreduce(\n    data: Any,\n    op: OP,\n) -&gt; Any:\n    \"\"\"Performs the reduction operation and stores the result on ALL the MPI\n    processes. Works differently based on the data type:\n\n    - numpy arrays: returns a numpy array of the same shape as the input\\\n    array\n    - torch tensors: returns a torch tensors of the same shape, dtype and\\\n    on the same device as the input array\n    - other types with length: returns the same type with the same length,\\\n    each entry is the product of the reduction operation specified with\\\n    other entries at the same index.\n    - other: standard reduction implementation.\n\n    Args:\n        data (Any): Data to be reduced.\n        op (OP): Enumerator specifying the reduction operation.\n\n    Returns:\n        Any: Data of the same type and the same shape, where each entry is\\\n        the product of the reduction operation specified.\n    \"\"\"\n\n    if isinstance(data, np.ndarray):\n        shape = data.shape\n        result = np.empty(data.size, dtype=data.dtype)\n        MPI.COMM_WORLD.Allreduce(data.flatten(), result, op=op.value)\n        return result.reshape(shape)\n\n    if isinstance(data, torch.Tensor):\n        data_np = data.numpy().astype(np.float32)\n        shape = data_np.shape\n        result = np.empty(data_np.size, dtype=np.float32)\n        MPI.COMM_WORLD.Allreduce(data_np.flatten(), result, op=op.value)\n        return torch.tensor(\n            result.reshape(shape), dtype=data.dtype, device=data.device\n        )\n\n    if hasattr(data, \"__len__\") and hasattr(data, \"dtype\"):\n        result = np.empty(len(data), dtype=data.dtype)\n        MPI.COMM_WORLD.Allreduce(data, result, op=op.value)\n        return result\n\n    # scalar value\n    return MPI.COMM_WORLD.allreduce(data, op=op.value)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.assert_equality","title":"assert_equality  <code>staticmethod</code>","text":"<pre><code>assert_equality(value: Any) -&gt; None\n</code></pre> <p>Gathers values from all MPI processes to the master process and checks if all values are equal.</p> <p>Calls MPI.Abort when some values are different.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>A value to be compared.</p> required Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef assert_equality(value: Any) -&gt; None:\n    \"\"\"Gathers values from all MPI processes to the master process and\n    checks if all values are equal.\n\n    Calls MPI.Abort when some values are different.\n\n    Args:\n        value (Any): A value to be compared.\n    \"\"\"\n    values = mpi.gather(value, root=0)\n    # Logger.warning(f'EQUALITY_CHECK: {value}')\n    if mpi.get_rank() == 0:\n        # mpi.print(values)\n        for v in values[1:]:\n            if v != values[0]:\n                # for idx, w in enumerate(v):\n                #     Logger.warning(f'{values[0][idx]} x {w}')\n\n                msg = \"\\n\"\n                for p in range(mpi.get_world_size()):\n                    msg += f\"process[{p:3d}] \"\n                    for val in values[p]:\n                        msg += f\"{val:30s} \"\n                mpi.abort(\n                    0, f\"Equality test failed, gathered values: {msg}\"\n                )\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.broadcast","title":"broadcast  <code>staticmethod</code>","text":"<pre><code>broadcast(data: Any, root: int = 0) -&gt; Any\n</code></pre> <p>A simple broadcast wrapper using the underlying pickled broadcast version.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Single value or a list of values to be broadcast from            root to the rest of the MPI group</p> required <code>root</code> <code>int</code> <p>Index of the root process. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Returns the data on all MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef broadcast(\n    data: Any,\n    root: int = 0,\n) -&gt; Any:\n    \"\"\"A simple broadcast wrapper using the underlying pickled broadcast\n    version.\n\n    Args:\n        data (Any): Single value or a list of values to be broadcast from\\\n        root to the rest of the MPI group\n        root (int): Index of the root process. Defaults to 0.\n\n    Returns:\n        Any: Returns the data on all MPI processes.\n    \"\"\"\n    data = MPI.COMM_WORLD.bcast(data, root=root)\n    return data\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.cleanup","title":"cleanup  <code>staticmethod</code>","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Cleans up the MPI distributed environment.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef cleanup() -&gt; None:\n    \"\"\"Cleans up the MPI distributed environment.\"\"\"\n    torch.distributed.destroy_process_group()\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.gather","title":"gather  <code>staticmethod</code>","text":"<pre><code>gather(data: Any, root: int = 0) -&gt; list[Any]\n</code></pre> <p>Gathers data from all MPI processes and creates a vector out of them.</p> <p>TODO: seems complicated, maybe the underlying pickled implementation        can be utilized.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Single value</p> required <code>root</code> <code>int</code> <p>The rank of the process on which to gather the list of            values.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A list of values, where the i-th value corresponds to            the value of data on the i-th MPI process.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef gather(data: Any, root: int = 0) -&gt; list[Any]:\n    \"\"\"Gathers data from all MPI processes and creates a vector out of\n    them.\n\n    TODO: seems complicated, maybe the underlying pickled implementation\\\n    can be utilized.\n\n    Args:\n        data (Any): Single value\n        root (int): The rank of the process on which to gather the list of\\\n        values.\n\n    Returns:\n        list[Any]: A list of values, where the i-th value corresponds to\\\n        the value of data on the i-th MPI process.\n    \"\"\"\n\n    if isinstance(data, np.ndarray):\n        sendcounts = np.array(mpi._comm.gather(data.size, root))\n        shape = data.shape[1:]\n\n        if mpi._rank == root:\n            recvbuf = np.empty(sum(sendcounts), dtype=data.dtype)\n        else:\n            recvbuf = None\n\n        # print(f'[{mpi._rank}]sendcounts: {sendcounts}, data.size: {data.size}', data.shape, shape)\n        mpi._comm.Gatherv(\n            sendbuf=data.reshape(-1),\n            recvbuf=(recvbuf, sendcounts),\n            root=root,\n        )\n\n        if mpi._rank == root:\n            out = []\n            index_shift = 0\n            for shift in sendcounts:\n                out.append(\n                    recvbuf[index_shift : index_shift + shift].reshape(\n                        (-1,) + shape\n                    )\n                )\n                index_shift += shift\n            return out\n        return None\n\n    if hasattr(data, \"__len__\"):\n        pickled_data = np.frombuffer(pickle.dumps(data), dtype=np.uint8)\n        len_ = len(pickled_data)\n        sendcounts = np.array(mpi._comm.gather(len_, root))\n\n        if mpi._rank == root:\n            recvbuf = np.empty(sum(sendcounts), dtype=np.uint8)\n        else:\n            recvbuf = None\n\n        mpi._comm.Gatherv(\n            sendbuf=pickled_data, recvbuf=(recvbuf, sendcounts), root=root\n        )\n\n        if mpi._rank == root:\n            out = []\n            index_shift = 0\n            for shift in sendcounts:\n                out.append(\n                    pickle.loads(recvbuf[index_shift : index_shift + shift])\n                )\n                index_shift += shift\n            return out\n        return None\n\n    # scalar value\n    return MPI.COMM_WORLD.gather(data, root)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.get_comm","title":"get_comm  <code>staticmethod</code>","text":"<pre><code>get_comm() -&gt; Intracomm\n</code></pre> <p>Retrieves the global intranode communicator.</p> <p>Returns:</p> Type Description <code>Intracomm</code> <p>MPI.Intracomm: MPI communicator.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef get_comm() -&gt; MPI.Intracomm:\n    \"\"\"Retrieves the global intranode communicator.\n\n    Returns:\n        MPI.Intracomm: MPI communicator.\n    \"\"\"\n\n    if mpi._comm is None:\n        mpi.init()\n\n    return mpi._comm\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.get_rank","title":"get_rank  <code>staticmethod</code>","text":"<pre><code>get_rank() -&gt; int\n</code></pre> <p>Retrieves the global rank of this MPI process.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the global MPI rank.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef get_rank() -&gt; int:\n    \"\"\"Retrieves the global rank of this MPI process.\n\n    Returns:\n        int: the global MPI rank.\n    \"\"\"\n\n    if mpi._comm is None:\n        mpi.init()\n\n    return mpi._rank\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.get_task_timeline","title":"get_task_timeline  <code>staticmethod</code>","text":"<pre><code>get_task_timeline(block_matrix: ndarray) -&gt; Any\n</code></pre> <p>An experimental method designed to compute a communication scheme for a given computational block matrix.</p> <p>Assume we are tasked to perform some work on data arranged into a 2D        grid. This grid can be segmented into PxP block matrix M        (computational block matrix). Let k = M[i, j], i.e. the work on the        grid in the block with row index i and column index j is supposed to be        calculated by MPI process k.</p> <p>Furthermore, assume that for performing the calculations required by        M[i, j], we need data from processes i and j. The goal of this method        is to calculate a consistent sequence of computational and        communication tasks so that the work is computed as prescribed with the        least amount of waiting on communication.</p> <p>Parameters:</p> Name Type Description Default <code>block_matrix</code> <code>ndarray</code> <p>The computational block matrix with P            rows and P columns with integer values.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>description</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef get_task_timeline(\n    block_matrix: np.ndarray,\n) -&gt; Any:\n    \"\"\"An experimental method designed to compute a communication scheme\n    for a given computational block matrix.\n\n    Assume we are tasked to perform some work on data arranged into a 2D\\\n    grid. This grid can be segmented into PxP block matrix M\\\n    (computational block matrix). Let k = M[i, j], i.e. the work on the\\\n    grid in the block with row index i and column index j is supposed to be\\\n    calculated by MPI process k.\n\n    Furthermore, assume that for performing the calculations required by\\\n    M[i, j], we need data from processes i and j. The goal of this method\\\n    is to calculate a consistent sequence of computational and\\\n    communication tasks so that the work is computed as prescribed with the\\\n    least amount of waiting on communication.\n\n    Args:\n        block_matrix (np.ndarray): The computational block matrix with P\\\n        rows and P columns with integer values.\n\n    Returns:\n        Any: _description_\n    \"\"\"\n\n    task_timeline = []\n    nprocesses = block_matrix.shape[0]\n\n    # a set of computations performed by each MPI process\n    task_compute = [set() for _ in range(nprocesses)]\n\n    # lets constructs the communication graph first\n    # communication_graph[i, j] = 1 means that process i has to send its\n    # data to process j at some point during the computation\n    graph_edges = set()\n    communication_graph = np.zeros((nprocesses, nprocesses), dtype=np.int32)\n    for pi in range(block_matrix.shape[0]):\n        for pj in range(block_matrix.shape[0]):\n            if block_matrix[pi, pj] &lt; 0:\n                continue\n\n            pidx = block_matrix[pi, pj]\n            communication_graph[pi, pidx] = 1\n            communication_graph[pj, pidx] = 1\n            task_compute[pidx].add((pi, pj))\n\n    # why would processes need to send information to themselves?\n    for i in range(nprocesses):\n        communication_graph[i, i] = 0\n\n    # input+output edge degree of the communication graph, for greedy\n    # solution\n    for i in range(nprocesses):\n        for j in range(nprocesses):\n            if communication_graph[i, j] &gt; 0:\n                graph_edges.add((i, j))\n\n    # keeps track of what processes should send/recieve information\n    # to/from other processes. Contains a list of lists T1, T2, ..., Tk\n    # entries in Ti can be performed at the same time.\n    task_timeline_data_exchange = []\n    while len(graph_edges) &gt; 0:\n\n        # edges represent communication that had yet to happen, we pick\n        # the maximal number of disjoint edges (because processes can have\n        # at most one communication channel open at one time)\n        maximal_matching_edges = get_maximal_matching(graph_edges)\n\n        # exclude the found edges from future iterations\n        graph_edges = graph_edges.difference(maximal_matching_edges)\n\n        task = [None for _ in range(nprocesses)]\n        for edge in maximal_matching_edges:\n            task[edge[0]] = (\"send\", edge[1])\n            task[edge[1]] = (\"recieve\", edge[0])\n        task_timeline_data_exchange.append(task)\n\n    # we have a communication timeline, now we need to interleave it with\n    # the compute requirements\n    # initially, each process holds information only about their particular\n    # diagonal block\n    available_data = [\n        [False for _ in range(nprocesses)] for _ in range(nprocesses)\n    ]\n    for i in range(nprocesses):\n        available_data[i][i] = True\n\n    comm_exchange_index = 0\n    while np.max([len(v) for v in task_compute]) &gt; 0:\n        task = [None for _ in range(nprocesses)]\n        # we try to find each process a work to do\n        # find at most one block to compute\n        for pidx in range(nprocesses):\n            Q = None\n            for T in task_compute[pidx]:\n                u = T[0]\n                v = T[1]\n                if available_data[pidx][u] and available_data[pidx][v]:\n                    Q = T\n                    break\n            if Q is not None:\n                task_compute[pidx].remove(Q)\n                task[pidx] = (\"compute\", Q)\n\n        # first we compute\n        task_timeline.append(task)\n\n        # there is nothing to communicate\n        if comm_exchange_index &gt;= len(task_timeline_data_exchange):\n            break\n        task = task_timeline_data_exchange[comm_exchange_index]\n        comm_exchange_index += 1\n\n        # then we communicate\n        task_timeline.append(task)\n\n        for pidx in range(nprocesses):\n            T = task[pidx]\n            if T is None:\n                continue\n\n            if T[0] == \"recieve\":\n                available_data[pidx][T[1]] = True\n\n    for T in task_timeline:\n        task = \"| \"\n        for i in range(nprocesses):\n            if T[i] is None:\n                task += \"--------------- | \"\n            elif T[i][0] == \"compute\":\n                task += f\"{T[i][0]:7s}[{T[i][1][0]:2d}][{T[i][1][1]:2d}] | \"\n            else:\n                task += f\"{T[i][0]:7s}    [{T[i][1]:2d}] | \"\n        Logger.warning(task)\n\n    return task_timeline\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.get_world_size","title":"get_world_size  <code>staticmethod</code>","text":"<pre><code>get_world_size() -&gt; int\n</code></pre> <p>Retrieves the total number of MPI processes in this communicator context.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef get_world_size() -&gt; int:\n    \"\"\"Retrieves the total number of MPI processes in this communicator\n    context.\n\n    Returns:\n        int: The number of MPI processes.\n    \"\"\"\n\n    if mpi._comm is None:\n        mpi.init()\n\n    return mpi._size\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.init","title":"init  <code>staticmethod</code>","text":"<pre><code>init() -&gt; None\n</code></pre> <p>Initializes the MPI environment and associated internal parameters.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef init() -&gt; None:\n    \"\"\"Initializes the MPI environment and associated internal parameters.\"\"\"\n    if mpi._comm is not None:\n        return\n\n    mpi._comm = MPI.COMM_WORLD\n    mpi._rank = mpi._comm.Get_rank()\n    mpi._size = mpi._comm.Get_size()\n    mpi._is_master = mpi._rank == 0\n    mpi._synchronization_counter = 0\n    mpi._local_rank = 0\n    mpi._nGPUs = 0\n\n    if \"OMPI_COMM_WORLD_LOCAL_RANK\" in os.environ:\n        mpi._local_rank = int(os.environ[\"OMPI_COMM_WORLD_LOCAL_RANK\"])\n        mpi._nGPUs = int(os.environ[\"OMPI_COMM_WORLD_LOCAL_SIZE\"])\n        os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n        os.environ[\"MASTER_PORT\"] = \"6108\"\n\n    if torch.cuda.is_available():\n        mpi._device = torch.device(\"cuda:{}\".format(mpi._local_rank))\n        torch.cuda.set_device(mpi._device)\n        torch.distributed.init_process_group(\n            backend=\"nccl\", rank=mpi._rank, world_size=mpi._size\n        )\n    else:\n        mpi._nGPUs = 0\n        mpi._local_rank = 0\n        mpi._device = torch.device(\"cpu\")\n        torch.distributed.init_process_group(\n            backend=\"gloo\", rank=mpi._rank, world_size=mpi._size\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.is_master","title":"is_master  <code>staticmethod</code>","text":"<pre><code>is_master() -&gt; bool\n</code></pre> <p>Returns True when this process is the master process in this communicator context.</p> <p>otherwise returns False.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True when master, False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef is_master() -&gt; bool:\n    \"\"\"Returns True when this process is the master process in this\n    communicator context.\n\n    otherwise returns False.\n\n    Returns:\n        bool: True when master, False otherwise.\n    \"\"\"\n\n    return mpi._is_master\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.iterator","title":"iterator  <code>staticmethod</code>","text":"<pre><code>iterator(obj: Iterable) -&gt; Iterable\n</code></pre> <p>Returns an iterator over the iterable object such that all MPI processes retrieve a subset of the object's entries.</p> <p>Assumes ALL MPI processes call this method on the same object, containing ALL the data.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Iterable</code> <p>An iterable object.</p> required <p>Returns:</p> Name Type Description <code>Iterable</code> <code>Iterable</code> <p>a generator used to iterate over the object in a            distributed manner.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef iterator(obj: Iterable) -&gt; Iterable:\n    \"\"\"Returns an iterator over the iterable object such that all MPI\n    processes retrieve a subset of the object's entries.\n\n    Assumes ALL MPI processes call this method on the same object,\n    containing ALL the data.\n\n    Args:\n        obj (Iterable): An iterable object.\n\n    Returns:\n        Iterable: a generator used to iterate over the object in a\\\n        distributed manner.\n    \"\"\"\n    return obj[mpi.get_rank() :: mpi.get_world_size()]\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.make_dir","title":"make_dir  <code>staticmethod</code>","text":"<pre><code>make_dir(tgt_dir)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef make_dir(tgt_dir):\n    if mpi._comm is None:\n        mpi.init()\n\n    if mpi.get_rank() == 0:\n        os.makedirs(tgt_dir, exist_ok=True)\n\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.print","title":"print  <code>staticmethod</code>","text":"<pre><code>print(*args, end='\\n')\n</code></pre> <p>Prints information only on master rank.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef print(*args, end=\"\\n\"):\n    \"\"\"Prints information only on master rank.\"\"\"\n    if mpi._comm is None:\n        mpi.init()\n\n    if mpi.get_rank() == 0:\n        print(*args, end=end)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.reduce","title":"reduce  <code>staticmethod</code>","text":"<pre><code>reduce(data: Any, op: OP, root: int = 0) -&gt; Any\n</code></pre> <p>Performs the reduction operation and stores the result on the specified process. Works differently based on the data type:</p> <ul> <li>numpy arrays: returns a numpy array of the same shape as the input        array</li> <li>torch tensors: returns a torch tensors of the same shape, dtype and        on the same device as the input array</li> <li>other types with length: returns the same type with the same length,        each entry is the product of the reduction operation specified with        other entries at the same index.</li> <li>other: standard reduction implementation.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Data to be reduced.</p> required <code>op</code> <code>OP</code> <p>Enumerator specifying the reduction operation.</p> required <code>root</code> <code>int</code> <p>Target process rank. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Data of the same type and the same shape, where each entry is            the product of the reduction operation specified.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef reduce(\n    data: Any,\n    op: OP,\n    root: int = 0,\n) -&gt; Any:\n    \"\"\"Performs the reduction operation and stores the result on the\n    specified process. Works differently based on the data type:\n\n    - numpy arrays: returns a numpy array of the same shape as the input\\\n    array\n    - torch tensors: returns a torch tensors of the same shape, dtype and\\\n    on the same device as the input array\n    - other types with length: returns the same type with the same length,\\\n    each entry is the product of the reduction operation specified with\\\n    other entries at the same index.\n    - other: standard reduction implementation.\n\n    Args:\n        data (Any): Data to be reduced.\n        op (OP): Enumerator specifying the reduction operation.\n        root (int): Target process rank. Defaults to 0.\n\n    Returns:\n        Any: Data of the same type and the same shape, where each entry is\\\n        the product of the reduction operation specified.\n    \"\"\"\n\n    if isinstance(data, np.ndarray):\n        shape = data.shape\n        result = np.empty(data.size, dtype=data.dtype)\n        MPI.COMM_WORLD.Reduce(\n            data.flatten(), result, op=op.value, root=root\n        )\n        return result.reshape(shape)\n\n    if isinstance(data, torch.Tensor):\n        data_np = data.numpy().astype(np.float32)\n        shape = data_np.shape\n        result = np.empty(data_np.size, dtype=data_np.dtype)\n        MPI.COMM_WORLD.Reduce(\n            data_np.flatten(), result, op=op.value, root=root\n        )\n        return torch.tensor(\n            result.reshape(shape), dtype=data.dtype, device=data.device\n        )\n\n    if hasattr(data, \"__len__\") and hasattr(data, \"dtype\"):\n        result = np.empty(len(data), dtype=data.dtype)\n        MPI.COMM_WORLD.Reduce(data, result, op=op.value, root=root)\n        return result\n\n    # scalar value\n    return MPI.COMM_WORLD.reduce(data, op=op.value, root=root)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.scatter_numpy","title":"scatter_numpy  <code>staticmethod</code>","text":"<pre><code>scatter_numpy(\n    data: list[ndarray], root: int = 0\n) -&gt; ndarray\n</code></pre> <p>Scatters a list of numpy arrays from the root MPI process on all MPI        processes in the context communicator.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[ndarray]</code> <p>List of numpy arrays to be scattered.</p> required <code>root</code> <code>int</code> <p>Rank of the root process, i.e. the process that holds            the data to be scattered around. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: numpy array from the input list.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef scatter_numpy(\n    data: list[np.ndarray],\n    root: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Scatters a list of numpy arrays from the root MPI process on all MPI\\\n    processes in the context communicator.\n\n    Args:\n        data (list[np.ndarray]): List of numpy arrays to be scattered.\n        root (int): Rank of the root process, i.e. the process that holds\\\n        the data to be scattered around. Defaults to 0.\n\n    Returns:\n        np.ndarray: numpy array from the input list.\n    \"\"\"\n\n    if mpi.get_rank() == root:\n        pickled_data = [\n            np.frombuffer(pickle.dumps(D), dtype=np.uint8) for D in data\n        ]\n        sendcounts = [len(D) for D in pickled_data]\n        sendbuff = np.empty(sum(sendcounts), dtype=np.uint8)\n        shift_index = 0\n        for D in pickled_data:\n            shift = len(D)\n            sendbuff[shift_index : shift_index + shift] = D\n            shift_index += shift\n    else:\n        pickled_data = None\n        sendcounts = None\n        sendbuff = None\n\n    sendcounts = mpi.broadcast(sendcounts, root=root)\n\n    recvbuf = np.empty(sendcounts[mpi.get_rank()], dtype=np.uint8)\n\n    mpi._comm.Scatterv(\n        sendbuf=(sendbuff, sendcounts), recvbuf=recvbuf, root=root\n    )\n\n    return pickle.loads(recvbuf)\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.send_recv","title":"send_recv  <code>staticmethod</code>","text":"<pre><code>send_recv(\n    data: Any, target_process: int, source_process: int\n) -&gt; Any\n</code></pre> <p>A 2-in-1 send/recieve wrapper using the underlying pickled send/recv version. Requires that two processes with ranks i, j call this function with the same rank arguments, i.e.: process i calls 'data_j = send_recv(..., i, j)', process j calls 'data_j = send_recv(data_j, i, j)', which sends the data_j from process j to process i</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Any data.</p> required <code>target_process</code> <code>int</code> <p>If my rank is the target_process, I recieve            data from the source_process.</p> required <code>source_process</code> <code>int</code> <p>If my rank is the source_process, I send            data to target_process.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>data.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef send_recv(\n    data: Any,\n    target_process: int,\n    source_process: int,\n) -&gt; Any:\n    \"\"\"A 2-in-1 send/recieve wrapper using the underlying pickled send/recv\n    version. Requires that two processes with ranks i, j call this function\n    with the same rank arguments, i.e.:\n    process i calls 'data_j = send_recv(..., i, j)',\n    process j calls 'data_j = send_recv(data_j, i, j)',\n    which sends the data_j from process j to process i\n\n    Args:\n        data (Any): Any data.\n        target_process (int): If my rank is the target_process, I recieve\\\n        data from the source_process.\n        source_process (int): If my rank is the source_process, I send\\\n        data to target_process.\n\n    Returns:\n        Any: data.\n    \"\"\"\n\n    if mpi.get_rank() == source_process:\n        mpi.get_comm().send(\n            data,\n            dest=target_process,\n            tag=source_process * 4096 + target_process,\n        )\n    elif mpi.get_rank() == target_process:\n        data = mpi.get_comm().recv(\n            source=source_process,\n            tag=source_process * 4096 + target_process,\n        )\n    return data\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.stdout_write","title":"stdout_write  <code>staticmethod</code>","text":"<pre><code>stdout_write(msg: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\ndef stdout_write(msg: str):\n    if mpi._comm is None:\n        mpi.init()\n\n    if mpi.get_rank() == 0:\n        sys.stdout.write(msg)\n        sys.stdout.flush()\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.sync","title":"sync  <code>staticmethod</code>","text":"<pre><code>sync() -&gt; None\n</code></pre> <p>Compares the stack history on all MPI processes to ensure all processes call this function from the same spot. Calls MPI_Barrier.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef sync() -&gt; None:\n    \"\"\"Compares the stack history on all MPI processes to ensure all\n    processes call this function from the same spot. Calls MPI_Barrier.\n    \"\"\"\n    if mpi._comm is None:\n        mpi.init()\n\n    history = []\n    frame = inspect.currentframe().f_back\n    while frame is not None:\n        history.append((frame.f_code.co_filename, frame.f_lineno))\n        frame = frame.f_back\n\n    # Gather all call locations across ranks\n    mpi.assert_equality(history)\n\n    Logger.debug(f\"Calling synchronization with codes: {history}\")\n\n    MPI.COMM_WORLD.barrier()\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.mpi.tqdm","title":"tqdm  <code>staticmethod</code>","text":"<pre><code>tqdm(obj: Any, desc: str) -&gt; Any\n</code></pre> <p>A TQDM progress bar wrapper for convenience purposes. Wraps the object in a tqdm wrapper when called from master.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object which can be iterated over.</p> required <code>desc</code> <code>str</code> <p>Message to be displayed in the progressbar.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>TQDM progress bar if called from master, the original object            otherwise.</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef tqdm(\n    obj: Any,\n    desc: str,\n) -&gt; Any:\n    \"\"\"A TQDM progress bar wrapper for convenience purposes. Wraps the\n    object in a tqdm wrapper when called from master.\n\n    Args:\n        obj (Any): Any object which can be iterated over.\n        desc (str): Message to be displayed in the progressbar.\n\n    Returns:\n        Any: TQDM progress bar if called from master, the original object\\\n        otherwise.\n    \"\"\"\n    if mpi.is_master():\n        return tqdm(obj, desc=desc)\n    return obj\n</code></pre>"},{"location":"api/API%20Reference/src/mpi/#AI4SurrogateModelling.src.mpi.main_loop","title":"main_loop","text":"<pre><code>main_loop(logger_level: LoggerInfoLevel) -&gt; Any\n</code></pre> <p>A decorator for the main program function. It initializes the logging environment, the MPI environment and automatically cleans up the MPI environment upon termination.</p> <p>Parameters:</p> Name Type Description Default <code>logger_level</code> <code>LoggerInfoLevel</code> <p>An enumerator value related to the        information level to be logged during the program execution.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>decorator</p> Source code in <code>AI4SurrogateModelling/src/mpi.py</code> <pre><code>def main_loop(logger_level: LoggerInfoLevel) -&gt; Any:\n    \"\"\"A decorator for the main program function. It initializes the logging\n    environment, the MPI environment and automatically cleans up the MPI\n    environment upon termination.\n\n    Args:\n        logger_level (LoggerInfoLevel): An enumerator value related to the\\\n        information level to be logged during the program execution.\n\n    Returns:\n        Any: decorator\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n\n            mpi.init()\n            Logger.init(\n                mpi_rank=mpi._rank,\n                mpi_size=mpi._size,\n                mpi_is_master=mpi._is_master,\n                logger_level=logger_level,\n            )\n\n            result = func(*args, **kwargs)\n\n            mpi.cleanup()\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/","title":"Module <code>src.terminal_dashboard</code>","text":""},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard","title":"AI4SurrogateModelling.src.terminal_dashboard","text":"<p>Contains modules and functions related to the concurrent information display in the terminal.</p>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard","title":"TrainingDashboard","text":"<pre><code>TrainingDashboard()\n</code></pre> <p>Represents a terminal dashboard displaying progress of the training process.</p> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def __init__(\n    self,\n):\n    self.key_length = 55\n    self.gap = 13\n    self.header = (\n        \" \" * (self.gap + self.key_length)\n        + \"TRAINING\"\n        + \" \" * self.gap\n        + \"TESTING\"\n        + \" \" * self.gap\n        + \"VALIDATION\"\n        + \" \" * self.gap\n    )\n    self.header_length = len(self.header)\n    self.delim = \"-\" * self.header_length\n    self.isCursor_saved = False\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.delim","title":"delim  <code>instance-attribute</code>","text":"<pre><code>delim = '-' * header_length\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.gap","title":"gap  <code>instance-attribute</code>","text":"<pre><code>gap = 13\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.header","title":"header  <code>instance-attribute</code>","text":"<pre><code>header = (\n    \" \" * (gap + key_length)\n    + \"TRAINING\"\n    + \" \" * gap\n    + \"TESTING\"\n    + \" \" * gap\n    + \"VALIDATION\"\n    + \" \" * gap\n)\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.header_length","title":"header_length  <code>instance-attribute</code>","text":"<pre><code>header_length = len(header)\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.isCursor_saved","title":"isCursor_saved  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>isCursor_saved = False\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.key_length","title":"key_length  <code>instance-attribute</code>","text":"<pre><code>key_length = 55\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.refresh","title":"refresh","text":"<pre><code>refresh(\n    *,\n    progress: float,\n    time_per_epoch: float,\n    learning_rate: float,\n    time_remaining: float,\n    criterion_train: float,\n    criterion_test: float = None,\n    criterion_validation: float = None,\n    loss_train: float,\n    loss_test: float = None,\n    loss_validation: float = None,\n    losses_train: dict,\n    losses_test: dict = None,\n    losses_validation: dict = None\n)\n</code></pre> <p>Refreshes the dashboard information display containing the provided  information.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>float</code> <p>Relative progress of the training process.</p> required <code>time_per_epoch</code> <code>float</code> <p>Estimated time per one training epoch.</p> required <code>learning_rate</code> <code>float</code> <p>Last learning rate used by the optimizer.</p> required <code>time_remaining</code> <code>float</code> <p>Estimated remaining time until training                finishes.</p> required <code>criterion_train</code> <code>float</code> <p>Optimality criterion w.r.t. to the                training data.</p> required <code>criterion_test</code> <code>float</code> <p>Optimality criterion w.r.t. to the                testing data.</p> <code>None</code> <code>criterion_validation</code> <code>float</code> <p>Optimality criterion w.r.t. to the                validation data.</p> <code>None</code> <code>loss_train</code> <code>float</code> <p>Loss calculated on the training data.</p> required <code>loss_test</code> <code>float</code> <p>Loss calculated on the testing data.</p> <code>None</code> <code>loss_validation</code> <code>float</code> <p>Loss calculated on the validation data.</p> <code>None</code> <code>losses_train</code> <code>dict</code> <p>Dictionary containing individual training loss                components.</p> required <code>losses_test</code> <code>dict</code> <p>Dictionary containing individual testing loss                components.</p> <code>None</code> <code>losses_validation</code> <code>dict</code> <p>Dictionary containing individual                validation loss components.</p> <code>None</code> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def refresh(\n    self,\n    *,\n    progress: float,\n    time_per_epoch: float,\n    learning_rate: float,\n    time_remaining: float,\n    criterion_train: float,\n    criterion_test: float = None,\n    criterion_validation: float = None,\n    loss_train: float,\n    loss_test: float = None,\n    loss_validation: float = None,\n    losses_train: dict,\n    losses_test: dict = None,\n    losses_validation: dict = None,\n):\n    \"\"\"Refreshes the dashboard information display containing the provided \n    information.\n\n    Args:\n        progress (float): Relative progress of the training process.\n        time_per_epoch (float): Estimated time per one training epoch.\n        learning_rate (float): Last learning rate used by the optimizer.\n        time_remaining (float): Estimated remaining time until training\\\n            finishes.\n        criterion_train (float): Optimality criterion w.r.t. to the\\\n            training data.\n        criterion_test (float): Optimality criterion w.r.t. to the\\\n            testing data.\n        criterion_validation (float): Optimality criterion w.r.t. to the\\\n            validation data.\n        loss_train (float): Loss calculated on the training data.\n        loss_test (float): Loss calculated on the testing data.\n        loss_validation (float): Loss calculated on the validation data.\n        losses_train (dict): Dictionary containing individual training loss\\\n            components.\n        losses_test (dict): Dictionary containing individual testing loss\\\n            components.\n        losses_validation (dict): Dictionary containing individual\\\n            validation loss components.\n    \"\"\"\n    # Logger.warning(losses_train)\n\n    progress_text = f\"{100 * progress:6.2f} [%]\"\n    progress_text_length = len(progress_text)\n    progress_text_length_before = (\n        self.header_length - progress_text_length\n    ) // 2\n    progress_text_length_after = (\n        self.header_length\n        - progress_text_length\n        - progress_text_length_before\n    )\n    progress_text = (\n        \" \" * progress_text_length_before\n        + progress_text\n        + \" \" * progress_text_length_after\n    )\n\n    mpi.print()\n    self._print_progress(\n        progress_text=progress_text,\n        progress=progress,\n    )\n\n    mpi.print(self.delim)\n    mpi.print(self.header)\n\n    mpi.print(self.delim)\n    self._print_values(\n        key=\"Criterion\",\n        value_train=criterion_train,\n        value_test=criterion_test,\n        value_validation=criterion_validation,\n    )\n    self._print_values(\n        key=\"Loss\",\n        value_train=loss_train,\n        value_test=loss_test,\n        value_validation=loss_validation,\n    )\n    mpi.print(self.delim)\n    training_data = parse_dictionary_data(losses_train)\n    testing_data = parse_dictionary_data(losses_test)\n    validation_data = parse_dictionary_data(losses_validation)\n    for key in training_data:\n        if training_data is None:\n            value_train = None\n        else:\n            value_train = training_data[key]\n\n        if testing_data is None:\n            value_test = None\n        else:\n            value_test = testing_data[key]\n\n        if validation_data is None:\n            value_validation = None\n        else:\n            value_validation = validation_data[key]\n\n        self._print_values(\n            key=key,\n            value_train=value_train,\n            value_test=value_test,\n            value_validation=value_validation,\n        )\n\n    mpi.print(self.delim)\n    mpi.print(\n        f'{\"Learning rate\":{self.key_length}s}{\" \"*(len('TRAINING') + len('TESTING') + 2*self.gap)}{learning_rate:{len('VALIDATION') + self.gap}.8f}'\n    )\n    mpi.print(\n        f'{\"Time per epoch\":{self.key_length}s}{\" \"*(len('TRAINING') + len('TESTING') + 2*self.gap)}{time_per_epoch:{len('VALIDATION') + self.gap}.3f} [s]'\n    )\n    mpi.print(\n        f'{\"Remaining time\":{self.key_length}s}{\" \"*(len('TRAINING') + len('TESTING') + 2*self.gap)}{time_remaining:{len('VALIDATION') + self.gap}.3f} [s]'\n    )\n    mpi.print(self.delim)\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.release_cursor","title":"release_cursor","text":"<pre><code>release_cursor()\n</code></pre> <p>Allows the dashboard to be placed somewhere else.</p> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def release_cursor(\n    self,\n):\n    \"\"\"Allows the dashboard to be placed somewhere else.\"\"\"\n    self.isCursor_saved = False\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.TrainingDashboard.set_cursor","title":"set_cursor","text":"<pre><code>set_cursor()\n</code></pre> <p>Sets the cursor for information display at the current cursor position.</p> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def set_cursor(\n    self,\n):\n    \"\"\"Sets the cursor for information display at the current cursor\n    position.\n    \"\"\"\n    if not self.isCursor_saved:\n        mpi.stdout_write(\"\\033[s\")\n        self.isCursor_saved = True\n    else:\n        mpi.stdout_write(\"\\033[u\")\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.flatten_dict","title":"flatten_dict","text":"<pre><code>flatten_dict(d, parent_key='', sep='-')\n</code></pre> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def flatten_dict(d, parent_key=\"\", sep=\"-\"):\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep))\n        else:\n            items.append((new_key, v))\n    return items\n</code></pre>"},{"location":"api/API%20Reference/src/terminal_dashboard/#AI4SurrogateModelling.src.terminal_dashboard.parse_dictionary_data","title":"parse_dictionary_data","text":"<pre><code>parse_dictionary_data(data: dict) -&gt; dict\n</code></pre> <p>Takes the input nested dictionary and non-nested dictionary containing nested paths as keys with corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A nested dictionary to be parsed.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A non-nested dictionary containing the same data as the input.</p> Source code in <code>AI4SurrogateModelling/src/terminal_dashboard.py</code> <pre><code>def parse_dictionary_data(\n    data: dict,\n) -&gt; dict:\n    \"\"\"Takes the input nested dictionary and non-nested dictionary containing\n    nested paths as keys with corresponding values.\n\n    Args:\n        data (dict): A nested dictionary to be parsed.\n\n    Returns:\n        dict: A non-nested dictionary containing the same data as the input.\n    \"\"\"\n    if data is None:\n        return None\n\n    items = flatten_dict(data)\n\n    out = {}\n    for path, value in items:\n        out[path] = value\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/time_monitor/","title":"Module <code>src.time_monitor</code>","text":""},{"location":"api/API%20Reference/src/time_monitor/#AI4SurrogateModelling.src.time_monitor","title":"AI4SurrogateModelling.src.time_monitor","text":"<p>Module containing classes for simple time measurement and monitoring.</p>"},{"location":"api/API%20Reference/src/time_monitor/#AI4SurrogateModelling.src.time_monitor.TimeMonitor","title":"TimeMonitor","text":"<p>A static class containing methods for simple time measurements.</p>"},{"location":"api/API%20Reference/src/time_monitor/#AI4SurrogateModelling.src.time_monitor.TimeMonitor.get","title":"get  <code>staticmethod</code>","text":"<pre><code>get(key: str) -&gt; dict\n</code></pre> <p>Retrieves the elapsed time in seconds for the CPU and WALLTIME clocks with the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the clocks.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>CPU and WALLTIME elapsed times since the            clocks with the specified name were restarted.</p> Source code in <code>AI4SurrogateModelling/src/time_monitor.py</code> <pre><code>@staticmethod\ndef get(\n    key: str,\n) -&gt; dict:\n    \"\"\"Retrieves the elapsed time in seconds for the CPU and WALLTIME clocks\n    with the specified name.\n\n    Args:\n        key (str): Name of the clocks.\n\n    Returns:\n        dict: CPU and WALLTIME elapsed times since the\\\n        clocks with the specified name were restarted.\n    \"\"\"\n    value_cpu = time.perf_counter() - TimeMonitor._time_table_cpu[key]\n    value_wall = time.process_time() - TimeMonitor._time_table_wall[key]\n\n    value_cpu = mpi.allreduce(value_cpu, mpi.OP.MAX)\n    value_wall = mpi.allreduce(value_wall, mpi.OP.MAX)\n\n    return {\"cpu\": value_cpu, \"wall\": value_wall}\n</code></pre>"},{"location":"api/API%20Reference/src/time_monitor/#AI4SurrogateModelling.src.time_monitor.TimeMonitor.increment","title":"increment  <code>staticmethod</code>","text":"<pre><code>increment(key: str, value: float) -&gt; None\n</code></pre> <p>Increments the internal clocks by the specified amount for the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the clock to be restarted.</p> required <code>value</code> <code>float</code> <p>Amount of time to add to the internal counters.</p> required Source code in <code>AI4SurrogateModelling/src/time_monitor.py</code> <pre><code>@staticmethod\ndef increment(\n    key: str,\n    value: float,\n) -&gt; None:\n    \"\"\"Increments the internal clocks by the specified amount for the given\n    key.\n\n    Args:\n        key (str): Name of the clock to be restarted.\n        value (float): Amount of time to add to the internal counters.\n    \"\"\"\n\n    if key in TimeMonitor._time_table_cpu:\n        TimeMonitor._time_table_cpu[key] += value\n\n    if key in TimeMonitor._time_table_wall:\n        TimeMonitor._time_table_wall[key] += value\n</code></pre>"},{"location":"api/API%20Reference/src/time_monitor/#AI4SurrogateModelling.src.time_monitor.TimeMonitor.start","title":"start  <code>staticmethod</code>","text":"<pre><code>start(key: str) -&gt; None\n</code></pre> <p>Restarts the CPU and WALLTIME clocks with the supplied name.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the clock to be restarted.</p> required Source code in <code>AI4SurrogateModelling/src/time_monitor.py</code> <pre><code>@staticmethod\ndef start(\n    key: str,\n) -&gt; None:\n    \"\"\"Restarts the CPU and WALLTIME clocks with the supplied name.\n\n    Args:\n        key (str): Name of the clock to be restarted.\n    \"\"\"\n\n    value_cpu = time.perf_counter()\n    value_wall = time.process_time()\n\n    TimeMonitor._time_table_cpu[key] = value_cpu\n    TimeMonitor._time_table_wall[key] = value_wall\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/","title":"Module <code>src.conf_rules.main_config</code>","text":""},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config","title":"AI4SurrogateModelling.src.conf_rules.main_config","text":"<p>A module containing the configuration specification of the main configuration script.</p>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main configuration specification. Expects the main configuration file to contain the fields 'main', 'aliases' and 'operations'.</p>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.Config.aliases","title":"aliases  <code>instance-attribute</code>","text":"<pre><code>aliases: dict[str, Any]\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.Config.main","title":"main  <code>instance-attribute</code>","text":"<pre><code>main: MainConfig\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.Config.operations","title":"operations  <code>instance-attribute</code>","text":"<pre><code>operations: dict[str, str]\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.Config.check_operation_keys","title":"check_operation_keys","text":"<pre><code>check_operation_keys(v)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/conf_rules/main_config.py</code> <pre><code>@field_validator(\"operations\")\ndef check_operation_keys(cls, v):\n    OperationsHub.validate_keys(v)\n    return v\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.MainConfig","title":"MainConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration specifying the overall settings of the program. It constrols the level of logging, where the logs should be stored and what type of parallelization should be used.</p>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.MainConfig.logging","title":"logging  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logging: Literal[\"info\", \"debug\"] = Field(\n    \"info\", description=\"Logging level (info/debug)\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.MainConfig.logging_directory","title":"logging_directory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logging_directory: str = Field(\n    \"logs\", description=\"Directory for logging output\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.MainConfig.parallelization","title":"parallelization  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parallelization: Literal[\"distributed\", \"isolated\"] = Field(\n    \"distributed\", description=\"Parallelization mode\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.MainConfig.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int = Field(\n    42, description=\"Seed for reproducibility\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.OperationsHub","title":"OperationsHub","text":"<p>               Bases: <code>RootModel[dict[str, str]]</code></p> <p>Specifies how the sequence of operations should be performed.</p>"},{"location":"api/API%20Reference/src/conf_rules/main_config/#AI4SurrogateModelling.src.conf_rules.main_config.OperationsHub.validate_keys","title":"validate_keys  <code>classmethod</code>","text":"<pre><code>validate_keys(value: dict)\n</code></pre> <p>Goes through the operation keys and checks if they have the correct  naming convention and correct ordering.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>dict</code> <p>A dictionary containing the operation keys and files                to load the specific operation configurations.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws this error when the operation key has incorrect                naming.</p> Source code in <code>AI4SurrogateModelling/src/conf_rules/main_config.py</code> <pre><code>@classmethod\ndef validate_keys(cls, value: dict):\n    \"\"\"Goes through the operation keys and checks if they have the correct \n    naming convention and correct ordering.\n\n    Args:\n        value (dict): A dictionary containing the operation keys and files\\\n            to load the specific operation configurations.\n\n    Raises:\n        ValueError: Throws this error when the operation key has incorrect\\\n            naming.\n    \"\"\"\n    keys = []\n    global_2_local_key_map = {}\n    for key in value.keys():\n        if not key.startswith(\"operation_\"):\n            msg = f\"Invalid operation key: {key}\"\n            Logger.warning(msg)\n            raise ValueError(msg)\n        try:\n            tmp_ = key.split(\"_\")\n            if len(tmp_) &gt; 2:\n                msg = f\"Invalid operation key: {key}. Expected: operation_N, where N is an integer &gt;= 0\"\n                Logger.warning(msg)\n                raise ValueError(msg)\n\n            operation_idx = int(tmp_[-1])\n\n            # check the length of the operation for flawless operation key\n            # reconstruction elsewhere in the code\n            str_len = len(tmp_[-1].strip())\n            if operation_idx == 0:\n                N_len = 1\n            else:\n                N_len = int(np.floor(np.log10(operation_idx))) + 1\n\n            if N_len &lt; str_len:\n                msg = f'Operation key in the form \"operation_N\" requires that \"N\" is the shortest representation of that integer, i.e. 5 is OK, 00005 is not.'\n                raise ValueError(msg)\n\n            global_2_local_key_map[key] = len(keys)\n            keys.append(operation_idx)\n\n        except ValueError as e:\n            Logger.warning(e)\n            raise ValueError(e)\n\n        except:\n            msg = f\"Invalid operation key: {key}. Expected: operation_N, where N is an integer &gt;= 0\"\n            Logger.warning(msg)\n            raise ValueError(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/","title":"Module <code>src.conf_rules.operation_config</code>","text":""},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config","title":"AI4SurrogateModelling.src.conf_rules.operation_config","text":"<p>A module containing operation configuration file validation. Each operation comprises of a set of objects to be initialized and a set of actions to be called on some existing objects.</p>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition","title":"ActionDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines the configuration validator for the action sequence to be perfomed in the configuration file.</p>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition.fname","title":"fname  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fname: str = Field(\n    description=\"A class member function to be called.\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition.object_uid","title":"object_uid  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>object_uid: str = Field(\n    None,\n    description=\"A unique ID of the object to be manipulated with.\",\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Dict[str, Any] = Field(\n    {},\n    description=\"Parameters to be passed into the function.\",\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition.validate_actions","title":"validate_actions  <code>classmethod</code>","text":"<pre><code>validate_actions(*, actions: dict, operation_tasks: dict)\n</code></pre> <p>Validates the series of actions in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>dict</code> <p>A dictionary containing action keys, objects and                object functions with their parameters to be called in each                action.</p> required <code>operation_tasks</code> <code>dict</code> <p>To be filled with action indices.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>This is raised when the actions do not satisfy the                naming convention 'action_N' with N being an integer.</p> <code>AttributeError</code> <p>This is raised when the specified function does not                exist as a member function of the object.</p> Source code in <code>AI4SurrogateModelling/src/conf_rules/operation_config.py</code> <pre><code>@classmethod\ndef validate_actions(\n    cls,\n    *,\n    actions: dict,\n    operation_tasks: dict,\n):\n    \"\"\"Validates the series of actions in the configuration file.\n\n    Args:\n        actions (dict): A dictionary containing action keys, objects and\\\n            object functions with their parameters to be called in each\\\n            action.\n        operation_tasks (dict): To be filled with action indices.\n\n    Raises:\n        ValueError: This is raised when the actions do not satisfy the\\\n            naming convention 'action_N' with N being an integer.\n        AttributeError: This is raised when the specified function does not\\\n            exist as a member function of the object.\n    \"\"\"\n    keys = list(actions.keys())\n\n    indices = []\n    for k in keys:\n        if not k.startswith(\"action_\"):\n            msg = f\"Invalid action key: {k}\"\n            Logger.warning(msg)\n            raise ValueError(msg)\n\n        tmp_ = k.split(\"_\")\n        if len(tmp_) &gt; 2:\n            msg = f\"Invalid action key: {k}. Expected: operation_N, where N is an integer &gt;= 0\"\n            Logger.warning(msg)\n            raise ValueError(msg)\n\n        action_index = int(tmp_[-1])\n\n        indices.append(action_index)\n\n        str_len = len(tmp_[-1].strip())\n        if action_index == 0:\n            N_len = 1\n        else:\n            N_len = int(np.floor(np.log10(action_index))) + 1\n\n        if N_len &lt; str_len:\n            msg = f'Action key in the form \"action_N\" requires that \"N\" is the shortest representation of that integer, i.e. 5 is OK, 00005 is not.'\n            raise ValueError(msg)\n\n    indices = sorted(indices)\n\n    for action_index in indices:\n        k = f\"action_{action_index}\"\n\n        fname = actions[k].fname\n        object_uid = actions[k].object_uid\n        parameters = actions[k].parameters\n\n        ncurrent_actions = runtime_state.get_n_actions()\n        runtime_state.add_action(\n            object_uid=object_uid,\n            fname=fname,\n            parameters=parameters,\n        )\n        operation_tasks[\"actions\"].append(ncurrent_actions)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ActionDefinition.validate_object_references","title":"validate_object_references  <code>classmethod</code>","text":"<pre><code>validate_object_references(*, actions: dict, objects: dict)\n</code></pre> <p>Checks if the actions reference objects defined in the configuration hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>dict</code> <p>Action sequence to be validated.</p> required <code>objects</code> <code>dict</code> <p>Objects contained in the same configuration file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>This is raised when the object associated with some                action is not specified in the same configuration file or in                previously parsed configuration files.</p> Source code in <code>AI4SurrogateModelling/src/conf_rules/operation_config.py</code> <pre><code>@classmethod\ndef validate_object_references(\n    cls,\n    *,\n    actions: dict,\n    objects: dict,\n):\n    \"\"\"Checks if the actions reference objects defined in the configuration\n    hierarchy.\n\n    Args:\n        actions (dict): Action sequence to be validated.\n        objects (dict): Objects contained in the same configuration file.\n\n    Raises:\n        ValueError: This is raised when the object associated with some\\\n            action is not specified in the same configuration file or in\\\n            previously parsed configuration files.\n    \"\"\"\n    for action_id, action in actions.items():\n        if action.object_uid is None:\n            continue\n\n        if action.object_uid not in objects:\n            if not runtime_state.contains_uid(action.object_uid):\n                msg = (\n                    f\"Action '{action_id}' references unknown object_id\"\n                    \"'{action.object_uid}'\"\n                )\n                Logger.warning(msg)\n                raise ValueError(msg)\n            else:\n                msg = (\n                    f'Action \"{action_id}\" references an object '\n                    f'\"{action.object_uid}\" from a different configuration file.'\n                )\n                Logger.warning(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ObjectDefinition","title":"ObjectDefinition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains specification for object definition configuration files.</p>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ObjectDefinition.constructor","title":"constructor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constructor: str = Field(\n    description=\"Name of the object class or a function returning an object            instance specified as a path in the current codebase.\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ObjectDefinition.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Dict[str, Any] = Field(\n    {},\n    description=\"A dictionary of parameters supplied to the constructor            of the specified object class.\",\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.ObjectDefinition.validate_objects","title":"validate_objects  <code>classmethod</code>","text":"<pre><code>validate_objects(*, objects: dict, operation_tasks: dict)\n</code></pre> <p>Checks the validity of the objects defined in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>dict</code> <p>Contains unique identifiers, class names and                parameters for each object to be initialized.</p> required <code>operation_tasks</code> <code>dict</code> <p>Is filled with object configurations.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>This is raised when multiple objects with the same                identifier are being specified anywhere in the configuration                hierarchy. It is also raised when an object has an identifier                in the form 'action_N', wehere N is a nonnegative integer.</p> <code>AttributeError</code> <p>This is raised when the object class does not exist.</p> Source code in <code>AI4SurrogateModelling/src/conf_rules/operation_config.py</code> <pre><code>@classmethod\ndef validate_objects(\n    cls,\n    *,\n    objects: dict,\n    operation_tasks: dict,\n):\n    \"\"\"Checks the validity of the objects defined in the configuration file.\n\n    Args:\n        objects (dict): Contains unique identifiers, class names and\\\n            parameters for each object to be initialized.\n        operation_tasks (dict): Is filled with object configurations.\n\n    Raises:\n        ValueError: This is raised when multiple objects with the same\\\n            identifier are being specified anywhere in the configuration\\\n            hierarchy. It is also raised when an object has an identifier\\\n            in the form 'action_N', wehere N is a nonnegative integer.\n        AttributeError: This is raised when the object class does not exist.\n    \"\"\"\n    pattern = re.compile(r\"^action_(\\d+)$\")\n\n    for k, v in objects.items():\n        if runtime_state.contains_uid(k):\n            msg = (\n                f'An object with ID: \"{k}\" is already defined.'\n                \"Check your configuration files.\"\n            )\n            Logger.warning(msg)\n            # raise ValueError(msg)\n\n        if pattern.match(k):\n            msg = (\n                f\"Objects cannot have unique identifiers in the form \"\n                '\"action_N\", where N is a nonnegative integer.'\n                f\"but encountered an object with UID: {k}.\"\n            )\n            Logger.warning(msg)\n            raise ValueError(msg)\n\n        constructor = v.constructor\n        parameters = v.parameters\n        # split into module and object\n\n        runtime_state.add_object_configuration(\n            key=k,\n            constructor=constructor,\n            parameters=parameters,\n        )\n        operation_tasks[\"objects\"].append(k)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.OperationConfig","title":"OperationConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>The main operation configuration validator. Expects the configuration file to contain the 'objects' and 'actions' fields.</p>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.OperationConfig.actions","title":"actions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>actions: Dict[str, ActionDefinition] = Field(\n    {},\n    description=\"A set of actions to be perfomed in this operation.\",\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.OperationConfig.objects","title":"objects  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>objects: Dict[str, ObjectDefinition] = Field(\n    {}, description=\"A set of objects to be constructed.\"\n)\n</code></pre>"},{"location":"api/API%20Reference/src/conf_rules/operation_config/#AI4SurrogateModelling.src.conf_rules.operation_config.OperationConfig.validate_operation","title":"validate_operation  <code>classmethod</code>","text":"<pre><code>validate_operation(field_value)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/conf_rules/operation_config.py</code> <pre><code>@model_validator(mode=\"after\")\n@classmethod\ndef validate_operation(cls, field_value):\n\n    operation_tasks = {\n        \"objects\": [],\n        \"actions\": [],\n    }\n\n    ObjectDefinition.validate_objects(\n        objects=field_value.objects,\n        operation_tasks=operation_tasks,\n    )\n\n    ActionDefinition.validate_actions(\n        actions=field_value.actions,\n        operation_tasks=operation_tasks,\n    )\n    ActionDefinition.validate_object_references(\n        actions=field_value.actions,\n        objects=field_value.objects,\n    )\n\n    runtime_state.add_operation(operation_tasks)\n\n    return field_value\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/","title":"Module <code>src.database.labeled.database_labeled</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled","title":"AI4SurrogateModelling.src.database.labeled.database_labeled","text":"<p>A module containing classes and method usable by all labeled dataset classes, i.e. datasets where inputs and outputs are known beforehand.</p>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled","title":"DatabaseLabeled","text":"<pre><code>DatabaseLabeled(path_tgt: str)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Serves as an interface for child classes.</p> <p>Methods not implemented by children should throw an error.</p> <p>Initializes the database in a target directory. If it exists, loads relevant metadata. If it doesnt exist, creates new directory and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path_tgt</code> <code>str</code> <p>defines a path to the top directory of the database</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    path_tgt: str,\n) -&gt; None:\n    \"\"\"Initializes the database in a target directory. If it exists, loads\n    relevant metadata. If it doesnt exist, creates new directory and\n    metadata.\n\n    Args:\n        path_tgt (str): defines a path to the top directory of the database\n    \"\"\"\n\n    self.path_main = path_tgt\n    self.rank, self.world_size = mpi.get_rank(), mpi.get_world_size()\n\n    self.data_loaded = {}\n    self.data = {}\n    self.local2global_index_map = {}\n    self.global2local_index_map = {}\n\n    self.metadata = self.get_metadata()\n\n    for key in self.get_data_keys():\n        self.data_loaded[key] = False\n        self.data[key] = []\n\n    Logger.info(f'Initializing Database: \"{self.path_main}\"')\n\n    if mpi.get_rank() == 0:\n        os.makedirs(f\"{self.path_main}/data\", exist_ok=True)\n    mpi.sync()\n    self.database_file_controller = ParallelFile(f\"{self.path_main}/data\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = {}\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.data_loaded","title":"data_loaded  <code>instance-attribute</code>","text":"<pre><code>data_loaded = {}\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.database_file_controller","title":"database_file_controller  <code>instance-attribute</code>","text":"<pre><code>database_file_controller = ParallelFile(f\"{path_main}/data\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.global2local_index_map","title":"global2local_index_map  <code>instance-attribute</code>","text":"<pre><code>global2local_index_map = {}\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.local2global_index_map","title":"local2global_index_map  <code>instance-attribute</code>","text":"<pre><code>local2global_index_map = {}\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata = get_metadata()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.path_main","title":"path_main  <code>instance-attribute</code>","text":"<pre><code>path_main = path_tgt\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.add_data","title":"add_data  <code>abstractmethod</code>","text":"<pre><code>add_data(*, importer: Importer) -&gt; None\n</code></pre> <p>An abstract method to be implemented in child classes. Adds data to        this database utilizing the importer object.</p> <p>Parameters:</p> Name Type Description Default <code>importer</code> <code>Importer</code> <p>Importer used to load, parse and retrieve data.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@abstractmethod\ndef add_data(\n    self,\n    *,\n    importer: Importer,\n) -&gt; None:\n    \"\"\"An abstract method to be implemented in child classes. Adds data to\\\n    this database utilizing the importer object.\n\n    Args:\n        importer (Importer): Importer used to load, parse and retrieve data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.add_data_code","title":"add_data_code","text":"<pre><code>add_data_code(data_code: str) -&gt; None\n</code></pre> <p>Adds a new code to the list of database sources.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>data_code</code> <code>str</code> <p>A code of the database being added.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef add_data_code(\n    self,\n    data_code: str,\n) -&gt; None:\n    \"\"\"Adds a new code to the list of database sources.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        data_code (str): A code of the database being added.\n    \"\"\"\n\n    Logger.info(\n        f\"Adding new data source to already parsed sources: {data_code}\"\n    )\n    current_value = self.metadata_get_value(\"data_codes\")\n    current_value.add(data_code)\n    self.metadata_set_value(\"data_codes\", current_value)\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.add_data_key","title":"add_data_key","text":"<pre><code>add_data_key(key: str) -&gt; None\n</code></pre> <p>Adds a new database prefix key to the list of known prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A new key to be added.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef add_data_key(\n    self,\n    key: str,\n) -&gt; None:\n    \"\"\"Adds a new database prefix key to the list of known prefixes.\n\n    Args:\n        key (str): A new key to be added.\n    \"\"\"\n    current_keys = self.metadata_get_value(\"data_keys\")\n    current_keys.add(key)\n    self.metadata_set_value(\"data_keys\", current_keys)\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.add_n_entries","title":"add_n_entries","text":"<pre><code>add_n_entries(value: int) -&gt; None\n</code></pre> <p>Increments the number of currently stored data entries by the supplied amount. Negative value causes a subtraction.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Amount by which to alter the current value.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef add_n_entries(\n    self,\n    value: int,\n) -&gt; None:\n    \"\"\"Increments the number of currently stored data entries by the\n    supplied amount. Negative value causes a subtraction.\n\n    Args:\n        value (int): Amount by which to alter the current value.\n    \"\"\"\n    Logger.info(\n        f\"Setting an internal counter of loaded entries to\\\n        {self.get_n_entries_global()} + {value}\"\n    )\n    self.metadata_set_value(\n        \"nstored_entries\", self.get_n_entries_global() + value\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.add_transformation","title":"add_transformation","text":"<pre><code>add_transformation(f: Callable, kwargs: dict) -&gt; None\n</code></pre> <p>Adds a transformation to a list of transformations performed on the entries in this database.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>A function to be stored in the sequence of            transformations.</p> required <code>kwargs</code> <code>dict</code> <p>Arguments to the transformation function.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef add_transformation(\n    self,\n    f: Callable,\n    kwargs: dict,\n) -&gt; None:\n    \"\"\"Adds a transformation to a list of transformations performed on the\n    entries in this database.\n\n    Args:\n        f (Callable): A function to be stored in the sequence of\\\n        transformations.\n        kwargs (dict): Arguments to the transformation function.\n    \"\"\"\n    kwargs_tmp = {}\n    for k, v in kwargs.items():\n        if callable(v):\n            kwargs_tmp[k] = dill.dumps(v)\n        else:\n            kwargs_tmp[k] = v\n\n    self.metadata_set_values(\n        {\n            \"operations_history\": self.metadata_get_value(\n                \"operations_history\"\n            )\n            + [{\"function\": dill.dumps(f), \"kwargs\": kwargs_tmp}],\n        }\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.append_new_data","title":"append_new_data","text":"<pre><code>append_new_data(\n    data_local: dict[str], source_data_code: str\n)\n</code></pre> <p>Takes the data provided in the data_local dictionary and appends them to this database. Adds the data source code to the list of already known sources.</p> <p>Parameters:</p> Name Type Description Default <code>data_local</code> <code>dict[str]</code> <p>A dictionary containing            (prefix -&gt; data) entries. For each prefix, append the data to the            end of the database with the corresponding prefix key.</p> required <code>source_data_code</code> <code>str</code> <p>Code of the data source.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef append_new_data(\n    self,\n    data_local: dict[str],\n    source_data_code: str,\n):\n    \"\"\"Takes the data provided in the data_local dictionary and appends them\n    to this database. Adds the data source code to the list of already known\n    sources.\n\n    Args:\n        data_local (dict[str]): A dictionary containing\\\n        (prefix -&gt; data) entries. For each prefix, append the data to the\\\n        end of the database with the corresponding prefix key.\n        source_data_code (str): Code of the data source.\n    \"\"\"\n    mpi.sync()\n    n_local = 0\n    for _, v in data_local.items():\n        n_local = max(len(v), n_local)\n    nentries_to_store_local = n_local\n    nentries_to_store_global = mpi.allreduce(\n        nentries_to_store_local, mpi.OP.SUM\n    )\n\n    self.database_file_controller.append(\n        data=data_local,\n        msg=f\"Appending new data entries for keys ({data_local.keys()})\",\n    )\n\n    for key in data_local.keys():\n        self.add_data_key(key)\n\n    self.add_n_entries(nentries_to_store_global)\n    self.add_data_code(source_data_code)\n    self.data_changed()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.clear_database","title":"clear_database","text":"<pre><code>clear_database() -&gt; None\n</code></pre> <p>Deletes all data entries and metadata.</p> <p>Should be called by ALL MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef clear_database(\n    self,\n) -&gt; None:\n    \"\"\"Deletes all data entries and metadata.\n\n    Should be called by ALL MPI processes.\n    \"\"\"\n\n    self.database_file_controller.reset()\n\n    if mpi.is_master():\n        Logger.info(f'Clearing the database \"{self.path_main}\".')\n        shutil.rmtree(self.path_main)\n        os.makedirs(self.path_main)\n\n    self.data_changed()\n    self.metadata = self.get_metadata()\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.contains_data","title":"contains_data","text":"<pre><code>contains_data(data_code: str) -&gt; bool\n</code></pre> <p>Checks whether this database already contains data entries with the        specified data code.</p> <p>Parameters:</p> Name Type Description Default <code>data_code</code> <code>str</code> <p>A code uniquely determining the source of the data.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if data is contained, otherwise returns False.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef contains_data(\n    self,\n    data_code: str,\n) -&gt; bool:\n    \"\"\"Checks whether this database already contains data entries with the\\\n    specified data code.\n\n    Args:\n        data_code (str): A code uniquely determining the source of the data.\n\n    Returns:\n        bool: Returns True if data is contained, otherwise returns False.\n    \"\"\"\n\n    Logger.info(\n        f'This database contains the following data codes:\\\n        \"{self.metadata[\"data_codes\"]}\"'\n    )\n    if data_code in self.metadata[\"data_codes\"]:\n        Logger.info(\n            f'Data with code \"{data_code}\" is already contained in this\\\n            database.'\n        )\n        return True\n\n    Logger.info(\n        f'Data with code \"{data_code}\" is not contained in this database.'\n    )\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.contains_key","title":"contains_key","text":"<pre><code>contains_key(key: str) -&gt; bool\n</code></pre> <p>Determines whether this database contains the specified prefix key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A database prefix key to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the supplied prefix exists in this database, False            otherwise.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef contains_key(\n    self,\n    key: str,\n) -&gt; bool:\n    \"\"\"Determines whether this database contains the specified prefix key.\n\n    Args:\n        key (str): A database prefix key to be checked.\n\n    Returns:\n        bool: True if the supplied prefix exists in this database, False\\\n        otherwise.\n    \"\"\"\n    keys = self.get_data_keys()\n    if key in keys:\n        return True\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.copy_database","title":"copy_database  <code>staticmethod</code>","text":"<pre><code>copy_database(\n    *, database_dir_src: str, database_dir_tgt: str\n) -&gt; None\n</code></pre> <p>Copies the contents of an existing Database into this Database. This Database will be overwritten by this process.</p> <p>Should be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>database_dir_src</code> <code>str</code> <p>Path to the directory of the existing            database</p> required <code>database_dir_tgt</code> <code>str</code> <p>Path to the directory of the new database</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef copy_database(\n    *,\n    database_dir_src: str,\n    database_dir_tgt: str,\n) -&gt; None:\n    \"\"\"Copies the contents of an existing Database into this Database. This\n    Database will be overwritten by this process.\n\n    Should be called by ALL MPI processes.\n\n    Args:\n        database_dir_src (str): Path to the directory of the existing\\\n        database\n        database_dir_tgt (str): Path to the directory of the new database\n    \"\"\"\n\n    if mpi.is_master():\n\n        # check if database exists, if yes, copies the content\n        if DatabaseLabeled.is_database_directory(database_dir_src):\n            if os.path.exists(database_dir_tgt):\n                Logger.warning(\n                    f\"Target directory: '{database_dir_tgt}' already\\\n                    exists, removing contents\"\n                )\n                shutil.rmtree(database_dir_tgt)\n\n            Logger.info(f\"Copying database from '{database_dir_src}'\")\n            shutil.copytree(database_dir_src, database_dir_tgt)\n        else:\n            Logger.info(\n                f\"Path {database_dir_src} does not contain a database,\\\n                doing nothing.\"\n            )\n\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.data_changed","title":"data_changed","text":"<pre><code>data_changed() -&gt; None\n</code></pre> <p>Clears all chaches and supplementary structures. Called after the contents of the database has been effectivelly changed, so when required, the correct data is reloaded and/or recalculated.</p> <p>Must be called by ALL MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef data_changed(\n    self,\n) -&gt; None:\n    \"\"\"Clears all chaches and supplementary structures. Called after the\n    contents of the database has been effectivelly changed,\n    so when required, the correct data is reloaded and/or recalculated.\n\n    Must be called by ALL MPI processes.\n    \"\"\"\n    self.mark_stats_outdated()\n\n    del self.data\n    del self.local2global_index_map\n    del self.global2local_index_map\n\n    self.data_loaded = {}\n    self.data = {}\n    self.local2global_index_map = {}\n    self.global2local_index_map = {}\n\n    for key in self.get_data_keys():\n        self.data_loaded[key] = False\n        self.data[key] = []\n\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.find_databases","title":"find_databases  <code>staticmethod</code>","text":"<pre><code>find_databases(dir_tgt: str) -&gt; list[str]\n</code></pre> <p>Looks inside the specified directory and recursively determines all subdirectories containing a database.</p> <p>If a path A/B/C contains a database, then the recursion stops, i.e. potential databases in A/B/C/* will not be discovered.</p> <p>Parameters:</p> Name Type Description Default <code>dir_tgt</code> <code>str</code> <p>Specifies which directory should be searched for for            the databases.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of strings representing absolute paths to the            databases so they can be used as an argument to a database            constructor</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef find_databases(dir_tgt: str) -&gt; list[str]:\n    \"\"\"Looks inside the specified directory and recursively determines all\n    subdirectories containing a database.\n\n    If a path A/B/C contains a database, then the recursion stops, i.e.\n    potential databases in A/B/C/* will not be discovered.\n\n    Args:\n        dir_tgt (str): Specifies which directory should be searched for for\\\n        the databases.\n\n    Returns:\n        list[str]: A list of strings representing absolute paths to the\\\n        databases so they can be used as an argument to a database\\\n        constructor\n    \"\"\"\n\n    Logger.info(\n        f'Looking for databases in directory \"{dir_tgt}\" and all its\\\n        subdirectories'\n    )\n    # check if the path exists\n    if not os.path.exists(dir_tgt):\n        Logger.info(f'Directory \"{dir_tgt}\" does not exist')\n        return []\n\n    # when it exists, it must be a directory\n    if not os.path.isdir(dir_tgt):\n        Logger.info(f'\"{dir_tgt}\" is not a directory')\n        return []\n\n    working_dir = os.getcwd()\n    # first check if the specified directory itself is a Database\n    if DatabaseLabeled.is_database_directory(dir_tgt):\n        return [f\"{working_dir}/{dir_tgt}\"]\n\n    tmp_ = [\n        f\"{working_dir}/{Path(dir_tgt)}/{d.name}\"\n        for d in Path(dir_tgt).iterdir()\n        if d.is_dir()\n    ]\n    output = []\n    # we iteratively check if the subdirectories contain a database\n    # and if not, include subdirectories instead\n    while len(tmp_) &gt; 0:\n        tmp2_ = []\n        for d_ in tmp_:\n            if DatabaseLabeled.is_database_directory(d_):\n                output.append(d_)\n            else:\n                tmp2_ += [\n                    f\"{d_}/{d.name}\"\n                    for d in Path(d_).iterdir()\n                    if d.is_dir()\n                ]\n        tmp_ = tmp2_\n\n    return output\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_data","title":"get_data","text":"<pre><code>get_data(key: str) -&gt; list[Any]\n</code></pre> <p>Loads (if not loaded) and returns the local list of objects        associated with the supplied database prefix key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A database prefix key to be queried.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: A local list of objects corresponding to the supplied            prefix key.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_data(\n    self,\n    key: str,\n) -&gt; list[Any]:\n    \"\"\"Loads (if not loaded) and returns the local list of objects\\\n    associated with the supplied database prefix key.\n\n    Args:\n        key (str): A database prefix key to be queried.\n\n    Returns:\n        list[Any]: A local list of objects corresponding to the supplied\\\n        prefix key.\n    \"\"\"\n    self.load_database(keys=[key])\n    return self.data[key]\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_data_keys","title":"get_data_keys","text":"<pre><code>get_data_keys() -&gt; list[str]\n</code></pre> <p>Retrives and returns the list of database prefix keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of ALL the prefix keys contained in this database.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_data_keys(\n    self,\n) -&gt; list[str]:\n    \"\"\"Retrives and returns the list of database prefix keys.\n\n    Returns:\n        list[str]: A list of ALL the prefix keys contained in this database.\n    \"\"\"\n    return sorted(self.metadata_get_value(\"data_keys\"))\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_database_path","title":"get_database_path","text":"<pre><code>get_database_path() -&gt; str\n</code></pre> <p>Generates a filename for a file containing the input data.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name of the input file.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_database_path(\n    self,\n) -&gt; str:\n    \"\"\"Generates a filename for a file containing the input data.\n\n    Returns:\n        str: Name of the input file.\n    \"\"\"\n    return f\"{self.path_main}/data\"\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_dataloaders","title":"get_dataloaders  <code>abstractmethod</code>","text":"<pre><code>get_dataloaders(\n    *,\n    train_ratio: float,\n    validation_ratio: float,\n    batch_size: int,\n    dtype: ENUM_Dtypes,\n    inputs_require_gradient: bool = False\n) -&gt; tuple\n</code></pre> <p>An abstract method to be implemented in child classes. The method is supposed to construct a pair of dataloaders ready to be used in a training process in a distributed environment.</p> <p>Parameters:</p> Name Type Description Default <code>train_ratio</code> <code>float</code> <p>A number between 0 and 1, the portion of this            database to be used in training. 0 &lt; train_ratio + validation_ratio            &lt;= 1.</p> required <code>validation_ratio</code> <code>float</code> <p>A number between 0 and 1, the portion of            this database to be used in validation. 0 &lt; train_ratio +            validation_ratio &lt;= 1.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>dtype</code> <code>ENUM_Dtypes</code> <p>Data type of theentries sampled via the provided            dataloaders.</p> required <code>inputs_require_gradient</code> <code>bool</code> <p>Makes it possible to calculate            partial derivatives w.r.t. the inputs. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Training and Validation dataloaders.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@abstractmethod\ndef get_dataloaders(\n    self,\n    *,\n    train_ratio: float,\n    validation_ratio: float,\n    batch_size: int,\n    dtype: Dtypes,\n    inputs_require_gradient: bool = False,\n) -&gt; tuple:\n    \"\"\"An abstract method to be implemented in child classes. The method is\n    supposed to construct a pair of dataloaders ready to be used in a\n    training process in a distributed environment.\n\n    Args:\n        train_ratio (float): A number between 0 and 1, the portion of this\\\n        database to be used in training. 0 &lt; train_ratio + validation_ratio\\\n        &lt;= 1.\n        validation_ratio (float): A number between 0 and 1, the portion of\\\n        this database to be used in validation. 0 &lt; train_ratio +\\\n        validation_ratio &lt;= 1.\n        batch_size (int): Batch size.\n        dtype (Dtypes): Data type of theentries sampled via the provided\\\n        dataloaders.\n        inputs_require_gradient (bool): Makes it possible to calculate\\\n        partial derivatives w.r.t. the inputs. Defaults to False.\n\n    Returns:\n        tuple: Training and Validation dataloaders.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_default_metadata","title":"get_default_metadata","text":"<pre><code>get_default_metadata() -&gt; dict[str:Any]\n</code></pre> <p>Constructs and returns default metadata associated with this database.</p> <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>dict[str: Any]: default empty metadata dictionary.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_default_metadata(\n    self,\n) -&gt; dict[str:Any]:\n    \"\"\"Constructs and returns default metadata associated with this\n    database.\n\n    Returns:\n        dict[str: Any]: default empty metadata dictionary.\n    \"\"\"\n    metadata = {\n        \"description\": \"empty database\",\n        \"data_codes\": set(),\n        \"nstored_entries\": 0,\n        \"operations_history\": [],\n        \"data_keys\": set(),\n    }\n\n    Logger.info(\"Returning default metadata for the MAIN database class.\")\n    return metadata\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_directory_plots","title":"get_directory_plots","text":"<pre><code>get_directory_plots() -&gt; str\n</code></pre> <p>Returns the path to a directory containing the exported figures related to this database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the directory containing all figures.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_directory_plots(\n    self,\n) -&gt; str:\n    \"\"\"Returns the path to a directory containing the exported figures\n    related to this database.\n\n    Returns:\n        str: Path to the directory containing all figures.\n    \"\"\"\n    return f\"{self.path_main}/plots\"\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_directory_stats","title":"get_directory_stats","text":"<pre><code>get_directory_stats(\n    *, key: str, stat_type: ENUM_StatisticsType\n) -&gt; str\n</code></pre> <p>Constructs and returns the directory for the storage of database related statistics.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>database prefix key, e.g. \"inputs\" or \"outputs\".</p> required <code>stat_type</code> <code>ENUM_StatisticsType</code> <p>Enumerator of the statistics type</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the directory used to store the related statistics.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_directory_stats(\n    self,\n    *,\n    key: str,\n    stat_type: StatisticsType,\n) -&gt; str:\n    \"\"\"Constructs and returns the directory for the storage of database\n    related statistics.\n\n    Args:\n        key (str): database prefix key, e.g. \"inputs\" or \"outputs\".\n        stat_type (StatisticsType): Enumerator of the statistics type\n\n    Returns:\n        str: Path to the directory used to store the related statistics.\n    \"\"\"\n\n    if stat_type == StatisticsType.ROW:\n        return f\"{self.path_main}/stats/rows/{key}\"\n\n    if stat_type == StatisticsType.COL:\n        return f\"{self.path_main}/stats/columns/{key}\"\n\n    if stat_type == StatisticsType.NONE:\n        return f\"{self.path_main}/stats/{key}\"\n\n    if stat_type is None:\n        return f\"{self.path_main}/stats\"\n\n    mpi.abort(0, f\"Unknown StatisticsType: {stat_type}\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_label","title":"get_label","text":"<pre><code>get_label(*, key: str, idx: int) -&gt; str\n</code></pre> <p>Retrieves the specified column label in the part of the database with the supplied prefix key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Database prefix key to be queried.</p> required <code>idx</code> <code>int</code> <p>Index of the label to be retrieved.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The label.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_label(\n    self,\n    *,\n    key: str,\n    idx: int,\n) -&gt; str:\n    \"\"\"Retrieves the specified column label in the part of the database with\n    the supplied prefix key.\n\n    Args:\n        key (str): Database prefix key to be queried.\n        idx (int): Index of the label to be retrieved.\n\n    Returns:\n        str: The label.\n    \"\"\"\n    return self.metadata_get_value(\"labels\")[key][idx]\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_label_index","title":"get_label_index","text":"<pre><code>get_label_index(*, label: str, key: str) -&gt; int\n</code></pre> <p>Retrievs the index of the supplied label in the database prefix.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Label to be queried.</p> required <code>key</code> <code>str</code> <p>The label's database prefix key.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Index of the label among all the labels within the same            prefix group.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_label_index(\n    self,\n    *,\n    label: str,\n    key: str,\n) -&gt; int:\n    \"\"\"Retrievs the index of the supplied label in the database prefix.\n\n    Args:\n        label (str): Label to be queried.\n        key (str): The label's database prefix key.\n\n    Returns:\n        int: Index of the label among all the labels within the same\\\n        prefix group.\n    \"\"\"\n\n    for idx, lab in enumerate(self.metadata_get_value(\"labels\")[key]):\n        if lab == label:\n            return idx\n    return -1\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_label_key","title":"get_label_key","text":"<pre><code>get_label_key(label: str) -&gt; str\n</code></pre> <p>Retrieves the database prefix key in which the supplied label is occurring.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Label existing in one of the database prefixes.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Database prefix key containing the supplied label.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_label_key(\n    self,\n    label: str,\n) -&gt; str:\n    \"\"\"Retrieves the database prefix key in which the supplied label is\n    occurring.\n\n    Args:\n        label (str): Label existing in one of the database prefixes.\n\n    Returns:\n        str: Database prefix key containing the supplied label.\n    \"\"\"\n    for label_key, labels in self.metadata_get_value(\"labels\").items():\n        if label in labels:\n            return label_key\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_metadata","title":"get_metadata","text":"<pre><code>get_metadata() -&gt; dict[str:Any]\n</code></pre> <p>Returns metadata associated with this Database. If no metadata exist, constructs an empty metadata structure.</p> <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>dict[str: Any]: Dictionary containing the metadata</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_metadata(\n    self,\n) -&gt; dict[str:Any]:\n    \"\"\"Returns metadata associated with this Database. If no metadata\n    exist, constructs an empty metadata structure.\n\n    Returns:\n        dict[str: Any]: Dictionary containing the metadata\n    \"\"\"\n\n    metadata_path = self.get_metadata_path()\n    if os.path.isfile(metadata_path):\n        if self.rank == 0:\n            with open(metadata_path, \"rb\") as f:\n                metadata = pickle.load(f)\n        else:\n            metadata = None\n        metadata = mpi.broadcast(metadata, root=0)\n    else:\n        metadata = self.get_default_metadata()\n\n    Logger.info(\n        f\"Retrieved METADATA: {Logger.get_result_log_string(metadata)}\"\n    )\n    return metadata\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_metadata_path","title":"get_metadata_path","text":"<pre><code>get_metadata_path() -&gt; str\n</code></pre> <p>Returns the path to the file containing metadata associated with this Database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>path to the metadata file.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_metadata_path(\n    self,\n) -&gt; str:\n    \"\"\"Returns the path to the file containing metadata associated with\n    this Database.\n\n    Returns:\n        str: path to the metadata file.\n    \"\"\"\n    return f\"{self.path_main}/database_metadata.pkl\"\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_n_entries_global","title":"get_n_entries_global","text":"<pre><code>get_n_entries_global() -&gt; int\n</code></pre> <p>Returns the number of currently stored entries in this database.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of currently stored entries in this database</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_n_entries_global(\n    self,\n) -&gt; int:\n    \"\"\"Returns the number of currently stored entries in this database.\n\n    Returns:\n        int: Number of currently stored entries in this database\n    \"\"\"\n\n    return self.metadata_get_value(\"nstored_entries\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_n_entries_local","title":"get_n_entries_local","text":"<pre><code>get_n_entries_local(*, key: str) -&gt; int\n</code></pre> <p>Retrieves the number of dataset entries loaded by this process.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Database prefix key to be queried.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of entries on this MPI process associated with the            prefix key.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_n_entries_local(\n    self,\n    *,\n    key: str,\n) -&gt; int:\n    \"\"\"Retrieves the number of dataset entries loaded by this process.\n\n    Args:\n        key (str): Database prefix key to be queried.\n\n    Returns:\n        int: Number of entries on this MPI process associated with the\\\n        prefix key.\n    \"\"\"\n\n    return len(self.local2global_index_map[key])\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_stats_path","title":"get_stats_path","text":"<pre><code>get_stats_path(\n    *,\n    stat_code_value: tuple[str, ENUM_StatisticsType],\n    key: str\n) -&gt; str\n</code></pre> <p>Provided with the statistics type (either per dataset entry or per dataset category), constructs a relative path to a file containing the specified statistics.</p> <p>Parameters:</p> Name Type Description Default <code>stat_code_value</code> <code>tuple[str, ENUM_StatisticsType]</code> <p>statistics code name            and type.</p> required <code>key</code> <code>str</code> <p>database prefix key, i.g. \"inputs\" or \"outputs\".</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to a file containing the statistic.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_stats_path(\n    self,\n    *,\n    stat_code_value: tuple[str, StatisticsType],\n    key: str,\n) -&gt; str:\n    \"\"\"Provided with the statistics type\n    (either per dataset entry or per dataset category), constructs a\n    relative path to a file containing the specified statistics.\n\n    Args:\n        stat_code_value (tuple[str, StatisticsType]): statistics code name\\\n        and type.\n        key (str): database prefix key, i.g. \"inputs\" or \"outputs\".\n\n    Returns:\n        str: Path to a file containing the statistic.\n    \"\"\"\n    _stat_dir = self.get_directory_stats(\n        stat_type=stat_code_value[1], key=key\n    )\n    return f\"{_stat_dir}/{stat_code_value[0]}.dat\"\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.get_transformation","title":"get_transformation","text":"<pre><code>get_transformation(\n    t: dict[object, dict],\n) -&gt; tuple[Callable, dict]\n</code></pre> <p>Takes the stored transformation and converts it to a callable function and its arguments, then returns it.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>dict[object, dict]</code> <p>A dictionary created via the            add_transformation function, i.e. it contains the \"function\" and            \"kwargs\" fields.</p> required <p>Returns:</p> Type Description <code>tuple[Callable, dict]</code> <p>tuple[Callable, dict]: description</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef get_transformation(\n    self,\n    t: dict[object, dict],\n) -&gt; tuple[Callable, dict]:\n    \"\"\"Takes the stored transformation and converts it to a callable\n    function and its arguments, then returns it.\n\n    Args:\n        t (dict[object, dict]): A dictionary created via the\\\n        add_transformation function, i.e. it contains the \"function\" and\\\n        \"kwargs\" fields.\n\n    Returns:\n        tuple[Callable, dict]: _description_\n    \"\"\"\n    f_ = dill.loads(t[\"function\"])\n    kwargs_ = {}\n    for k, v in t[\"kwargs\"].items():\n        if isinstance(v, bytes):\n            kwargs_[k] = dill.loads(v)\n        else:\n            kwargs_[k] = v\n\n    return f_, kwargs_\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.is_database_directory","title":"is_database_directory  <code>staticmethod</code>","text":"<pre><code>is_database_directory(dir_path: str) -&gt; bool\n</code></pre> <p>A static method which checks whether the provided directory contains a Database or not.</p> <p>Assumes the directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>Path to the directory</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the directory contains a database, False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef is_database_directory(dir_path: str) -&gt; bool:\n    \"\"\"A static method which checks whether the provided directory contains\n    a Database or not.\n\n    Assumes the directory exists.\n\n    Args:\n        dir_path (str): Path to the directory\n\n    Returns:\n        bool: True if the directory contains a database, False otherwise.\n    \"\"\"\n    if os.path.isfile(f\"{dir_path}/database_metadata.pkl\"):\n        return True\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.load_database","title":"load_database","text":"<pre><code>load_database(*, keys: list[str]) -&gt; None\n</code></pre> <p>Loads the database from the disk for each of the provide database        prefixes.</p> <p>Should be called on ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of database prefixes to load.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef load_database(\n    self,\n    *,\n    keys: list[str],\n) -&gt; None:\n    \"\"\"Loads the database from the disk for each of the provide database\\\n    prefixes.\n\n    Should be called on ALL MPI processes.\n\n    Args:\n        keys (list[str]): A list of database prefixes to load.\n    \"\"\"\n\n    keys_to_load = []\n    for key in keys:\n        if key not in self.data_loaded.keys():\n            self.data_loaded[key] = False\n\n        if not self.data_loaded[key]:\n            keys_to_load.append(key)\n\n    if len(keys_to_load) == 0:\n        return\n\n    data_tmp, local2global_index_map_tmp = (\n        self.database_file_controller.load(\n            keys=keys_to_load,\n            msg=f\"Loading database entries for keys: {keys_to_load}\",\n        )\n    )\n\n    for k in keys_to_load:\n        self.data[k] = data_tmp[k]\n        self.local2global_index_map[k] = local2global_index_map_tmp[k]\n\n    self.recalculate_global2local_indexing(keys=keys_to_load)\n\n    for key in keys:\n        self.data_loaded[key] = True\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.load_stats","title":"load_stats","text":"<pre><code>load_stats(\n    *,\n    stat_code_value: tuple[str, ENUM_StatisticsType],\n    data_keys: list[str]\n) -&gt; dict[str:Any]\n</code></pre> <p>Loads the supplied database statistic for the specified database prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>stat_code_value</code> <code>tuple[str, ENUM_StatisticsType]</code> <p>statistics code name            and type.</p> required <code>data_keys</code> <code>list[str]</code> <p>A list of database prefix keys to be loaded.</p> required <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>dict[str: Any]: Calculated statistics in a (key -&gt; value(s))            dictionary.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef load_stats(\n    self,\n    *,\n    stat_code_value: tuple[str, StatisticsType],\n    data_keys: list[str],\n) -&gt; dict[str:Any]:\n    \"\"\"Loads the supplied database statistic for the specified database\n    prefixes.\n\n    Args:\n        stat_code_value (tuple[str, StatisticsType]): statistics code name\\\n        and type.\n        data_keys (list[str]): A list of database prefix keys to be loaded.\n\n    Returns:\n        dict[str: Any]: Calculated statistics in a (key -&gt; value(s))\\\n        dictionary.\n    \"\"\"\n    if not self.stats_calculated(\n        stat_code_value=stat_code_value, data_keys=data_keys\n    ):\n        msg = f'Statistic \"{stat_code_value[0]}\" has not been calculated\\\n        yet and cannot be loaded.'\n        Logger.info(msg)\n        mpi.abort(5, msg)\n\n    if stat_code_value[1] == StatisticsType.COL:\n        output = {}\n        if mpi.get_rank() == 0:\n            # if the stat file exists, we load it and return the contained\n            # value\n            for i, key in enumerate(data_keys):\n                stats_path = self.get_stats_path(\n                    stat_code_value=stat_code_value, key=key\n                )\n                if os.path.isfile(stats_path):\n                    with open(stats_path, \"rb\") as f:\n                        output[key] = pickle.load(f)\n                else:\n                    for j, key2 in enumerate(data_keys[i:]):\n                        k_ = f\"{key}-{key2}\"\n                        stats_path = self.get_stats_path(\n                            stat_code_value=stat_code_value, key=k_\n                        )\n                        if os.path.isfile(stats_path):\n                            with open(stats_path, \"rb\") as f:\n                                output[k_] = pickle.load(f)\n\n        return mpi.broadcast(output, root=0)\n\n    elif stat_code_value[1] == StatisticsType.ROW:\n        output = {}\n        for key in data_keys:\n            stat_dir = self.get_directory_stats(\n                stat_type=stat_code_value[1], key=key\n            )\n            stat_file_path = f\"{stat_dir}/{stat_code_value[0]}\"\n            pio_file = ParallelFile(stat_file_path)\n\n            stats, _ = pio_file.load(\n                keys=[key],\n                msg=f\"Loading statistic: '{stat_code_value[2]}' for keys:\\\n                {[key]}\",\n            )\n            output[key] = stats[key]\n\n        return output\n\n    else:\n        mpi.abort(0, f\"Uknown Statistics Type: {stat_code_value[1]}\")\n\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.mark_stats_outdated","title":"mark_stats_outdated","text":"<pre><code>mark_stats_outdated() -&gt; None\n</code></pre> <p>Deletes all files containing calculated dataset statistics, effectivelly forcing a recalculation when queried.</p> <p>Must be called by ALL MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef mark_stats_outdated(\n    self,\n) -&gt; None:\n    \"\"\"Deletes all files containing calculated dataset statistics,\n    effectivelly forcing a recalculation when queried.\n\n    Must be called by ALL MPI processes.\n    \"\"\"\n\n    Logger.info(\"Removing calculated statistics.\")\n    if mpi.is_master():\n        stat_dir = self.get_directory_stats(stat_type=None, key=None)\n        if os.path.exists(stat_dir):\n            shutil.rmtree(stat_dir)\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.metadata_get_value","title":"metadata_get_value","text":"<pre><code>metadata_get_value(key: str) -&gt; Any\n</code></pre> <p>Retrieves the metadata associated with the specified key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key for the metadata field</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Value of the metadata field</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef metadata_get_value(\n    self,\n    key: str,\n) -&gt; Any:\n    \"\"\"Retrieves the metadata associated with the specified key.\n\n    Args:\n        key (str): Key for the metadata field\n\n    Returns:\n        Any: Value of the metadata field\n    \"\"\"\n\n    if key not in self.metadata:\n        msg = f\"Metada does not contain key '{key}'\"\n        Logger.debug(msg)\n        mpi.abort(2, msg)\n\n    return self.metadata[key]\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.metadata_set_value","title":"metadata_set_value","text":"<pre><code>metadata_set_value(key: str, value: Any) -&gt; None\n</code></pre> <p>Sets a value of a metadata field and stores the updated state in the metadata file.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metadata field</p> required <code>value</code> <code>Any</code> <p>Desired value of the corresponding metadata field</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef metadata_set_value(\n    self,\n    key: str,\n    value: Any,\n) -&gt; None:\n    \"\"\"Sets a value of a metadata field and stores the updated state in the\n    metadata file.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        key (str): Name of the metadata field\n        value (Any): Desired value of the corresponding metadata field\n    \"\"\"\n\n    if key not in self.metadata:\n        self.metadata[key] = value\n        Logger.info(\n            f'Adding new metadata field \"{key}\" with value\\\n            {Logger.get_result_log_string(value)} and saving.'\n        )\n\n    else:\n        value_prev = self.metadata[key]\n        Logger.info(\n            f'Changing the value of an existing metadata field \"{key}\":\\\n            {Logger.get_result_log_string(value_prev)} -&gt;\\\n            {Logger.get_result_log_string(value)} and saving.'\n        )\n        self.metadata[key] = value\n    self.save_metadata()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.metadata_set_values","title":"metadata_set_values","text":"<pre><code>metadata_set_values(values: dict[str:Any]) -&gt; None\n</code></pre> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef metadata_set_values(\n    self,\n    values: dict[str:Any],\n) -&gt; None:\n\n    def deep_update(base_dict, update_dict):\n        for key, value in update_dict.items():\n            if (\n                key in base_dict\n                and isinstance(base_dict[key], dict)\n                and isinstance(value, dict)\n            ):\n                deep_update(base_dict[key], value)\n            else:\n                base_dict[key] = value\n        return base_dict\n\n    \"\"\"Sets multiple values in the metadata property at once. Assumes all\n    MPI processes are called with the same arguments.\n\n    Args:\n        values (dict[str: Any]): Contains a dictionary of key value pairs\\\n        to be set in the metadata property.\n    \"\"\"\n    # tracks whether the metadata has been changed by this call, and if so,\n    # saves the metadata.\n    save = True\n\n    deep_update(self.metadata, values)\n    # for key, value in values.items():\n    #     if key not in self.metadata:\n    #         self.metadata[key] = value\n    #         save = True\n    #         Logger.info(\n    #             f'Adding new metadata field \"{key}\" with value\\\n    #             {Logger.get_result_log_string(value)}'\n    #         )\n\n    #     elif pickle.dumps(self.metadata[key]) != pickle.dumps(value):\n    #         value_prev = self.metadata[key]\n    #         Logger.info(\n    #             f'Changing the value of an existing metadata field \"{key}\":\\\n    #             {Logger.get_result_log_string(value_prev)} -&gt;\\\n    #             {Logger.get_result_log_string(value)}'\n    #         )\n    #         self.metadata[key] = value\n    #         save = True\n\n    if save:\n        Logger.info(\"Saving updated metadata field.\")\n        self.save_metadata()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.prune_columns","title":"prune_columns","text":"<pre><code>prune_columns(\n    kept_columns: dict[str],\n) -&gt; tuple[dict[str], dict[str], dict[str], dict[str]]\n</code></pre> <p>For each database prefix, takes the list of kept columns and removes the rest.</p> <p>Parameters:</p> Name Type Description Default <code>kept_columns</code> <code>dict[str]</code> <p>A            (Prefix -&gt; kept column indices) map.</p> required <p>Returns:</p> Type Description <code>dict[str]</code> <p>dict[str]: Local data with pruned columns.</p> <code>dict[str]</code> <p>dict[str]: New label lists for each database prefix.</p> <code>dict[str]</code> <p>dict[str]: New dimensions for each database prefix            (i.e. number of columns).</p> <code>dict[str]</code> <p>dict[str]: Updated column            transformations.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef prune_columns(\n    self,\n    kept_columns: dict[str],\n) -&gt; tuple[\n    dict[str],\n    dict[str],\n    dict[str],\n    dict[str],\n]:\n    \"\"\"For each database prefix, takes the list of kept columns and removes\n    the rest.\n\n    Args:\n        kept_columns (dict[str]): A\\\n        (Prefix -&gt; kept column indices) map.\n\n    Returns:\n        dict[str]: Local data with pruned columns.\n        dict[str]: New label lists for each database prefix.\n        dict[str]: New dimensions for each database prefix\\\n        (i.e. number of columns).\n        dict[str]: Updated column\\\n        transformations.\n    \"\"\"\n    keys = kept_columns.keys()\n    self.load_database(keys=keys)\n    current_labels = self.metadata_get_value(\"labels\")\n    current_transforms = self.metadata_get_value(\"transforms\")\n\n    subindexing = {\n        k: np.sort([i for i in kept_columns[k]]).astype(int) for k in keys\n    }\n\n    data = {\n        k: [\n            np.array([D[i] for i in subindexing[k]], dtype=np.float32)\n            for D in self.data[k]\n        ]\n        for k in keys\n    }\n    labels = {\n        k: [current_labels[k][i] for i in subindexing[k]] for k in keys\n    }\n    dimensions = {k: len(labels[k]) for k in keys}\n    transforms = {\n        k: (\n            np.array([current_transforms[k][0][i] for i in subindexing[k]]),\n            np.array([current_transforms[k][1][i] for i in subindexing[k]]),\n        )\n        for k in keys\n    }\n\n    return data, labels, dimensions, transforms\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.recalculate_global2local_indexing","title":"recalculate_global2local_indexing","text":"<pre><code>recalculate_global2local_indexing(\n    *, keys: list[str]\n) -&gt; None\n</code></pre> <p>Constructs global -&gt; local map from the known local -&gt; global map.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>List of prefix keys for which the map should be            constructed.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef recalculate_global2local_indexing(\n    self,\n    *,\n    keys: list[str],\n) -&gt; None:\n    \"\"\"Constructs global -&gt; local map from the known local -&gt; global map.\n\n    Args:\n        keys (list[str]): List of prefix keys for which the map should be\\\n        constructed.\n    \"\"\"\n    for key in keys:\n        self.global2local_index_map[key] = {}\n        for i, j in enumerate(self.local2global_index_map[key]):\n            self.global2local_index_map[key][j] = i\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.save_metadata","title":"save_metadata","text":"<pre><code>save_metadata() -&gt; None\n</code></pre> <p>Saves metadata associated with this Database into a binary file via Pickle.</p> <p>Should be called by ALL MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef save_metadata(\n    self,\n) -&gt; None:\n    \"\"\"Saves metadata associated with this Database into a binary file via\n    Pickle.\n\n    Should be called by ALL MPI processes.\n    \"\"\"\n    Logger.info(\n        f\"Saving METADATA: {Logger.get_result_log_string(self.metadata)}\"\n    )\n    if mpi.is_master():\n        with open(self.get_metadata_path(), \"wb\") as f:\n            pickle.dump(self.metadata, f)\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.set_description","title":"set_description","text":"<pre><code>set_description(msg: str) -&gt; None\n</code></pre> <p>Sets the description of this database.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Text of the new description.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef set_description(\n    self,\n    msg: str,\n) -&gt; None:\n    \"\"\"Sets the description of this database.\n\n    Args:\n        msg (str): Text of the new description.\n    \"\"\"\n\n    self.metadata_set_value(\n        \"description\",\n        msg,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.stats_calculated","title":"stats_calculated","text":"<pre><code>stats_calculated(\n    *,\n    stat_code_value: tuple[str, ENUM_StatisticsType],\n    data_keys: list[str]\n) -&gt; dict[str:bool]\n</code></pre> <p>For each database prefix, determines whether the supplied statistic is up-to date or not.</p> <p>Parameters:</p> Name Type Description Default <code>stat_code_value</code> <code>tuple</code> <p>statistics code name and type.</p> required <code>data_keys</code> <code>list[str]</code> <p>Database prefix keys for which to check the            statistics.</p> required <p>Returns:</p> Type Description <code>dict[str:bool]</code> <p>dict[str: bool]: output[prefix] is True when the statistic is up-to            date, False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef stats_calculated(\n    self,\n    *,\n    stat_code_value: tuple[str, StatisticsType],\n    data_keys: list[str],\n) -&gt; dict[str:bool]:\n    \"\"\"For each database prefix, determines whether the supplied statistic\n    is up-to date or not.\n\n    Args:\n        stat_code_value (tuple): statistics code name and type.\n        data_keys (list[str]): Database prefix keys for which to check the\\\n        statistics.\n\n    Returns:\n        dict[str: bool]: output[prefix] is True when the statistic is up-to\\\n        date, False otherwise.\n    \"\"\"\n    calculated = {k: False for k in data_keys}\n    for key in data_keys:\n        stats_path = self.get_stats_path(\n            stat_code_value=stat_code_value, key=key\n        )\n        if os.path.isfile(stats_path):\n            calculated[key] = True\n    return calculated\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(\n    train_ratio: float,\n    test_ratio: float,\n    validation_ratio: float,\n) -&gt; None\n</code></pre> <p>Randomly splits the entries in this database into three disjoint  sets for training, testing and validation. Assumes the training ratio is greater than zero. When the ratios sum up to values greater than one,  clips the values.</p> <p>Parameters:</p> Name Type Description Default <code>train_ratio</code> <code>float</code> <p>What portion of this database should be used                for training.</p> required <code>test_ratio</code> <code>float</code> <p>What portion of this database should be used                for testing.</p> required <code>validation_ratio</code> <code>float</code> <p>What portion of this database should be                used for validation.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When the supplied ratios are lower than zero.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef train_test_split(\n    self,\n    train_ratio: float,\n    test_ratio: float,\n    validation_ratio: float,\n) -&gt; None:\n    \"\"\"Randomly splits the entries in this database into three disjoint \n    sets for training, testing and validation. Assumes the training ratio is\n    greater than zero. When the ratios sum up to values greater than one, \n    clips the values.\n\n    Args:\n        train_ratio (float): What portion of this database should be used\\\n            for training.\n        test_ratio (float): What portion of this database should be used\\\n            for testing.\n        validation_ratio (float): What portion of this database should be\\\n            used for validation.\n\n    Raises:\n        ValueError: When the supplied ratios are lower than zero.\n    \"\"\"\n\n    if train_ratio &lt;= 0 or test_ratio &lt; 0 or validation_ratio &lt; 0:\n        msg = f\"Incorrect database split ratios: {train_ratio}, {test_ratio}, {validation_ratio}\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    Logger.info(\n        f\"Calculating database split for train ratio: {train_ratio}, test ratio: {test_ratio}, validation ratio: {validation_ratio}\"\n    )\n    nglob = self.get_n_entries_global()\n\n    n_train = int(nglob * train_ratio)\n    n_test = int(nglob * test_ratio)\n    n_validation = int(nglob * validation_ratio)\n    if n_train &gt; nglob:\n        n_train = nglob\n        n_test = 0\n        n_validation = 0\n\n    if n_train + n_test &gt; nglob:\n        n_test = nglob - n_train\n        n_validation = 0\n\n    if n_train + n_test + n_validation &gt; nglob:\n        n_validation = nglob - n_train - n_test\n\n    Logger.info(\n        f\"Database will be split into {n_train} training, {n_test} testing and {n_validation} validation samples.\"\n    )\n\n    P = np.random.permutation(nglob)\n    P = mpi.broadcast(P)\n    train_indices = P[:n_train]\n    test_indices = P[n_train : n_train + n_test]\n    validation_indices = P[\n        n_train + n_test : n_train + n_test + n_validation\n    ]\n\n    # store the values into the metadata field\n    self.metadata_set_values(\n        values={\n            \"split_train_indices\": train_indices,\n            \"split_test_indices\": test_indices,\n            \"split_validation_indices\": validation_indices,\n        }\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.update_metadata_value","title":"update_metadata_value","text":"<pre><code>update_metadata_value(key: str, values: dict) -&gt; None\n</code></pre> <p>Retrives the current value of the metadata field with the given key and updates it so it contains the information contained the the values dictionary. That means existing values are overwritten and other values are added. The updated metadata is then saved.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Metadata field key to be updated.</p> required <code>values</code> <code>dict</code> <p>The new state of values for the given metadata field.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef update_metadata_value(\n    self,\n    key: str,\n    values: dict,\n) -&gt; None:\n    \"\"\"Retrives the current value of the metadata field with the given key\n    and updates it so it contains the information contained the the values\n    dictionary. That means existing values are overwritten and other values\n    are added. The updated metadata is then saved.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        key (str): Metadata field key to be updated.\n        values (dict): The new state of values for the given metadata field.\n    \"\"\"\n    current_values = self.metadata_get_value(key)\n    for k in values.keys():\n        current_values[k] = values[k]\n    self.metadata_set_value(key, current_values)\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/database_labeled/#AI4SurrogateModelling.src.database.labeled.database_labeled.DatabaseLabeled.update_stats","title":"update_stats","text":"<pre><code>update_stats(\n    *,\n    stat_code_value: tuple[str, ENUM_StatisticsType],\n    data: dict[str]\n) -&gt; None\n</code></pre> <p>Takes the supplied calculated statistics dictionary and stores them in the corresponding statistics file(s).</p> <p>Should be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>stat_code_value</code> <code>tuple[str, ENUM_StatisticsType]</code> <p>statistics code name            and type.</p> required <code>data</code> <code>dict[str]</code> <p>Calculated statistics in a (key -&gt; value(s))            manner to be saved to disk.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/database_labeled.py</code> <pre><code>@Logger.logged()\ndef update_stats(\n    self,\n    *,\n    stat_code_value: tuple[str, StatisticsType],\n    data: dict[str],\n) -&gt; None:\n    \"\"\"Takes the supplied calculated statistics dictionary and stores them\n    in the corresponding statistics file(s).\n\n    Should be called by ALL MPI processes.\n\n    Args:\n        stat_code_value (tuple[str, StatisticsType]): statistics code name\\\n        and type.\n        data (dict[str]): Calculated statistics in a (key -&gt; value(s))\\\n        manner to be saved to disk.\n    \"\"\"\n    Logger.warning(stat_code_value)\n    calculated_stats = self.stats_calculated(\n        stat_code_value=stat_code_value, data_keys=data.keys()\n    )\n    keys_to_calculate = []\n    for k, v in calculated_stats.items():\n        if not v:\n            keys_to_calculate.append(k)\n\n    if len(keys_to_calculate) == 0:\n        Logger.info(\n            f'Statistic \"{stat_code_value[0]} is already up-to-date for\\\n            keys: {data.keys()}.\"'\n        )\n        return\n\n    for key in keys_to_calculate:\n        stat_dir = self.get_directory_stats(\n            stat_type=stat_code_value[1], key=key\n        )\n        if not os.path.exists(stat_dir) and mpi.is_master():\n            os.makedirs(stat_dir)\n            Logger.info(\n                f'Statistic directory \"{stat_dir}\" does not exist,\\\n                creating...'\n            )\n    mpi.sync()\n\n    if stat_code_value[1] == StatisticsType.COL:\n        if mpi.is_master():\n            for key in keys_to_calculate:\n                with open(\n                    self.get_stats_path(\n                        stat_code_value=stat_code_value, key=key\n                    ),\n                    \"wb\",\n                ) as f:\n                    pickle.dump(data[key], f)\n                    Logger.info(f'Saving statistic \"{stat_code_value[0]}\"')\n        mpi.sync()\n\n    elif stat_code_value[1] == StatisticsType.ROW:\n\n        for key in keys_to_calculate:\n            stat_dir = self.get_directory_stats(\n                stat_type=stat_code_value[1], key=key\n            )\n            stat_file_path = f\"{stat_dir}/{stat_code_value[0]}\"\n            pio_file = ParallelFile(stat_file_path)\n\n        pio_file.update(\n            data={k: data[k] for k in keys_to_calculate},\n            indices_global={\n                k: self.local2global_index_map[k] for k in keys_to_calculate\n            },\n            msg=f\"Storing statistic: '{stat_code_value[2]}' for\\\n            {keys_to_calculate} data\",\n        )\n    else:\n        mpi.abort(0, f\"Uknown Statistics Type: {stat_code_value[1]}\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/aux/","title":"Module <code>src.database.labeled.tabular.aux</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/aux/#AI4SurrogateModelling.src.database.labeled.tabular.aux","title":"AI4SurrogateModelling.src.database.labeled.tabular.aux","text":"<p>A module containing auxilliary methods used in the tabular dataset workflow.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/aux/#AI4SurrogateModelling.src.database.labeled.tabular.aux.stat_function_decorator","title":"stat_function_decorator","text":"<pre><code>stat_function_decorator(func: Callable) -&gt; Callable\n</code></pre> <p>A decorator function checking the presence of arguments for functions used for tabular statistics calculations.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function which calculates the tabular statistic.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>decorator</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/aux.py</code> <pre><code>def stat_function_decorator(\n    func: Callable,\n) -&gt; Callable:\n    \"\"\"A decorator function checking the presence of arguments for functions\n    used for tabular statistics calculations.\n\n    Args:\n        func (Callable): Function which calculates the tabular statistic.\n\n    Returns:\n        Callable: decorator\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        failed = False\n        msg = \"\"\n        if \"tabular_dataset\" not in kwargs.keys():\n            failed = True\n            msg += f\"Function: {func.__name__} is missing a required argument:\\\n            tabular_dataset\\n\"\n\n        if \"keys\" not in kwargs.keys():\n            failed = True\n            msg += f\"Function: {func.__name__} is missing a required argument:\\\n            keys\\n\"\n\n        if failed:\n            Logger.warning(msg)\n            history = []\n            frame = inspect.currentframe().f_back\n            while frame is not None:\n                if (\n                    \"/logging.py\" not in frame.f_code.co_filename\n                    and \"/mpi.py\" not in frame.f_code.co_filename\n                ):\n                    history.append((frame.f_code.co_filename, frame.f_lineno))\n                frame = frame.f_back\n            for H in history:\n                Logger.warning(f\"{H}\")\n            MPI.COMM_WORLD.Abort(0)\n\n        result = func(*args, **kwargs)\n\n        return result\n\n    return wrapper\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/","title":"Module <code>src.database.labeled.tabular.database_tabular</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular","title":"AI4SurrogateModelling.src.database.labeled.tabular.database_tabular","text":"<p>Module containing methods and classes related to the inner workings of Tabular Datasets (Derived from Labeled Datasets)</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular","title":"DatabaseTabular","text":"<pre><code>DatabaseTabular(path_tgt: str)\n</code></pre> <p>               Bases: <code>DatabaseLabeled</code></p> <p>Constructs a database containing Tabular data.</p> <p>Contains methods suitable to Tabular data analysis, filtering/cleaning up noise and plotting functionality.</p> <p>Loads the existing Database. If it does not exist, creates the neccessary directory structures and metadata files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tgt</code> <code>str</code> <p>Path to where the Database is stored</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    path_tgt: str,\n) -&gt; None:\n    \"\"\"Loads the existing Database. If it does not exist, creates the\n    neccessary directory structures and metadata files.\n\n    Args:\n        path_tgt (str): Path to where the Database is stored\n    \"\"\"\n\n    super().__init__(path_tgt)\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.add_data","title":"add_data","text":"<pre><code>add_data(importers: list[ImporterTabular]) -&gt; None\n</code></pre> <p>Uses the provided Importers to parse the Tabular data from the specified file/directory. The Database is then updated. Applies all transformations already included in the history of this database</p> <p>Parameters:</p> Name Type Description Default <code>importers</code> <code>list[ImporterTabular]</code> <p>User provided importer instances for reading new data. Assumes the order of importing does not matter.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef add_data(\n    self,\n    importers: list[ImporterTabular],\n) -&gt; None:\n    \"\"\"Uses the provided Importers to parse the Tabular data from the\n    specified file/directory. The Database is then updated. Applies all\n    transformations already included in the history of this database\n\n    Args:\n        importers (list[ImporterTabular]): User provided importer instances\n            for reading new data. Assumes the order of importing does not\n            matter.\n    \"\"\"\n\n    # if the data has already been parsed, we ignore this request\n    for importer in importers:\n        data_code = importer.get_importer_code()\n        if data_code is not None:\n            if self.contains_data(data_code):\n                continue\n\n        (\n            data_new,\n            labels_new,\n        ) = importer.get_data()\n\n        dimensions = self.metadata_get_value(\"dimensions\")\n        data_keys = list(data_new.keys())\n        for data_key in data_keys:\n            if data_key not in dimensions:\n                dimensions[data_key] = 0\n\n            if dimensions[data_key] == 0:\n                dim = len(labels_new[data_key])\n                self.metadata_set_values(\n                    {\n                        \"labels\": {\n                            data_key: labels_new[data_key],\n                        },\n                        \"dimensions\": {\n                            data_key: dim,\n                        },\n                        \"transforms\": {\n                            data_key: (np.ones(dim), np.zeros(dim)),\n                        },\n                    }\n                )\n        labels_current_dict = self.metadata_get_value(\"labels\")\n\n        # what if the newly added data is correct, but has permuted columns?\n        data = {}\n        Logger.warning(f\"DATA_KEYS: {labels_current_dict}\")\n        for data_key in data_keys:\n            data[data_key] = permute_columns(\n                data_new[data_key],\n                labels_current_dict[data_key],\n                labels_new[data_key],\n            )\n\n        # for transformation in active_transformations:\n        #     # Logger.warning(transformation)\n        #     f, kwargs = self.get_transformation(transformation)\n        #     # kwargs['data'] = data\n        #     for k, v in kwargs.items():\n        #         Logger.warning(k)\n        #     # Logger.warning(kwargs)\n        #     results = f(\n        #         data = data,\n        #         **kwargs,\n        #     )\n\n        #     data = results['data']\n\n        # check for consistency of the newly appended data\n        self.append_new_data(\n            data_local=data,\n            source_data_code=data_code,\n        )\n\n    self.remove_duplicate_rows(\n        duplicate_row_eps=1e-9,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.add_new_column","title":"add_new_column","text":"<pre><code>add_new_column(\n    tgt_data_key: str,\n    new_column_name: str,\n    src_data_columns: list[tuple[str, int]],\n    op_func: Callable,\n) -&gt; None\n</code></pre> <p>Augments the the data of this database by adding a new column as a function of existing columns.</p> <p>Parameters:</p> Name Type Description Default <code>tgt_data_key</code> <code>str</code> <p>The database prefix key for the augmentation            to be added.</p> required <code>new_column_name</code> <code>str</code> <p>The label associated with the newly created            column.</p> required <code>src_data_columns</code> <code>list[tuple[str, int]]</code> <p>A list of database prefix            and index pairs to be used as inputs to the augmentation function.</p> required <code>op_func</code> <code>Callable</code> <p>The augmentation function.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef add_new_column(\n    self,\n    tgt_data_key: str,\n    new_column_name: str,\n    src_data_columns: list[tuple[str, int]],\n    op_func: Callable,\n) -&gt; None:\n    \"\"\"Augments the the data of this database by adding a new column as\n    a function of existing columns.\n\n    Args:\n        tgt_data_key (str): The database prefix key for the augmentation\\\n        to be added.\n        new_column_name (str): The label associated with the newly created\\\n        column.\n        src_data_columns (list[tuple[str, int]]): A list of database prefix\\\n        and index pairs to be used as inputs to the augmentation function.\n        op_func (Callable): The augmentation function.\n    \"\"\"\n\n    current_labels = self.metadata_get_value(\"labels\")\n    current_dimensions = self.metadata_get_value(\"dimensions\")\n    current_transforms = self.metadata_get_value(\"transforms\")\n\n    keys_to_load = set([tgt_data_key])\n    for k, _ in src_data_columns:\n        keys_to_load.add(k)\n\n    self.load_database(keys=list(keys_to_load))\n\n    results = self._create_new_column(\n        data=self.data,\n        tgt_data_key=tgt_data_key,\n        src_data_columns=src_data_columns,\n        op_func=op_func,\n    )\n\n    self.add_transformation(\n        f=self._create_new_column,\n        kwargs={\n            \"tgt_data_key\": tgt_data_key,\n            \"new_column_name\": new_column_name,\n            \"src_data_columns\": src_data_columns,\n            \"op_func\": op_func,\n        },\n    )\n\n    current_labels[tgt_data_key].append(new_column_name)\n    current_dimensions[tgt_data_key] += 1\n    current_transforms[tgt_data_key] = (\n        np.append(current_transforms[tgt_data_key][0], 1),\n        np.append(current_transforms[tgt_data_key][1], 0),\n    )\n    self.update_metadata_value(\n        key=\"labels\",\n        values=current_labels,\n    )\n    self.update_metadata_value(\n        key=\"dimensions\",\n        values=current_dimensions,\n    )\n    self.update_metadata_value(\n        key=\"transforms\",\n        values=current_transforms,\n    )\n\n    self.database_file_controller.update(\n        indices_global={\n            tgt_data_key: self.local2global_index_map[tgt_data_key]\n        },\n        data=results[\"data\"],\n        msg=f\"Updating entries with key: {tgt_data_key} after creating a\\\n        new column: {new_column_name}\",\n    )\n    self.data_changed()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.calculate_statistic","title":"calculate_statistic","text":"<pre><code>calculate_statistic(\n    *,\n    stat_code: ENUM_StatisticsCode_Tabular,\n    data_keys: list[str]\n) -&gt; None\n</code></pre> <p>Method calculating the supplied statistic depending on the data retrieved from the StatCode Enumerator for each of the database prefix keys specified.</p> <p>Parameters:</p> Name Type Description Default <code>stat_code</code> <code>ENUM_StatisticsCode_Tabular</code> <p>Enumerator containing instructions for calculating the statistic.</p> required <code>data_keys</code> <code>list[str]</code> <p>Database prefix keys for which to calculate the statistic.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef calculate_statistic(\n    self,\n    *,\n    stat_code: StatCode,\n    data_keys: list[str],\n) -&gt; None:\n    \"\"\"Method calculating the supplied statistic depending on\n    the data retrieved from the StatCode Enumerator for each\n    of the database prefix keys specified.\n\n    Args:\n        stat_code (StatCode): Enumerator containing instructions for\n            calculating the statistic.\n        data_keys (list[str]): Database prefix keys for which to calculate\n            the statistic.\n    \"\"\"\n    calculated_stats = self.stats_calculated(\n        stat_code_value=stat_code.value,\n        data_keys=data_keys,\n    )\n\n    # which keys of the supplied ones are missing this statistic?\n    keys_to_calculate = []\n    for k, v in calculated_stats.items():\n        if not v:\n            keys_to_calculate.append(k)\n\n    if len(keys_to_calculate) == 0:\n        Logger.info(f\"{stat_code.value[2]} have been already calculated\")\n        return\n\n    Logger.info(\n        f\"'{stat_code.value[2]}' have either not been calculated, or the\\\n        data has since changed and recalculation is required\"\n    )\n\n    values_ = None\n\n    # the the Enumerator contains the required data, we can continue.\n    if len(stat_code.value) &gt;= 5:\n\n        # some statistics may depend on other statistics, we need to\n        # calculate those first\n        if \"dependencies\" in stat_code.value[4].keys():\n            dependencies_calculated = {}\n            for dependency_key, dependency_code in stat_code.value[4][\n                \"dependencies\"\n            ].items():\n                dep_value = self.get_statistic(\n                    stat_code=dependency_code, data_keys=keys_to_calculate\n                )\n                dependencies_calculated[dependency_key] = dep_value\n            stat_code.value[4][\n                \"dependencies_calculated\"\n            ] = dependencies_calculated\n\n        values_ = stat_code.value[3](\n            tabular_dataset=self,\n            keys=keys_to_calculate,\n            **stat_code.value[4],\n        )\n        self.update_stats(stat_code_value=stat_code.value, data=values_)\n\n    else:\n        mpi.abort(\n            1,\n            f\"'{stat_code.value[2]}' calculation not implemented in Tabular\\\n            Database.\",\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.cleanup_columns","title":"cleanup_columns","text":"<pre><code>cleanup_columns(\n    *,\n    keys_to_consider: list[str],\n    correlation_eps: float,\n    variance_eps: float\n) -&gt; None\n</code></pre> <p>Calls a suite of column cleaning methods. As of now, calls two methods, one based on column variance and the other on the column correlation.</p> <p>Parameters:</p> Name Type Description Default <code>keys_to_consider</code> <code>list[str]</code> <p>Database prefix keys for which the            cleaning process is executed.</p> required <code>correlation_eps</code> <code>float</code> <p>Epsilon value for determining when a            column is correlated to other columns.</p> required <code>variance_eps</code> <code>float</code> <p>Epsilon value for determining when a column            has zero variance or not.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef cleanup_columns(\n    self,\n    *,\n    keys_to_consider: list[str],\n    correlation_eps: float,\n    variance_eps: float,\n) -&gt; None:\n    \"\"\"Calls a suite of column cleaning methods. As of now, calls two\n    methods, one based on column variance and the other on the column\n    correlation.\n\n    Args:\n        keys_to_consider (list[str]): Database prefix keys for which the\\\n        cleaning process is executed.\n        correlation_eps (float): Epsilon value for determining when a\\\n        column is correlated to other columns.\n        variance_eps (float): Epsilon value for determining when a column\\\n        has zero variance or not.\n    \"\"\"\n\n    column_indices_A = self._remove_columns_with_no_variance(\n        variance_eps=variance_eps, keys=keys_to_consider\n    )\n    column_indices_B = self._remove_columns_with_high_correlation(\n        correlation_eps=correlation_eps, keys=keys_to_consider\n    )\n\n    removed_columns = {\n        k: column_indices_A[k] | column_indices_B[k]\n        for k in keys_to_consider\n    }\n\n    nremoved = np.sum(\n        [len(removed_columns[key]) for key in keys_to_consider]\n    )\n    if nremoved &gt; 0:\n        labels = self.metadata_get_value(\"labels\")\n        for key in keys_to_consider:\n            if len(removed_columns[key]) &gt; 0:\n                Logger.info(\n                    f\"Removed columns [{key}]:\\\n                    {[(idx, labels[key][idx]) for idx in removed_columns[key]]}\"\n                )\n\n        dimensions = self.metadata_get_value(\"dimensions\")\n        kept_columns = {\n            k: np.sort(\n                list(\n                    set(range(0, dimensions[k])).difference(\n                        removed_columns[k]\n                    )\n                )\n            )\n            for k in keys_to_consider\n        }\n\n        data_tmp, labels_tmp, dimensions_tmp, transforms_tmp = (\n            self.prune_columns(\n                kept_columns=kept_columns,\n            )\n        )\n\n        self.update_metadata_value(\n            key=\"labels\",\n            values=labels_tmp,\n        )\n        self.update_metadata_value(\n            key=\"dimensions\",\n            values=dimensions_tmp,\n        )\n        self.update_metadata_value(\n            key=\"transforms\",\n            values=transforms_tmp,\n        )\n\n        self.database_file_controller.update(\n            indices_global={\n                k: self.local2global_index_map[k] for k in keys_to_consider\n            },\n            data=data_tmp,\n            msg=f\"Updating database for keys: {keys_to_consider}\\\n            after column pruning\",\n        )\n\n        self.data_changed()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.cleanup_rows_iqr","title":"cleanup_rows_iqr","text":"<pre><code>cleanup_rows_iqr(\n    *, keys_to_consider: list[str], iqr_coefficient: float\n) -&gt; None\n</code></pre> <p>Determines which rows fall outside the scope of the IQR test with the supplied iqr_coefficient and removes the rows which fail this test.</p> <p>Parameters:</p> Name Type Description Default <code>keys_to_consider</code> <code>list[str]</code> <p>Database prefix keys for which the            cleaning process is executed.</p> required <code>iqr_coefficient</code> <code>float</code> <p>A value greater than zero determining the            interval from the mean value.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef cleanup_rows_iqr(\n    self,\n    *,\n    keys_to_consider: list[str],\n    iqr_coefficient: float,\n) -&gt; None:\n    \"\"\"Determines which rows fall outside the scope of the IQR test with the\n    supplied iqr_coefficient and removes the rows which fail this test.\n\n    Args:\n        keys_to_consider (list[str]): Database prefix keys for which the\\\n        cleaning process is executed.\n        iqr_coefficient (float): A value greater than zero determining the\\\n        interval from the mean value.\n    \"\"\"\n\n    iqr = self.get_statistic(\n        stat_code=StatCode.IQR, data_keys=keys_to_consider\n    )\n    if iqr is None:\n        Logger.warning(\n            \"Database contains no inputs, skipping pruning of rows via IQR\\\n            analysis.\"\n        )\n        return\n    q1 = self.get_statistic(\n        stat_code=StatCode.P25, data_keys=keys_to_consider\n    )\n    q3 = self.get_statistic(\n        stat_code=StatCode.P75, data_keys=keys_to_consider\n    )\n\n    bound_lo = {}\n    bound_hi = {}\n    local2global_indexing = {}\n    for key in keys_to_consider:\n        bound_lo[key] = q1[key] - iqr_coefficient * iqr[key]\n        bound_hi[key] = q3[key] + iqr_coefficient * iqr[key]\n        local2global_indexing[key] = self.local2global_index_map[key]\n\n    self.load_database(keys=keys_to_consider)\n\n    results = self._remove_rows_IQR(\n        data=self.data,\n        bound_lo=bound_lo,\n        bound_hi=bound_hi,\n        local2global_indexing=local2global_indexing,\n    )\n\n    removed_rows = results[\"removed_rows\"]\n    removed_indices = list(removed_rows)\n\n    Logger.warning(f\"Removing {len(removed_indices)} rows via IQR\")\n    if len(removed_indices) &gt; 0:\n        self.database_file_controller.remove(\n            keys=list(self.get_data_keys()),\n            indices_global=removed_indices,\n            msg=\"Removing entries IQR range\",\n        )\n\n        self.add_n_entries(-len(removed_rows))\n\n        self.data_changed()\n\n    self.add_transformation(\n        f=self._remove_rows_IQR,\n        kwargs={\n            \"bound_lo\": bound_lo,\n            \"bound_hi\": bound_hi,\n        },\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.cleanup_rows_subspace","title":"cleanup_rows_subspace","text":"<pre><code>cleanup_rows_subspace(\n    simplex_points: ndarray,\n    simplices: list[list[int]],\n    simplex_point_labels: list[str],\n    tol: float,\n) -&gt; None\n</code></pre> <p>Cleans up the rows of this database. The criterion for removal is based on the proximity of the row to a set of simplices defined by their coordinates and adjacency. When the shortest distance of the row to any point in the simplices is less then the supplied tolerance, it remains in the set.</p> <p>Not all columns are taken into account for the proximity calculation. The subspace in which the comparison is calculated is defined by the labels of the simplex coordinates. For example, when the dataset contains column labels A, B, C, D and the simplex coordinates contain labels D, B, then the proximity calculation is based off of the columns B, D.</p> <p>Parameters:</p> Name Type Description Default <code>simplex_points</code> <code>ndarray</code> <p>An array of shape (n, m), where n is            the number of simplex coordinates and m is the dimension of each            simplex point.</p> required <code>simplices</code> <code>list[list[int]]</code> <p>An incidence list forming the simplex            set. For example: simplices = [[0, 1, 4]] represents one 3d            simplices with coordinates from the simplex_points parameter at            rows: 0, 1, 4.</p> required <code>simplex_point_labels</code> <code>list[str]</code> <p>Each dimension in the simplex            coordinate system correspond to one column in one of this            database's prefix keys.</p> required <code>tol</code> <code>float</code> <p>The distance treshold.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef cleanup_rows_subspace(\n    self,\n    simplex_points: np.ndarray,\n    simplices: list[list[int]],\n    simplex_point_labels: list[str],\n    tol: float,\n) -&gt; None:\n    \"\"\"Cleans up the rows of this database. The criterion\n    for removal is based on the proximity of the row to a\n    set of simplices defined by their coordinates and\n    adjacency. When the shortest distance of the row to\n    any point in the simplices is less then the supplied\n    tolerance, it remains in the set.\n\n    Not all columns are taken into account for the proximity\n    calculation. The subspace in which the comparison is calculated\n    is defined by the labels of the simplex coordinates. For example,\n    when the dataset contains column labels A, B, C, D and the simplex\n    coordinates contain labels D, B, then the proximity calculation is\n    based off of the columns B, D.\n\n    Args:\n        simplex_points (np.ndarray): An array of shape (n, m), where n is\\\n        the number of simplex coordinates and m is the dimension of each\\\n        simplex point.\n        simplices (list[list[int]]): An incidence list forming the simplex\\\n        set. For example: simplices = [[0, 1, 4]] represents one 3d\\\n        simplices with coordinates from the simplex_points parameter at\\\n        rows: 0, 1, 4.\n        simplex_point_labels (list[str]): Each dimension in the simplex\\\n        coordinate system correspond to one column in one of this\\\n        database's prefix keys.\n        tol (float): The distance treshold.\n    \"\"\"\n    # Ensure points are in numpy array and 2D\n    simplex_points = np.array(simplex_points)\n    simplices = np.array(simplices)\n\n    self.load_database(keys=self.get_data_keys())\n    results = self.remove_rows_simplex_proximity(\n        data=self.data,\n        simplex_points=simplex_points,\n        tol=tol,\n        simplices=simplices,\n        simplex_point_labels=simplex_point_labels,\n        local2global_index_map=self.local2global_index_map[\"inputs\"],\n    )\n\n    self.add_transformation(\n        f=self.remove_rows_simplex_proximity,\n        kwargs={\n            \"simplex_points\": simplex_points,\n            \"tol\": tol,\n            \"simplices\": simplices,\n            \"coordinates_labels\": simplex_point_labels,\n        },\n    )\n\n    removed_rows = results[\"removed_rows\"]\n    removed_rows_global = mpi.allgather(removed_rows)\n    for S in removed_rows_global:\n        removed_rows |= S\n\n    removed_indices = list(removed_rows)\n\n    Logger.warning(\n        f\"Removing {len(removed_indices)} rows via simplex proximity\"\n    )\n    if len(removed_indices) &gt; 0:\n        self.database_file_controller.remove(\n            keys=list(self.get_data_keys()),\n            indices_global=removed_indices,\n            msg=\"Removing entries IQR range\",\n        )\n\n        self.data_changed()\n        self.add_n_entries(-len(removed_indices))\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.convert_external_data_to_csv","title":"convert_external_data_to_csv","text":"<pre><code>convert_external_data_to_csv(\n    *, data: dict, fn: str, delimiter: str\n)\n</code></pre> <p>Takes the user provided data and uses labeling from this database and exports it to a comma separated file using the supplied delimitting character.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing the data to be exported. The                keys in the dictionary must correspond to the keys in this                    database.</p> required <code>fn</code> <code>str</code> <p>Target filename.</p> required <code>delimiter</code> <code>str</code> <p>Delimiting character.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef convert_external_data_to_csv(\n    self,\n    *,\n    data: dict,\n    fn: str,\n    delimiter: str,\n):\n    \"\"\"Takes the user provided data and uses labeling from this database\n    and exports it to a comma separated file using the supplied delimitting\n    character.\n\n    Args:\n        data (dict): Dictionary containing the data to be exported. The\\\n            keys in the dictionary must correspond to the keys in this\\\n                database.\n        fn (str): Target filename.\n        delimiter (str): Delimiting character.\n    \"\"\"\n\n    try:\n\n        keys = sorted(list(data.keys()))\n        n = len(data[keys[0]])\n\n        labels = self.metadata_get_value(\"labels\")\n        dimensions = self.metadata_get_value(\"dimensions\")\n        max_label_length = (\n            np.max([np.max([len(v) for v in labels[k]]) for k in keys]) + 11\n        )\n        label_formatting = \"{:&gt;\" + f\"{max_label_length}s\" + \"}{}\"\n        value_formatting = \"{:&gt;\" + f\"{max_label_length}.12f\" + \"}{}\"\n\n        if mpi.get_rank() == 0:\n            with open(fn, \"w\") as f:\n                for k in keys:\n                    for i in range(dimensions[k]):\n                        f.write(\n                            label_formatting.format(labels[k][i], delimiter)\n                        )\n                f.write(\"\\n\")\n\n                for j in range(n):\n                    for k in keys:\n                        for i in range(dimensions[k]):\n                            f.write(\n                                value_formatting.format(\n                                    data[k][j, i], delimiter\n                                )\n                            )\n                    f.write(\"\\n\")\n\n        mpi.sync()\n        mpi.print(f\"\u2705The CSV file '{fn}' has been written successfully.\")\n    except Exception:\n        mpi.print(f\"\u274cThe writing of CSV file '{fn}' failed.\")\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.convert_to_csv","title":"convert_to_csv","text":"<pre><code>convert_to_csv(\n    *, fn: str, delimiter: str, denormalize: bool = False\n) -&gt; None\n</code></pre> <p>Takes the content of this tabular database and stores it in a comma separated file with the supplied delimiting character.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>Filename of the data to be saved in.</p> required <code>delimiter</code> <code>str</code> <p>Delimiting character to be used.</p> required <code>denormalize</code> <code>bool</code> <p>If true, converts the data back to the original                values.</p> <code>False</code> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef convert_to_csv(\n    self,\n    *,\n    fn: str,\n    delimiter: str,\n    denormalize: bool = False,\n) -&gt; None:\n    \"\"\"Takes the content of this tabular database and stores it in a comma\n    separated file with the supplied delimiting character.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        fn (str): Filename of the data to be saved in.\n        delimiter (str): Delimiting character to be used.\n        denormalize (bool): If true, converts the data back to the original\\\n            values.\n    \"\"\"\n\n    keys = self.get_data_keys()\n    self.load_database(keys=keys)\n\n    if denormalize:\n        data = self.revert_transform_range_external(\n            data=self.data,\n            keys=keys,\n        )\n    else:\n        data = self.data\n\n    n = len(data[keys[0]])\n\n    labels = self.metadata_get_value(\"labels\")\n    dimensions = self.metadata_get_value(\"dimensions\")\n    max_label_length = (\n        np.max([np.max([len(v) for v in labels[k]]) for k in keys]) + 5\n    )\n    label_formatting = \"{:&gt;\" + f\"{max_label_length}s\" + \"}{}\"\n    value_formatting = \"{:&gt;\" + f\"{max_label_length}.6f\" + \"}{}\"\n\n    for pidx in range(mpi.get_world_size()):\n        if pidx == mpi.get_rank():\n            if pidx == 0:\n                with open(fn, \"w\") as f:\n                    for k in keys:\n                        for i in range(dimensions[k]):\n                            f.write(\n                                label_formatting.format(\n                                    labels[k][i], delimiter\n                                )\n                            )\n                    f.write(\"\\n\")\n\n                    for j in range(n):\n                        for k in keys:\n                            for i in range(dimensions[k]):\n                                f.write(\n                                    value_formatting.format(\n                                        data[k][j][i],\n                                        delimiter,\n                                    )\n                                )\n                        f.write(\"\\n\")\n            else:\n                with open(fn, \"a\") as f:\n                    for j in range(n):\n                        for k in keys:\n                            for i in range(dimensions[k]):\n                                f.write(\n                                    value_formatting.format(\n                                        data[k][j][i],\n                                        delimiter,\n                                    )\n                                )\n                        f.write(\"\\n\")\n\n        mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_data_range","title":"get_data_range","text":"<pre><code>get_data_range(key: str) -&gt; ndarray\n</code></pre> <p>Retrives the minimal and maximal values associated with the supplied database prefix key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A database prefix key for which to retrieve the stats.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A tensor fo shape (n, 2), where the first column                represents the minimal values and the second column represent                the maximal values.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_data_range(\n    self,\n    key: str,\n) -&gt; np.ndarray:\n    \"\"\"Retrives the minimal and maximal values associated with the supplied database\n    prefix key.\n\n    Args:\n        key (str): A database prefix key for which to retrieve the stats.\n\n    Returns:\n        np.ndarray: A tensor fo shape (n, 2), where the first column\\\n            represents the minimal values and the second column represent\\\n            the maximal values.\n    \"\"\"\n    minimal_values = self.get_statistic(\n        data_keys=[key],\n        stat_code=StatCode.MIN,\n    )\n    maximal_values = self.get_statistic(\n        data_keys=[key],\n        stat_code=StatCode.MAX,\n    )\n    result = np.hstack(\n        [\n            minimal_values[key].reshape(-1, 1),\n            maximal_values[key].reshape(-1, 1),\n        ]\n    )\n    return result\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_data_ranges","title":"get_data_ranges","text":"<pre><code>get_data_ranges() -&gt; dict[tensor]\n</code></pre> <p>Retrives the minimal and maximal values associated with all database prefix keys.</p> <p>Returns:</p> Type Description <code>dict[tensor]</code> <p>dict[torch.tensor]: A dictionary containing tensors with minimal                and maximal values.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_data_ranges(\n    self,\n) -&gt; dict[torch.tensor]:\n    \"\"\"Retrives the minimal and maximal values associated with all database\n    prefix keys.\n\n    Returns:\n        dict[torch.tensor]: A dictionary containing tensors with minimal\\\n            and maximal values.\n    \"\"\"\n    result = {key: self.get_data_range(key) for key in self.get_data_keys()}\n    return result\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_dataloader_random","title":"get_dataloader_random","text":"<pre><code>get_dataloader_random(\n    *, n: int, batch_size: int, dtype: dtype\n) -&gt; DictionaryDataLoader\n</code></pre> <p>Constructs and returns pytorch Dataloader to be used outside of the  training process for various testing purposes.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <code>dtype</code> <code>dtype</code> <p>Data type of the objects generated by the            dataloader.</p> required <p>Returns:</p> Name Type Description <code>DictionaryDataLoader</code> <code>DictionaryDataLoader</code> <p>Random data uniformly distributed.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_dataloader_random(\n    self,\n    *,\n    n: int,\n    batch_size: int,\n    dtype: torch.dtype,\n) -&gt; DictionaryDataLoader:\n    \"\"\"Constructs and returns pytorch Dataloader to be used outside of the \n    training process for various testing purposes.\n\n    Args:\n        batch_size (int): Size of each batch.\n        dtype (torch.dtype): Data type of the objects generated by the\\\n        dataloader.\n\n    Returns:\n        DictionaryDataLoader: Random data uniformly distributed.\n    \"\"\"\n\n    data_ranges = self.get_data_ranges()\n\n    data_torch = {}\n    for key in data_ranges:\n        T = []\n        for _ in mpi.iterator(range(n)):\n            T.append(torch.rand((1, self.get_dim(key)), device=mpi._device))\n        T = torch.vstack(T)\n        vmin = data_ranges[key][:, 0].reshape(1, -1)\n        vmax = data_ranges[key][:, 1].reshape(1, -1)\n        T = T * (vmax - vmin) + vmin\n\n        data_torch[key] = T.to(dtype)\n\n    dataset = DictionaryDataset(data=data_torch)\n\n    dataloader = DictionaryDataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        device=mpi._device,\n        generator=torch.Generator(device=mpi._device),\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_dataloaders","title":"get_dataloaders","text":"<pre><code>get_dataloaders(\n    *,\n    batch_size: int,\n    dtype: dtype,\n    data_require_gradient: dict\n) -&gt; tuple[\n    DictionaryDataLoader,\n    DictionaryDataLoader,\n    DictionaryDataLoader,\n]\n</code></pre> <p>Constructs and returns pytorch Dataloaders to be used in the training process. Assumes that database split has been called  previously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <code>dtype</code> <code>dtype</code> <p>Data type of the objects generated by the            dataloaders.</p> required <code>data_require_gradient</code> <code>dict</code> <p>Specifies whether the generated                objects should contain gradient information or not.                Defaults to False.</p> required <p>Returns:</p> Type Description <code>tuple[DictionaryDataLoader, DictionaryDataLoader, DictionaryDataLoader]</code> <p>tuple[DictionaryDataLoader, DictionaryDataLoader, DictionaryDataLoader]: Training, Testing and            Validation Dataloaders.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_dataloaders(\n    self,\n    *,\n    batch_size: int,\n    dtype: torch.dtype,\n    data_require_gradient: dict,\n) -&gt; tuple[\n    DictionaryDataLoader, DictionaryDataLoader, DictionaryDataLoader\n]:\n    \"\"\"Constructs and returns pytorch Dataloaders to be used in the\n    training process. Assumes that database split has been called \n    previously.\n\n    Args:\n        batch_size (int): Size of each batch.\n        dtype (torch.dtype): Data type of the objects generated by the\\\n        dataloaders.\n        data_require_gradient (dict): Specifies whether the generated\\\n            objects should contain gradient information or not.\\\n            Defaults to False.\n\n    Returns:\n        tuple[DictionaryDataLoader, DictionaryDataLoader, DictionaryDataLoader]: Training, Testing and\\\n        Validation Dataloaders.\n    \"\"\"\n    train_indices_global = self.metadata_get_value(\n        \"split_train_indices\",\n    )\n    test_indices_global = self.metadata_get_value(\n        \"split_test_indices\",\n    )\n    validation_indices_global = self.metadata_get_value(\n        \"split_validation_indices\",\n    )\n\n    keys = self.get_data_keys()\n    data_train = self.database_file_controller.load_subset(\n        indices_global=mpi.iterator(train_indices_global),\n        keys=keys,\n        msg=\"Loading training database subset.\",\n    )\n    data_test = self.database_file_controller.load_subset(\n        indices_global=mpi.iterator(test_indices_global),\n        keys=keys,\n        msg=\"Loading testing database subset.\",\n    )\n    data_validation = self.database_file_controller.load_subset(\n        indices_global=mpi.iterator(validation_indices_global),\n        keys=keys,\n        msg=\"Loading validation database subset.\",\n    )\n\n    data_train_torch = {}\n    data_test_torch = {}\n    data_validation_torch = {}\n\n    for key in keys:\n        if key not in data_require_gradient:\n            data_require_gradient[key] = False\n\n    for key in keys:\n        data_train_torch[key] = torch.tensor(\n            np.array(data_train[key]),\n            dtype=dtype,\n            device=mpi._device,\n            requires_grad=data_require_gradient[key],\n        )\n        data_test_torch[key] = torch.tensor(\n            np.array(data_test[key]),\n            dtype=dtype,\n            device=mpi._device,\n            requires_grad=data_require_gradient[key],\n        )\n        data_validation_torch[key] = torch.tensor(\n            np.array(data_validation[key]),\n            dtype=dtype,\n            device=mpi._device,\n            requires_grad=data_require_gradient[key],\n        )\n\n    dataset_train = DictionaryDataset(data=data_train_torch)\n    dataset_test = DictionaryDataset(data=data_test_torch)\n    dataset_validation = DictionaryDataset(data=data_validation_torch)\n\n    dataloader_train = DictionaryDataLoader(\n        dataset=dataset_train,\n        batch_size=batch_size,\n        shuffle=True,\n        device=mpi._device,\n        generator=torch.Generator(device=mpi._device),\n    )\n    dataloader_test = DictionaryDataLoader(\n        dataset=dataset_test,\n        batch_size=batch_size,\n        shuffle=False,\n        device=mpi._device,\n        generator=torch.Generator(device=mpi._device),\n    )\n    dataloader_validation = DictionaryDataLoader(\n        dataset=dataset_validation,\n        batch_size=batch_size,\n        shuffle=False,\n        device=mpi._device,\n        generator=torch.Generator(device=mpi._device),\n    )\n\n    return dataloader_train, dataloader_test, dataloader_validation\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_default_metadata","title":"get_default_metadata","text":"<pre><code>get_default_metadata() -&gt; dict\n</code></pre> <p>Constructs and returns default metadata associated with this type of database. Calls the parent method and adds new entries, namely: - dimensions of inputs and outputs - labels of inputs and outputs - trasnforms of inputs and outputs</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Constructed metadata</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_default_metadata(\n    self,\n) -&gt; dict:\n    \"\"\"Constructs and returns default metadata associated with this type of\n    database. Calls the parent method and adds new entries, namely:\n    - dimensions of inputs and outputs\n    - labels of inputs and outputs\n    - trasnforms of inputs and outputs\n\n    Returns:\n        dict: Constructed metadata\n    \"\"\"\n    Logger.info(\"Returning default metadata for the MAIN database class.\")\n    metadata_tabular = {\n        \"dimensions\": {},\n        \"labels\": {},\n        \"transforms\": {},\n    }\n    metadata_parent = super().get_default_metadata()\n\n    for k, v in metadata_parent.items():\n        metadata_tabular[k] = v\n\n    Logger.info(\n        \"Returning default metadata for the TABULAR database class.\"\n    )\n    return metadata_tabular\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_dim","title":"get_dim","text":"<pre><code>get_dim(key: str) -&gt; int\n</code></pre> <p>Retrives the number of columns associated with the supplied database prefix key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A database prefix key for which to retrieve the number            of columns.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of columns.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_dim(\n    self,\n    key: str,\n) -&gt; int:\n    \"\"\"Retrives the number of columns associated with the supplied database\n    prefix key.\n\n    Args:\n        key (str): A database prefix key for which to retrieve the number\\\n        of columns.\n\n    Returns:\n        int: Number of columns.\n    \"\"\"\n    return self.metadata_get_value(\"dimensions\")[key]\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.get_statistic","title":"get_statistic","text":"<pre><code>get_statistic(\n    *,\n    data_keys: list[str],\n    stat_code: ENUM_StatisticsCode_Tabular = NONE\n) -&gt; dict[str:Any]\n</code></pre> <p>If not already calculated, calculates the corresponding statistic and stores it on disk. Then returns it for the database prefix keys supplied by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data_keys</code> <code>list[str]</code> <p>Database prefix keys for which to retrieve the statistic.</p> required <code>stat_code</code> <code>ENUM_StatisticsCode_Tabular</code> <p>Statistic identifier. Defaults to <code>StatCode.NONE</code>.</p> <code>NONE</code> <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>dict[str, Any] | None: Calculated statistics keyed by prefix, or</p> <code>dict[str:Any]</code> <p><code>None</code> when <code>stat_code</code> is <code>NONE</code>.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef get_statistic(\n    self,\n    *,\n    data_keys: list[str],\n    stat_code: StatCode = StatCode.NONE,\n) -&gt; dict[str:Any]:\n    \"\"\"If not already calculated, calculates the corresponding statistic\n    and stores it on disk. Then returns it for the database prefix keys\n    supplied by the user.\n\n    Args:\n        data_keys (list[str]): Database prefix keys for which to retrieve\n            the statistic.\n        stat_code (StatCode): Statistic identifier. Defaults to\n            ``StatCode.NONE``.\n\n    Returns:\n        dict[str, Any] | None: Calculated statistics keyed by prefix, or\n        ``None`` when ``stat_code`` is ``NONE``.\n    \"\"\"\n\n    if stat_code != StatCode.NONE:\n        self.calculate_statistic(stat_code=stat_code, data_keys=data_keys)\n        return self.load_stats(\n            stat_code_value=stat_code.value, data_keys=data_keys\n        )\n\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.remove_duplicate_rows","title":"remove_duplicate_rows","text":"<pre><code>remove_duplicate_rows(duplicate_row_eps: float) -&gt; None\n</code></pre> <p>Iterates over all pairs of rows in this database and detects duplicate entries. Then it removes the duplicities from the filesystem.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>duplicate_row_eps</code> <code>float</code> <p>Duplicity detection epsilon (|A-B| &lt;= epsilon \u21d2 duplicate).</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef remove_duplicate_rows(\n    self,\n    duplicate_row_eps: float,\n) -&gt; None:\n    \"\"\"Iterates over all pairs of rows in this database and detects\n    duplicate entries. Then it removes the duplicities from the filesystem.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        duplicate_row_eps (float): Duplicity detection epsilon (|A-B| &lt;=\n            epsilon \u21d2 duplicate).\n    \"\"\"\n\n    removed_rows = set()\n    nentries_global = self.get_n_entries_global()\n\n    # we process the data in batches for less memory intensive algorithm\n    # each batch contains the indices of rows in it\n    batch_size = nentries_global // (mpi.get_world_size() * 4)\n    batches = []\n    for idx in range(0, nentries_global, batch_size):\n        batches.append((idx, min(nentries_global, idx + batch_size)))\n    nbatches = len(batches)\n\n    # then we construct all pairs of batches which need to be compared\n    batch_pairs = []\n    for b1 in range(nbatches):\n        for b2 in range(b1, nbatches):\n            batch_pairs.append((b1, b2))\n\n    # each MPI process has a subset of the batch-&gt;batch interaction\n    mybatch_incidence = [[] for _ in range(nbatches)]\n    for u, v in mpi.iterator(batch_pairs):\n        mybatch_incidence[u].append(v)\n\n    # now each process finds duplicate entries in its batch-&gt;batch subset\n    similar_row_pairs = set()\n    for u in range(nbatches):\n        Vs = mybatch_incidence[u]\n        if len(Vs) == 0:\n            continue\n        indices_global_u = [v for v in range(batches[u][0], batches[u][1])]\n\n        # we load the data corresponding to one batch\n        data_u = self.database_file_controller.load_subset(\n            indices_global=indices_global_u,\n            keys=list(self.get_data_keys()),\n            msg=f\"  Loading {u}/{nbatches}-th portion of the data\",\n        )\n\n        U = np.hstack([np.vstack(data_u[k]) for k in data_u.keys()])\n        Uindices = np.lexsort(U.T, axis=-1)\n\n        for v in Vs:\n            if v == u:\n                V = U\n                Vindices = Uindices\n                indices_global_v = indices_global_u\n            else:\n                indices_global_v = [\n                    i for i in range(batches[v][0], batches[v][1])\n                ]\n\n                # or load the data corresponding to the paired batch\n                data_v = self.database_file_controller.load_subset(\n                    indices_global=indices_global_v,\n                    keys=list(self.get_data_keys()),\n                    msg=f\"    Loading {v}/{nbatches}-th portion of the data\",\n                )\n                V = np.hstack([np.vstack(data_v[k]) for k in data_v.keys()])\n                Vindices = np.lexsort(V.T, axis=-1)\n\n                del data_v\n\n            # we detect similar rows in an optimized manner\n            similar_row_pairs |= get_similar_row_pairs(\n                U,\n                Uindices,\n                indices_global_u,\n                V,\n                Vindices,\n                indices_global_v,\n                duplicate_row_eps,\n            )\n\n        del data_u\n\n    # now its time to construct a global set of duplicate rows\n    similar_row_pairs_gather = mpi.gather(similar_row_pairs, root=0)\n    if mpi.get_rank() == 0:\n        similar_row_pairs = set()\n        for S in similar_row_pairs_gather:\n            similar_row_pairs |= S\n\n        # now, one row can be similar to many other rows, we cant delete\n        # them willy-nilly\n        # we keep track of how many similarities there are for each row and\n        # construct an undirected graph\n        cardinality = np.zeros(nentries_global, dtype=int)\n        neighborhood = [set() for _ in range(nentries_global)]\n        for u, v in similar_row_pairs:\n            neighborhood[u].add(v)\n            neighborhood[v].add(u)\n            cardinality[u] += 1\n            cardinality[v] += 1\n\n        # we are interested in rows who have the most similarities\n        cardinality_indexing = np.argsort(cardinality)[::-1]\n\n        for u in cardinality_indexing:\n\n            # if the rows was already removed, we cannot remove its\n            # neighbours\n            if u in removed_rows:\n                continue\n\n            # remove all the neghbours of the row\n            removed_rows |= neighborhood[u]\n\n    # collected the removed rows for decision purposes\n    removed_rows = mpi.broadcast(removed_rows, root=0)\n    nremoved = len(removed_rows)\n\n    if nremoved &gt; 0:\n        Logger.info(\n            f\"About to remove {nremoved} rows via row similarity:\\\n            {list(removed_rows)}\"\n        )\n\n        removed_indices = list(removed_rows)\n\n        self.database_file_controller.remove(\n            keys=list(self.get_data_keys()),\n            indices_global=removed_indices,\n            msg=\"Removing entries due to similarity\",\n        )\n\n        self.data_changed()\n        self.add_n_entries(-len(removed_rows))\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.remove_rows_simplex_proximity","title":"remove_rows_simplex_proximity","text":"<pre><code>remove_rows_simplex_proximity(\n    *,\n    data: dict[str],\n    simplex_points: ndarray,\n    simplices: list[list[int]],\n    tol: float,\n    simplex_point_labels: list[str],\n    local2global_index_map: dict = None\n) -&gt; dict\n</code></pre> <p>Takes the input tabular data and removes some of its rows. The criterion for removal is based on the proximity of the row to a set of simplices defined by their coordinates and adjacency. When the shortest distance of the row to any point in the simplices is less then the supplied tolerance, it remains in the set.</p> <p>Not all columns are taken into account for the proximity calculation. The subspace in which the comparison is calculated is defined by the labels of the simplex coordinates. For example, when the dataset contains column labels A, B, C, D and the simplex coordinates contain labels D, B, then the proximity calculation is based off of the columns B, D.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str]</code> <p>Dictionary containing the data to be            cleaned. The keys in the dictionary correspond to the keys in this            database.</p> required <code>simplex_points</code> <code>ndarray</code> <p>An array of shape (n, m), where n is            the number of simplex coordinates and m is the dimension of each            simplex point.</p> required <code>simplices</code> <code>list[list[int]]</code> <p>An incidence list forming the simplex            set. For example: simplices = [[0, 1, 4]] represents one 3d            simplices with coordinates from the simplex_points parameter at            rows: 0, 1, 4.</p> required <code>tol</code> <code>float</code> <p>The distance treshold.</p> required <code>simplex_point_labels</code> <code>list[str]</code> <p>Each dimension in the simplex            coordinate system correspond to one column in one of this            database's prefix keys.</p> required <code>local2global_index_map</code> <code>dict</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with the same format as the input data, but with            possibly lesser number of rows per prefix key.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\n@transform_function\ndef remove_rows_simplex_proximity(\n    self,\n    *,\n    data: dict[str],\n    simplex_points: np.ndarray,\n    simplices: list[list[int]],\n    tol: float,\n    simplex_point_labels: list[str],\n    local2global_index_map: dict = None,\n) -&gt; dict:\n    \"\"\"Takes the input tabular data and removes some of its rows. The\n    criterion for removal is based on the proximity of the row to a\n    set of simplices defined by their coordinates and adjacency. When\n    the shortest distance of the row to any point in the simplices is\n    less then the supplied tolerance, it remains in the set.\n\n    Not all columns are taken into account for the proximity calculation.\n    The subspace in which the comparison is calculated is defined by the\n    labels of the simplex coordinates. For example, when the dataset\n    contains column labels A, B, C, D and the simplex coordinates contain\n    labels D, B, then the proximity calculation is based off of the\n    columns B, D.\n\n    Args:\n        data (dict[str]): Dictionary containing the data to be\\\n        cleaned. The keys in the dictionary correspond to the keys in this\\\n        database.\n        simplex_points (np.ndarray): An array of shape (n, m), where n is\\\n        the number of simplex coordinates and m is the dimension of each\\\n        simplex point.\n        simplices (list[list[int]]): An incidence list forming the simplex\\\n        set. For example: simplices = [[0, 1, 4]] represents one 3d\\\n        simplices with coordinates from the simplex_points parameter at\\\n        rows: 0, 1, 4.\n        tol (float): The distance treshold.\n        simplex_point_labels (list[str]): Each dimension in the simplex\\\n        coordinate system correspond to one column in one of this\\\n        database's prefix keys.\n        local2global_index_map (dict): _description_. Defaults to None.\n\n    Returns:\n        dict: Dictionary with the same format as the input data, but with\\\n        possibly lesser number of rows per prefix key.\n    \"\"\"\n\n    # The simplex is in the original space coordinates,\n    # but this dataset might have been transformed already\n    simplex_points, local2global_indices = self.transform_data(\n        simplex_points, simplex_point_labels\n    )\n\n    removed_rows = set()\n    removed_points = []\n    kept_points = []\n\n    n = np.min([len(data[k]) for k in data.keys()])\n\n    keys = data.keys()\n    Logger.warning(keys)\n    data_filtered = {k: [] for k in keys}\n    for i in range(n):\n        Pi = np.zeros(simplex_points.shape[1])\n\n        # we construct a subset of the input data so it corresponds to the\n        # simplex Euclidean subspace\n        for key in local2global_indices.keys():\n            for idx_loc, idx_glob in local2global_indices[key]:\n                Pi[idx_loc] = data[key][i][idx_glob]\n\n        # remove or not to remove?\n        if not lies_near_simplex_set(\n            Pi,\n            simplex_points,\n            simplices,\n            tol,\n        ):\n            if local2global_index_map is not None:\n                removed_rows.add(local2global_index_map[i])\n                removed_points.append(Pi)\n        else:\n            kept_points.append(Pi)\n            for key in keys:\n                data_filtered[key].append(data[key][i])\n\n    results = {}\n\n    results[\"removed_rows\"] = removed_rows\n    results[\"removed_points\"] = removed_points\n    results[\"kept_points\"] = kept_points\n    results[\"data\"] = data_filtered\n\n    return results\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.revert_transform_range","title":"revert_transform_range","text":"<pre><code>revert_transform_range(keys: list[str]) -&gt; None\n</code></pre> <p>Reverts the normalization transformations on this database for the supplied list of database prefix keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of database prefix keys for which to            revert the transformation operations.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef revert_transform_range(\n    self,\n    keys: list[str],\n) -&gt; None:\n    \"\"\"Reverts the normalization transformations on this database for the\n    supplied list of database prefix keys.\n\n    Args:\n        keys (list[str]): A list of database prefix keys for which to\\\n        revert the transformation operations.\n    \"\"\"\n\n    transforms = self.metadata_get_value(\"transforms\")\n    transform_a = {k: transforms[k][0] for k in keys}\n    transform_b = {k: transforms[k][1] for k in keys}\n\n    self.load_database(keys=keys)\n\n    results = self._revert_transform_range(\n        data=self.data,\n        keys=keys,\n        A=transform_a,\n        B=transform_b,\n    )\n    transformed_data = results[\"data\"]\n\n    self.database_file_controller.update(\n        data=transformed_data,\n        indices_global={k: self.local2global_index_map[k] for k in keys},\n        msg=f\"Updating database after transformation of keys: {keys}\",\n    )\n\n    self.data_changed()\n    del transformed_data\n\n    current_transforms = self.metadata_get_value(\"transforms\")\n    current_transforms_tmp = {}\n    for key in keys:\n        Ta, Tb = current_transforms[key]\n        current_transforms_tmp[key] = (np.ones_like(Ta), np.zeros_like(Tb))\n\n    self.update_metadata_value(\n        key=\"transforms\",\n        values=current_transforms_tmp,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.revert_transform_range_external","title":"revert_transform_range_external","text":"<pre><code>revert_transform_range_external(\n    *, keys: list[str], data: dict[str]\n) -&gt; dict[str]\n</code></pre> <p>Takes the input data and applies the backward transformations on it as if it were part of this database.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of prefix keys for which to apply the            backward transformation.</p> required <code>data</code> <code>dict[str]</code> <p>Dictionary containing the data for some of            this database's prefix keys.</p> required <p>Returns:</p> Type Description <code>dict[str]</code> <p>dict[str]: Dictionary containing the transformed data in the            same format as the input data dictionary.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef revert_transform_range_external(\n    self,\n    *,\n    keys: list[str],\n    data: dict[str],\n) -&gt; dict[str]:\n    \"\"\"Takes the input data and applies the backward transformations on it\n    as if it were part of this database.\n\n    Args:\n        keys (list[str]): A list of prefix keys for which to apply the\\\n        backward transformation.\n        data (dict[str]): Dictionary containing the data for some of\\\n        this database's prefix keys.\n\n    Returns:\n        dict[str]: Dictionary containing the transformed data in the\\\n        same format as the input data dictionary.\n    \"\"\"\n\n    transforms = self.metadata_get_value(\"transforms\")\n    transform_a = {k: transforms[k][0] for k in keys}\n    transform_b = {k: transforms[k][1] for k in keys}\n\n    results = self._revert_transform_range(\n        data=data,\n        keys=keys,\n        A=transform_a,\n        B=transform_b,\n    )\n    transformed_data = results[\"data\"]\n\n    for k in keys:\n        data_tmp = transformed_data[k]\n        min_values = np.min(data_tmp, axis=0)\n        max_values = np.max(data_tmp, axis=0)\n        for idx, w in enumerate(min_values):\n            Logger.warning(\n                f\"{k}: {self.get_label(key = k, idx = idx)} -&gt;\\\n                {w} - {max_values[idx]}\"\n            )\n\n    return transformed_data\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.transform_data","title":"transform_data","text":"<pre><code>transform_data(\n    *, data: ndarray, data_labels: list[str]\n) -&gt; tuple[ndarray, dict[str:tuple]]\n</code></pre> <p>Takes the supplied data with the corresponding labels and applies all transformations in the history of this database to it.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Raw data to be transformed.</p> required <code>data_labels</code> <code>list[str]</code> <p>Labels for each column of the supplied raw            data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[ndarray, dict[str:tuple]]</code> <p>Transformed data with the same shape as the raw data and a            dictionary containing label index mapping from the respective            prefix key ordering to the raw data ordering.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef transform_data(\n    self,\n    *,\n    data: np.ndarray,\n    data_labels: list[str],\n) -&gt; tuple[np.ndarray, dict[str:tuple]]:\n    \"\"\"Takes the supplied data with the corresponding labels and applies all\n    transformations in the history of this database to it.\n\n    Args:\n        data (np.ndarray): Raw data to be transformed.\n        data_labels (list[str]): Labels for each column of the supplied raw\\\n        data.\n\n    Returns:\n        tuple: Transformed data with the same shape as the raw data and a\\\n        dictionary containing label index mapping from the respective\\\n        prefix key ordering to the raw data ordering.\n    \"\"\"\n\n    # we start by determing the database prefixes and label indexing in the\n    # current context\n\n    # contains the raw data segmented w.r.t. this database's prefix keys\n    data_keyed = {}\n    # contains tuples of (local label index, global label index) for each\n    # supplied label and segments them w.r.t. this database's prefix keys\n    label_local2global_map = {}\n\n    for col_idx, label in enumerate(data_labels):\n        label_key = self.get_label_key(label)\n        label_idx = self.get_label_index(label=label, key=label_key)\n\n        if label_key not in data_keyed.keys():\n            data_keyed[label_key] = []\n        if label_key not in label_local2global_map.keys():\n            label_local2global_map[label_key] = []\n\n        data_keyed[label_key].append(data[:, col_idx])\n        label_local2global_map[label_key].append((col_idx, label_idx))\n\n    keys = data_keyed.keys()\n\n    labels = {k: [] for k in keys}\n    for col_idx, label in enumerate(data_labels):\n        key = self.get_label_key(label)\n        labels[key].append(label)\n\n    # right now applies only data range transformation\n    transforms = self.metadata_get_value(\"transforms\")\n    for key in labels.keys():\n        subdata = np.array(\n            [data[:, idx] for idx, _ in label_local2global_map[key]]\n        ).T\n        A = {\n            key: np.array(\n                [\n                    transforms[key][0][idx]\n                    for _, idx in label_local2global_map[key]\n                ]\n            )\n        }\n        B = {\n            key: np.array(\n                [\n                    transforms[key][1][idx]\n                    for _, idx in label_local2global_map[key]\n                ]\n            )\n        }\n\n        results = self._transform_range(\n            data={key: subdata},\n            keys=[key],\n            A=A,\n            B=B,\n        )\n\n        for idx, loc_idx in label_local2global_map[key]:\n            # print(f'col_idx: {idx}, data.shape: {data.shape}, source_idx: {loc_idx}')\n            data[:, idx] = results[\"data\"][key][:, idx]\n\n    # TODO fix transforms\n    # Logger.warning(transforms)\n\n    # active_transformations = self.metadata_get_value(\"operations_history\")\n\n    # for transformation in active_transformations:\n    #     pass\n    #     f_, kwargs_ = self.get_transformation(transformation)\n    #     results = f_(\n    #         data = data,\n    #         **kwargs_,\n    #     )\n\n    # #     data = results['data']\n\n    # #     fname = transformation[\"function\"]\n    # #     arguments = transformation[\"args\"]\n\n    # #     if fname == \"normalize_inputs\":\n    # #         transform_a = arguments[0]\n    # #         transform_b = arguments[1]\n\n    # #         for input_index, i in input_indices:\n    # #             coordinates[:, input_index] = self.__transform_forward(\n    # #                 transform_a=transform_a[i : i + 1],\n    # #                 transform_b=transform_b[i : i + 1],\n    # #                 x=coordinates[:, input_index],\n    # #             )\n\n    # #     if fname == \"normalize_outputs\":\n    # #         transform_a = arguments[0]\n    # #         transform_b = arguments[1]\n\n    # #         for output_index, i in output_indices:\n    # #             coordinates[:, output_index] = self.__transform_forward(\n    # #                 transform_a=transform_a[i : i + 1],\n    # #                 transform_b=transform_b[i : i + 1],\n    # #                 x=coordinates[:, output_index],\n    # #             )\n\n    return data, label_local2global_map\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular.DatabaseTabular.transform_range","title":"transform_range","text":"<pre><code>transform_range(\n    keys: list[str], target_range: tuple[float, float]\n) -&gt; None\n</code></pre> <p>For each of the supplied database prefix keys, we perform linear scaling and shift operations so the resulting values lie in the specified range.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>A list of database prefix keys for which to            perform the transformation.</p> required <code>target_range</code> <code>tuple</code> <p>Target minimal and maximal values of the            transformed data ranges.</p> required Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/database_tabular.py</code> <pre><code>@Logger.logged()\ndef transform_range(\n    self,\n    keys: list[str],\n    target_range: tuple[float, float],\n) -&gt; None:\n    \"\"\"For each of the supplied database prefix keys, we perform linear\n    scaling and shift operations so the resulting values lie in the\n    specified range.\n\n    Args:\n        keys (list[str]): A list of database prefix keys for which to\\\n        perform the transformation.\n        target_range (tuple): Target minimal and maximal values of the\\\n        transformed data ranges.\n    \"\"\"\n    min_values = self.get_statistic(stat_code=StatCode.MIN, data_keys=keys)\n    max_values = self.get_statistic(stat_code=StatCode.MAX, data_keys=keys)\n    value_lo = target_range[0]\n    value_hi = target_range[1]\n\n    for k, v in min_values.items():\n        for idx, w in enumerate(v):\n            Logger.warning(\n                f\"{k}: {self.get_label(key = k, idx = idx)} -&gt;\\\n                {w} - {max_values[k][idx]}\"\n            )\n\n    transform_a = {}\n    transform_b = {}\n    for key in keys:\n        transform_a[key] = np.ones_like(min_values[key])\n        transform_b[key] = np.zeros_like(min_values[key])\n\n        n = len(min_values[key])\n        for i in range(n):\n            if max_values[key][i] - min_values[key][i] &gt; 1e-12:\n                transform_a[key][i] = (value_hi - value_lo) / (\n                    max_values[key][i] - min_values[key][i]\n                )\n                transform_b[key][i] = (\n                    value_hi - max_values[key][i] * transform_a[key][i]\n                )\n\n            Logger.warning(\n                f\"{key}: {self.get_label(key = key, idx = i)} -&gt;\\\n                {transform_a[key][i]}x + {transform_b[key][i]}\"\n            )\n    self.load_database(keys=keys)\n\n    results = self._transform_range(\n        data=self.data,\n        keys=keys,\n        A=transform_a,\n        B=transform_b,\n    )\n    transformed_data = results[\"data\"]\n\n    for k in keys:\n        data_tmp = transformed_data[k]\n        min_values = np.min(data_tmp, axis=0)\n        max_values = np.max(data_tmp, axis=0)\n        for idx, w in enumerate(min_values):\n            Logger.warning(\n                f\"{k}: {self.get_label(key = k, idx = idx)} -&gt;\\\n                {w} - {max_values[idx]}\"\n            )\n\n    self.database_file_controller.update(\n        data=transformed_data,\n        indices_global={k: self.local2global_index_map[k] for k in keys},\n        msg=f\"Updating database after transformation of keys: {keys}\",\n    )\n\n    self.data_changed()\n    del transformed_data\n\n    current_transforms = self.metadata_get_value(\"transforms\")\n    current_transforms_tmp = {}\n    # (ax+b)*c + d = (a*c)x + (b*c + d)\n    for key in keys:\n        Ta, Tb = current_transforms[key]\n        current_transforms_tmp[key] = (\n            Ta * transform_a[key],\n            Tb * transform_a[key] + transform_b[key],\n        )\n\n    self.update_metadata_value(\n        key=\"transforms\",\n        values=current_transforms_tmp,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular_stats/","title":"Module <code>src.database.labeled.tabular.database_tabular_stats</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/database_tabular_stats/#AI4SurrogateModelling.src.database.labeled.tabular.database_tabular_stats","title":"AI4SurrogateModelling.src.database.labeled.tabular.database_tabular_stats","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/correlation_pearson/","title":"Module <code>src.database.labeled.tabular.stat_calculations.correlation_pearson</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/correlation_pearson/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.correlation_pearson","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.correlation_pearson","text":"<p>Module containing functions related to calculating the Pearson's Correlation coefficient matrix in parallel.</p> <p>https://en.wikipedia.org/wiki/Pearson_correlation_coefficient</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/correlation_pearson/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.correlation_pearson.calculate_statistic_correlation_pearson","title":"calculate_statistic_correlation_pearson","text":"<pre><code>calculate_statistic_correlation_pearson(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the Pearson's correlation coefficient matrices for each combination of the database prefix keys in the supplied list of prefix keys.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the matrices.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the correlation        matrices. The key to each matrix is \"A-B\", where A and B are from the        list of keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/correlation_pearson.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_correlation_pearson(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the Pearson's correlation coefficient matrices for each\n    combination of the database prefix keys in the supplied list of prefix\n    keys.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the matrices.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the correlation\\\n        matrices. The key to each matrix is \"A-B\", where A and B are from the\\\n        list of keys.\n    \"\"\"\n\n    mean_values = kwargs[\"dependencies_calculated\"][\"mean\"]\n    data = {k: tabular_dataset.get_data(k) for k in keys}\n\n    values = {}\n\n    for i, k1 in enumerate(keys):\n        for j, k2 in enumerate(keys[i:]):\n            values[f\"{k1}-{k2}\"] = _calculate_statistic_correlation_pearson(\n                data1=data[k1],\n                data2=data[k2],\n                means1=mean_values[k1],\n                means2=mean_values[k2],\n            )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/covariance/","title":"Module <code>src.database.labeled.tabular.stat_calculations.covariance</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/covariance/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.covariance","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.covariance","text":"<p>Module containing functions related to calculating the covariance matrix in parallel.</p> <p>https://en.wikipedia.org/wiki/Covariance</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/covariance/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.covariance.calculate_statistic_covariance","title":"calculate_statistic_covariance","text":"<pre><code>calculate_statistic_covariance(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the covariance matrices for each combination of the database prefix keys in the supplied list of prefix keys.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the matrices.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the covariance matrices.        The key to each matrix is \"A-B\", where A and B are from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/covariance.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_covariance(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the covariance matrices for each combination of the database\n    prefix keys in the supplied list of prefix keys.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset: An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the matrices.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the covariance matrices.\\\n        The key to each matrix is \"A-B\", where A and B are from the list of\\\n        keys.\n    \"\"\"\n\n    data = {key: tabular_dataset.get_data(key=key) for key in keys}\n    mean_values = kwargs[\"dependencies_calculated\"][\"mean\"]\n\n    values = {}\n    for i, k1 in enumerate(keys):\n        for k2 in keys[i:]:\n            key = f\"{k1}-{k2}\"\n            values[key] = _calculate_statistic_covariance(\n                data1=data[k1],\n                data2=data[k2],\n                means1=mean_values[k1],\n                means2=mean_values[k2],\n            )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram/","title":"Module <code>src.database.labeled.tabular.stat_calculations.histogram</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram","text":"<p>Module containing functions related to calculating the various histogram data in parallel.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram.calculate_statistic_histogram","title":"calculate_statistic_histogram","text":"<pre><code>calculate_statistic_histogram(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the 1D histograms for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the histograms.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the histograms.        The key to each histogram is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/histogram.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_histogram(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the 1D histograms for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the histograms.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the histograms.\\\n        The key to each histogram is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values = {k: None for k in keys}\n\n    min_values = kwargs[\"dependencies_calculated\"][\"min\"]\n    max_values = kwargs[\"dependencies_calculated\"][\"max\"]\n\n    for k in keys:\n        values[k] = _calculate_statistic_histograms(\n            data=np.array(tabular_dataset.get_data(k)),\n            min_values=min_values[k],\n            max_values=max_values[k],\n        )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram_pair_wise/","title":"Module <code>src.database.labeled.tabular.stat_calculations.histogram_pair_wise</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram_pair_wise/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram_pair_wise","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram_pair_wise","text":"<p>Module containing functions related to calculating the pair-wise histograms in parallel.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/histogram_pair_wise/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.histogram_pair_wise.calculate_statistic_histogram_pair_wise","title":"calculate_statistic_histogram_pair_wise","text":"<pre><code>calculate_statistic_histogram_pair_wise(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the 2D histograms (heatmaps) for each combination of the database prefix keys in the supplied list of prefix keys.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the histograms.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the histograms.        The key to each histogram is \"A-B\", where A and B are from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/histogram_pair_wise.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_histogram_pair_wise(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the 2D histograms (heatmaps) for each combination of the\n    database prefix keys in the supplied list of prefix keys.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the histograms.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the histograms.\\\n        The key to each histogram is \"A-B\", where A and B are from the list of\\\n        keys.\n    \"\"\"\n\n    nbins = 50\n    min_values = kwargs[\"dependencies_calculated\"][\"min\"]\n    max_values = kwargs[\"dependencies_calculated\"][\"max\"]\n\n    data = {key: np.array(tabular_dataset.get_data(key=key)) for key in keys}\n\n    values = {}\n    for i, k1 in enumerate(keys):\n        for k2 in keys[i:]:\n            key = f\"{k1}-{k2}\"\n            values[key] = _calculate_statistic_histograms_pair_wise(\n                nbins=nbins,\n                data_1=data[k1],\n                data_2=data[k2],\n                min_values_1=min_values[k1],\n                max_values_1=max_values[k1],\n                min_values_2=min_values[k2],\n                max_values_2=max_values[k2],\n            )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/iqr/","title":"Module <code>src.database.labeled.tabular.stat_calculations.iqr</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/iqr/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.iqr","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.iqr","text":"<p>Module containing functions related to calculating the IQR tresholds in parallel.</p> <p>https://en.wikipedia.org/wiki/Interquartile_range</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/iqr/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.iqr.calculate_statistic_iqr","title":"calculate_statistic_iqr","text":"<pre><code>calculate_statistic_iqr(\n    *, tabular_dataset, keys, **kwargs\n) -&gt; dict\n</code></pre> <p>Uses the calculated q1 and q3 values and populates the IQR interval.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the data.        The key to each value is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/iqr.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_iqr(\n    *,\n    tabular_dataset,\n    keys,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"Uses the calculated q1 and q3 values and populates the IQR interval.\n\n    Returns:\n        dict: A dictionary containing the data.\\\n        The key to each value is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    q1_values = kwargs[\"dependencies_calculated\"][\"percentile_25\"]\n    q3_values = kwargs[\"dependencies_calculated\"][\"percentile_75\"]\n\n    values = {}\n\n    for key in keys:\n        if q1_values[key] is not None and q1_values[key] is not None:\n            values[key] = q3_values[key] - q1_values[key]\n        else:\n            values[key] = None\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/kurtosis/","title":"Module <code>src.database.labeled.tabular.stat_calculations.kurtosis</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/kurtosis/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.kurtosis","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.kurtosis","text":"<p>Module containing functions related to calculating the Kurtosis metric in parallel.</p> <p>https://en.wikipedia.org/wiki/Kurtosis</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/kurtosis/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.kurtosis.calculate_statistic_kurtosis","title":"calculate_statistic_kurtosis","text":"<pre><code>calculate_statistic_kurtosis(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the Kurtosis coefficient for each database prefix key and its  columns.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the coefficients.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the coefficients.        The key to each array of coefficients is \"A\", where A is from the list</p> <code>dict</code> <p>of keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/kurtosis.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_kurtosis(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the Kurtosis coefficient for each database prefix key and its \n    columns.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the coefficients.\n        kwargs: a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the coefficients.\\\n        The key to each array of coefficients is \"A\", where A is from the list\n        of keys.\n    \"\"\"\n\n    values_mean = kwargs[\"dependencies_calculated\"][\"mean\"]\n    values_std = kwargs[\"dependencies_calculated\"][\"std\"]\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_kurtosis(\n            data=tabular_dataset.get_data(k),\n            means=values_mean[k],\n            stds=values_std[k],\n        )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/max/","title":"Module <code>src.database.labeled.tabular.stat_calculations.max</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/max/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.max","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.max","text":"<p>Module containing functions related to calculating maximal values over tabular datasets in parallel.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/max/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.max.calculate_statistic_max","title":"calculate_statistic_max","text":"<pre><code>calculate_statistic_max(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the maximal values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/max.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_max(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the maximal values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_max(data=tabular_dataset.get_data(k))\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/mean/","title":"Module <code>src.database.labeled.tabular.stat_calculations.mean</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/mean/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.mean","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.mean","text":"<p>Module containing functions related to calculating mean values over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Mean</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/mean/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.mean.calculate_statistic_mean","title":"calculate_statistic_mean","text":"<pre><code>calculate_statistic_mean(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the mean values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/mean.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_mean(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the mean values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_mean(data=tabular_dataset.get_data(k))\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/median/","title":"Module <code>src.database.labeled.tabular.stat_calculations.median</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/median/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.median","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.median","text":"<p>Module containing functions related to calculating median values over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Median</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/median/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.median.calculate_statistic_median","title":"calculate_statistic_median","text":"<pre><code>calculate_statistic_median(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the median values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/median.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_median(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the median values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    percentiles = kwargs[\"dependencies_calculated\"][\"percentile_50\"]\n    return percentiles\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/min/","title":"Module <code>src.database.labeled.tabular.stat_calculations.min</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/min/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.min","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.min","text":"<p>Module containing functions related to calculating minimal values over tabular datasets in parallel.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/min/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.min.calculate_statistic_min","title":"calculate_statistic_min","text":"<pre><code>calculate_statistic_min(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the minimal values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/min.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_min(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the minimal values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_min(data=tabular_dataset.get_data(k))\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/pca/","title":"Module <code>src.database.labeled.tabular.stat_calculations.pca</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/pca/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.pca","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.pca","text":"<p>Contains functions used for Principal Component Analysis over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Principal_component_analysis</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/pca/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.pca.calculate_statistic_pca","title":"calculate_statistic_pca","text":"<pre><code>calculate_statistic_pca(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculate PCA statistics per key in a tabular dataset.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>Tabular database instance to analyze.</p> required <code>keys</code> <code>list[str]</code> <p>Keys identifying independent feature groups to analyze.</p> required <code>**kwargs</code> <code>dict</code> <p>Supplementary parameters; expects <code>dependencies_calculated['covariance']</code> with per-key covariance matrices.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mapping <code>\"{key}-{key}\"</code> -&gt; PCA results for each entry in <code>keys</code>. Each result contains:</p> <ul> <li><code>eigen-vectors</code>: Sorted eigenvectors of the covariance matrix.</li> <li><code>eigen-values</code>: Sorted eigenvalues of the covariance matrix.</li> <li><code>pca_max_error</code>: List of maximal reconstruction errors when   using the first <code>k</code> principal components (index <code>k-1</code>).</li> </ul> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/pca.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_pca(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculate PCA statistics per key in a tabular dataset.\n\n    Args:\n        tabular_dataset (DatabaseTabular): Tabular database instance to analyze.\n        keys (list[str]): Keys identifying independent feature groups to analyze.\n        **kwargs (dict): Supplementary parameters; expects\n            ``dependencies_calculated['covariance']`` with per-key covariance\n            matrices.\n\n    Returns:\n        dict: Mapping ``\"{key}-{key}\"`` -&gt; PCA results for each entry in\n            ``keys``. Each result contains:\n\n            * ``eigen-vectors``: Sorted eigenvectors of the covariance matrix.\n            * ``eigen-values``: Sorted eigenvalues of the covariance matrix.\n            * ``pca_max_error``: List of maximal reconstruction errors when\n              using the first ``k`` principal components (index ``k-1``).\n    \"\"\"\n\n    covariances = kwargs[\"dependencies_calculated\"][\"covariance\"]\n\n    values = {}\n    for i, k1 in enumerate(keys):\n        key = f\"{k1}-{k1}\"\n        values[key] = _calculate_statistic_pca(\n            covariance_matrix=covariances[key],\n        )\n\n        X_original = np.array(tabular_dataset.get_data(k1))\n        values[key][\"pca_max_error\"] = []\n        for k in range(1, X_original.shape[1] + 1):\n            Vk = values[key][\"eigen-vectors\"][:, :k]\n            X_reduced = X_original @ Vk\n\n            X_reconstructed = X_reduced @ Vk.T\n            diff = np.max(np.abs(X_original - X_reconstructed))\n            values[key][\"pca_max_error\"].append(diff)\n\n        # TODO: check correctness of this reduction\n        values[key][\"pca_max_error\"] = mpi.allreduce(\n            data=values[key][\"pca_max_error\"], op=mpi.OP.MAX\n        )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/percentile/","title":"Module <code>src.database.labeled.tabular.stat_calculations.percentile</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/percentile/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.percentile","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.percentile","text":"<p>Contains functions used for Percentile related calculations over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Percentile</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/percentile/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.percentile.calculate_statistic_percentile","title":"calculate_statistic_percentile","text":"<pre><code>calculate_statistic_percentile(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the corresponding percentile statistic for each column and each supplied key for the given tabular database.</p> <p>Must be called by all MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the analysis.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies or additional parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the analysis. The key to each percentile            is \"A\", where A is from the list of keys. Each entry is a numpy            array.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/percentile.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_percentile(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the corresponding percentile statistic for each column and\n    each supplied key for the given tabular database.\n\n    Must be called by all MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the analysis.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies or additional parameters.\n\n    Returns:\n        dict: A dictionary containing the analysis. The key to each percentile\\\n            is \"A\", where A is from the list of keys. Each entry is a numpy\\\n            array.\n    \"\"\"\n\n    values = {}\n\n    for key in keys:\n        data = tabular_dataset.get_data(key)\n\n        if len(data) &gt; 0:\n            values[key] = _calculate_statistic_percentile(\n                data=data,\n                p=kwargs[\"p\"],\n            )\n        else:\n            values[key] = None\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/ranks/","title":"Module <code>src.database.labeled.tabular.stat_calculations.ranks</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/ranks/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.ranks","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.ranks","text":"<p>Contains functions used for Rank determination over tabular datasets in parallel.</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/ranks/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.ranks.calculate_statistic_ranks","title":"calculate_statistic_ranks","text":"<pre><code>calculate_statistic_ranks(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the rank value for each column in each database key.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the analysis.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the analysis. The key to each rank            ordering is \"A\", where A is from the list of keys. Each entry is            a numpy array.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/ranks.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_ranks(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the rank value for each column in each database key.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the analysis.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the analysis. The key to each rank\\\n            ordering is \"A\", where A is from the list of keys. Each entry is\\\n            a numpy array.\n    \"\"\"\n\n    values = {}\n\n    for key in keys:\n        data = tabular_dataset.get_data(key)\n\n        if len(data) &gt; 0:\n            values[key] = _calculate_statistic_ranks(\n                data=data,\n            )\n        else:\n            values[key] = None\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/skewness/","title":"Module <code>src.database.labeled.tabular.stat_calculations.skewness</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/skewness/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.skewness","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.skewness","text":"<p>Module containing functions related to calculating standard deviations of values over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Skewness</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/skewness/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.skewness.calculate_statistic_skewness","title":"calculate_statistic_skewness","text":"<pre><code>calculate_statistic_skewness(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the Skewness coefficients for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/skewness.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_skewness(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the Skewness coefficients for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values_mean = kwargs[\"dependencies_calculated\"][\"mean\"]\n    values_std = kwargs[\"dependencies_calculated\"][\"std\"]\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_skewness(\n            data=tabular_dataset.get_data(k),\n            means=values_mean[k],\n            stds=values_std[k],\n        )\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/std/","title":"Module <code>src.database.labeled.tabular.stat_calculations.std</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/std/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.std","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.std","text":"<p>Module containing functions related to calculating standard deviations of values over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Standard_deviation</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/std/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.std.calculate_statistic_std","title":"calculate_statistic_std","text":"<pre><code>calculate_statistic_std(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the standard deviation values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/std.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_std(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the standard deviation values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    values = {k: None for k in keys}\n    variances_values = kwargs[\"dependencies_calculated\"][\"variance\"]\n\n    for k in keys:\n        values[k] = _calculate_statistic_std(variances=variances_values[k])\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/variance/","title":"Module <code>src.database.labeled.tabular.stat_calculations.variance</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/variance/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.variance","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.variance","text":"<p>Module containing functions related to calculating standard deviations of values over tabular datasets in parallel.</p> <p>https://en.wikipedia.org/wiki/Variance</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/variance/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.variance.calculate_statistic_variance","title":"calculate_statistic_variance","text":"<pre><code>calculate_statistic_variance(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the variance values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/variance.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_variance(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the variance values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    mean_values = kwargs[\"dependencies_calculated\"][\"mean\"]\n\n    values = {k: None for k in keys}\n\n    for k in keys:\n        values[k] = _calculate_statistic_variance(\n            data=tabular_dataset.get_data(k),\n            means=mean_values[k],\n        )\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/zscore/","title":"Module <code>src.database.labeled.tabular.stat_calculations.zscore</code>","text":""},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/zscore/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.zscore","title":"AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.zscore","text":"<p>Contains functions used for Z-Score related calculations over tabular datasets in parallel.</p> <p>https://www.statisticshowto.com/probability-and-statistics/z-score/</p>"},{"location":"api/API%20Reference/src/database/labeled/tabular/stat_calculations/zscore/#AI4SurrogateModelling.src.database.labeled.tabular.stat_calculations.zscore.calculate_statistic_zscore","title":"calculate_statistic_zscore","text":"<pre><code>calculate_statistic_zscore(\n    *,\n    tabular_dataset: DatabaseTabular,\n    keys: list[str],\n    **kwargs: dict\n) -&gt; dict\n</code></pre> <p>Calculates the standard deviation values for each database prefix key and its  columns.</p> <p>Must be called by ALL MPI processes.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_dataset</code> <code>DatabaseTabular</code> <p>An instantiation of the tabular database.</p> required <code>keys</code> <code>list[str]</code> <p>A list of some of the database's prefix key for which        to calculate the values.</p> required <code>kwargs</code> <code>dict</code> <p>a dictionary containing supplementary parameters, like        calculated dependencies.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the calculated values.        The key to each array of values is \"A\", where A is from the list of        keys.</p> Source code in <code>AI4SurrogateModelling/src/database/labeled/tabular/stat_calculations/zscore.py</code> <pre><code>@Logger.logged()\n@stat_function_decorator\ndef calculate_statistic_zscore(\n    *,\n    tabular_dataset:'DatabaseTabular',\n    keys: list[str],\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Calculates the standard deviation values for each database prefix key and its \n    columns.\n\n    Must be called by ALL MPI processes.\n\n    Args:\n        tabular_dataset (DatabaseTabular): An instantiation of the tabular database.\n        keys (list[str]): A list of some of the database's prefix key for which\\\n        to calculate the values.\n        kwargs (dict): a dictionary containing supplementary parameters, like\\\n        calculated dependencies.\n\n    Returns:\n        dict: A dictionary containing the calculated values.\\\n        The key to each array of values is \"A\", where A is from the list of\\\n        keys.\n    \"\"\"\n\n    mean_values = kwargs[\"dependencies_calculated\"][\"mean\"]\n    std_values = kwargs[\"dependencies_calculated\"][\"std\"]\n\n    values = {}\n\n    for key in keys:\n        data = tabular_dataset.get_data(key)\n\n        if len(data) &gt; 0:\n            values[key] = _calculate_statistic_zscore(\n                data=data,\n                means=mean_values[key],\n                stds=std_values[key],\n            )\n        else:\n            values[key] = None\n\n    return values\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/","title":"Module <code>src.dataloaders.dataloader_dictionary</code>","text":""},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary","title":"AI4SurrogateModelling.src.dataloaders.dataloader_dictionary","text":"<p>Contains modules and functions related to the creation of dictionary based pytorch dataloaders.</p>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader","title":"DictionaryDataLoader","text":"<pre><code>DictionaryDataLoader(\n    *,\n    dataset: DictionaryDataset,\n    batch_size: int,\n    shuffle: bool = False,\n    device=None,\n    generator=None\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DictDataset</code> <p>dataset instance</p> required <code>batch_size</code> <code>int</code> <p>number of samples per batch</p> required <code>shuffle</code> <code>bool</code> <p>whether to shuffle dataset each epoch</p> <code>False</code> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataloader_dictionary.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dataset: DictionaryDataset,\n    batch_size: int,\n    shuffle: bool = False,\n    device=None,\n    generator=None,\n):\n    \"\"\"\n    Args:\n        dataset (DictDataset): dataset instance\n        batch_size (int): number of samples per batch\n        shuffle (bool): whether to shuffle dataset each epoch\n    \"\"\"\n    self.dataset = dataset\n    self.batch_size = batch_size // mpi.get_world_size()\n    self.shuffle = shuffle\n    self.indices = list(range(len(dataset)))\n    self.cursor = 0\n    self.generator = generator\n    self.device = device\n    self.dtype = dataset.dtype\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size // get_world_size()\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.cursor","title":"cursor  <code>instance-attribute</code>","text":"<pre><code>cursor = 0\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = generator\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.indices","title":"indices  <code>instance-attribute</code>","text":"<pre><code>indices = list(range(len(dataset)))\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.shuffle","title":"shuffle  <code>instance-attribute</code>","text":"<pre><code>shuffle = shuffle\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataloader_dictionary.py</code> <pre><code>def __iter__(self):\n    if self.shuffle:\n        self.indices = torch.randperm(\n            len(self.dataset), generator=self.generator\n        ).tolist()\n    self.cursor = 0\n    return self\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataloader_dictionary.py</code> <pre><code>def __len__(self):\n    # Number of batches per epoch\n    return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataloader_dictionary/#AI4SurrogateModelling.src.dataloaders.dataloader_dictionary.DictionaryDataLoader.__next__","title":"__next__","text":"<pre><code>__next__()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataloader_dictionary.py</code> <pre><code>def __next__(self):\n    if self.cursor &gt;= len(self.dataset):\n        raise StopIteration\n\n    batch_indices = self.indices[\n        self.cursor : self.cursor + self.batch_size\n    ]\n    self.cursor += self.batch_size\n\n    batch_items = [self.dataset[i] for i in batch_indices]\n\n    # Collate into dict\n    batch = {}\n    for key in self.dataset.keys:\n        values = [item[key] for item in batch_items]\n        try:\n            stacked = torch.stack(values)\n            if self.device is not None:\n                stacked = stacked.to(self.device, non_blocking=True)\n            batch[key] = stacked\n        except Exception as e:\n            batch[key] = values\n    return batch\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/","title":"Module <code>src.dataloaders.dataset_dictionary</code>","text":""},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary","title":"AI4SurrogateModelling.src.dataloaders.dataset_dictionary","text":"<p>Contains modules and functions related to the creation of dictionary based pytorch datasets.</p>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset","title":"DictionaryDataset","text":"<pre><code>DictionaryDataset(*, data: dict)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>each key points to a list of values.</p> required Source code in <code>AI4SurrogateModelling/src/dataloaders/dataset_dictionary.py</code> <pre><code>def __init__(self, *, data: dict):\n    \"\"\"\n    Args:\n        data (dict): each key points to a list of values.\n    \"\"\"\n    self.data = data\n    self.keys = list(data.keys())\n    self.total_size = mpi.allreduce(len(self), op=mpi.OP.SUM)\n    self.dtype = data[self.keys[0]].dtype\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = data\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype = dtype\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.keys","title":"keys  <code>instance-attribute</code>","text":"<pre><code>keys = list(keys())\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.total_size","title":"total_size  <code>instance-attribute</code>","text":"<pre><code>total_size = allreduce(len(self), op=SUM)\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; dict\n</code></pre> <p>Retrieves a dictionary with internal keys, each pointing to a single entry in this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the entry in this dataset.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the entries.</p> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataset_dictionary.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; dict:\n    \"\"\"Retrieves a dictionary with internal keys, each pointing to a single\n    entry in this dataset.\n\n    Args:\n        idx (int): Index of the entry in this dataset.\n\n    Returns:\n        dict: Dictionary containing the entries.\n    \"\"\"\n    return {k: self.data[k][idx] for k in self.keys}\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of entries in this dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of entries.</p> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataset_dictionary.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of entries in this dataset.\n\n    Returns:\n        int: Number of entries.\n    \"\"\"\n    return len(self.data[self.keys[0]])\n</code></pre>"},{"location":"api/API%20Reference/src/dataloaders/dataset_dictionary/#AI4SurrogateModelling.src.dataloaders.dataset_dictionary.DictionaryDataset.get_total_size","title":"get_total_size","text":"<pre><code>get_total_size() -&gt; int\n</code></pre> Source code in <code>AI4SurrogateModelling/src/dataloaders/dataset_dictionary.py</code> <pre><code>def get_total_size(\n    self,\n) -&gt; int:\n    return self.total_size\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_dtypes/","title":"Module <code>src.enums.enums_dtypes</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_dtypes/#AI4SurrogateModelling.src.enums.enums_dtypes","title":"AI4SurrogateModelling.src.enums.enums_dtypes","text":"<p>A module containing datatypes enums.</p>"},{"location":"api/API%20Reference/src/enums/enums_dtypes/#AI4SurrogateModelling.src.enums.enums_dtypes.ENUM_Dtypes","title":"ENUM_Dtypes","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_dtypes/#AI4SurrogateModelling.src.enums.enums_dtypes.ENUM_Dtypes.float16","title":"float16  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float16 = float16\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_dtypes/#AI4SurrogateModelling.src.enums.enums_dtypes.ENUM_Dtypes.float32","title":"float32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float32 = float32\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_dtypes/#AI4SurrogateModelling.src.enums.enums_dtypes.ENUM_Dtypes.float64","title":"float64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float64 = float64\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_losses/","title":"Module <code>src.enums.enums_losses</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses","title":"AI4SurrogateModelling.src.enums.enums_losses","text":"<p>Contains enumerators for various Loss functions.</p>"},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses.ENUM_Losses","title":"ENUM_Losses","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses.ENUM_Losses.L1","title":"L1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L1 = {'class': Loss, 'metric': L1}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses.ENUM_Losses.L1_RELATIVE","title":"L1_RELATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L1_RELATIVE = {'class': Loss, 'metric': L1_RELATIVE}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses.ENUM_Losses.L2","title":"L2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2 = {'class': Loss, 'metric': L2}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_losses/#AI4SurrogateModelling.src.enums.enums_losses.ENUM_Losses.L2_RELATIVE","title":"L2_RELATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2_RELATIVE = {'class': Loss, 'metric': L2_RELATIVE}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/","title":"Module <code>src.enums.enums_metrics</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics","title":"AI4SurrogateModelling.src.enums.enums_metrics","text":"<p>Contains enumerators for various Metrics measuring the state of the models.</p>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics","title":"ENUM_Metrics","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.Histogram","title":"Histogram  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Histogram = {'class': Histogram}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.L1","title":"L1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L1 = {'class': L1}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.L1_RELATIVE","title":"L1_RELATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L1_RELATIVE = {'class': L1_RELATIVE}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.L2","title":"L2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2 = {'class': L2}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.L2_RELATIVE","title":"L2_RELATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2_RELATIVE = {'class': L2_RELATIVE}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_metrics/#AI4SurrogateModelling.src.enums.enums_metrics.ENUM_Metrics.Store","title":"Store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Store = {'class': Store}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/","title":"Module <code>src.enums.enums_models</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models","title":"AI4SurrogateModelling.src.enums.enums_models","text":"<p>Contains various enumerators specifying Activation functions, models and layers</p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations","title":"ENUM_Activations","text":"<p>               Bases: <code>Enum</code></p> <p>For a single vertex.</p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations.COSINE","title":"COSINE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COSINE = {\n    \"short_string\": \"activation_cosine\",\n    \"description\": \"Cosine activation function\",\n    \"function\": cos,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = {\n    \"short_string\": \"none\",\n    \"description\": \"No activation\",\n    \"function\": Identity(),\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations.RELU","title":"RELU  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RELU = {\n    \"short_string\": \"activation_relu\",\n    \"description\": \"ReLU activation function\",\n    \"function\": ReLU(),\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations.SIGMOID","title":"SIGMOID  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SIGMOID = {\n    \"short_string\": \"activation_sigmoid\",\n    \"description\": \"Sigmoidal activation function\",\n    \"function\": Sigmoid(),\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Activations.SINE","title":"SINE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SINE = {\n    \"short_string\": \"activation_sine\",\n    \"description\": \"Sine activation function\",\n    \"function\": sin,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Losses","title":"ENUM_Losses","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Losses.L1","title":"L1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L1 = 1\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Losses.L2","title":"L2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>L2 = 2\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Losses.MSE","title":"MSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MSE = 0\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Metrics","title":"ENUM_Metrics","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Metrics.MSE","title":"MSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MSE = 0\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Metrics.MSE_ENTRY_WISE","title":"MSE_ENTRY_WISE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MSE_ENTRY_WISE = 1\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Schedulers","title":"ENUM_Schedulers","text":"<p>               Bases: <code>Enum</code></p> <p>Schedulers and their configurations</p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_Schedulers.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_TransferLayers","title":"ENUM_TransferLayers","text":"<p>               Bases: <code>Enum</code></p> <p>Betwen a pair of vertices.</p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_TransferLayers.LINEAR","title":"LINEAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LINEAR = {\n    \"short_string\": \"layer_linear\",\n    \"description\": \"Linear Layer\",\n    \"parameters\": [],\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_TransferLayers.RESIDUAL","title":"RESIDUAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RESIDUAL = {\n    \"short_string\": \"layer_residual\",\n    \"description\": \"Residual Layer\",\n    \"parameters\": [],\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_VertexProcessingLayers","title":"ENUM_VertexProcessingLayers","text":"<p>               Bases: <code>Enum</code></p> <p>Transforms the values of a vertex in some way.</p>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_VertexProcessingLayers.FOURIER_FEATURE","title":"FOURIER_FEATURE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FOURIER_FEATURE = {\n    \"short_string\": \"layer_fourier_features\",\n    \"description\": \"Fourier Feature Layer\",\n    \"parameters\": [\"dim_inputs\"],\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_models/#AI4SurrogateModelling.src.enums.enums_models.ENUM_VertexProcessingLayers.NOISE_GAUSSIAN","title":"NOISE_GAUSSIAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NOISE_GAUSSIAN = {\n    \"short_string\": \"layer_noise_gaussian\",\n    \"description\": \"Gaussian noise Layer\",\n    \"parameters\": [\"stds\", \"musapplication_intervals\"],\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_objectives/","title":"Module <code>src.enums.enums_objectives</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives","title":"AI4SurrogateModelling.src.enums.enums_objectives","text":"<p>Contains enumerators related to the training objective functions.</p>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives","title":"ENUM_Objectives","text":"<p>               Bases: <code>Enum</code></p> <p>Contains the following enumerators: - PREDICTION: used for the model outputs. - INPUT_GRADIENT: used for stats related to the model inputs. - MODEL: used for stats related to the model parameters            (weights and biases). - MODEL_WEIGHTS: used for stats related solely to model weights. - MODEL_BIASES: used for stats related solely to model biases.</p>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives.INPUT_GRADIENT","title":"INPUT_GRADIENT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INPUT_GRADIENT = {\n    \"id\": 1,\n    \"data_stat\": True,\n    \"model_stat\": False,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives.MODEL","title":"MODEL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL = {'id': 2, 'data_stat': False, 'model_stat': True}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives.MODEL_BIASES","title":"MODEL_BIASES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL_BIASES = {\n    \"id\": 4,\n    \"data_stat\": False,\n    \"model_stat\": True,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives.MODEL_WEIGHTS","title":"MODEL_WEIGHTS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL_WEIGHTS = {\n    \"id\": 3,\n    \"data_stat\": False,\n    \"model_stat\": True,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_objectives/#AI4SurrogateModelling.src.enums.enums_objectives.ENUM_Objectives.PREDICTION","title":"PREDICTION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PREDICTION = {\n    \"id\": 0,\n    \"data_stat\": True,\n    \"model_stat\": False,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/","title":"Module <code>src.enums.enums_optimizers</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers","title":"AI4SurrogateModelling.src.enums.enums_optimizers","text":"<p>Contains enumerators specifying the optimizers to be used.</p>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers.ENUM_Optimizers","title":"ENUM_Optimizers","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers.ENUM_Optimizers.ADAM","title":"ADAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ADAM = {\n    \"name\": \"Adaptive Moment Estimation (ADAM)\",\n    \"class\": Adam,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers.ENUM_Optimizers.CUSTOM_ADAM","title":"CUSTOM_ADAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CUSTOM_ADAM = {\n    \"name\": \"Custom Adaptive Moment Estimation (ADAM)\",\n    \"class\": Adam,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers.ENUM_Optimizers.LBFGS","title":"LBFGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LBFGS = {\n    \"name\": \"Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS)\",\n    \"class\": LBFGS,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_optimizers/#AI4SurrogateModelling.src.enums.enums_optimizers.ENUM_Optimizers.SGD","title":"SGD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SGD = {\n    \"name\": \"Stochastic Gradient Descent (SGD)\",\n    \"class\": SGD,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/","title":"Module <code>src.enums.enums_schedulers</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers","title":"AI4SurrogateModelling.src.enums.enums_schedulers","text":"<p>Contains enumerators specifying the schedulers to be used.</p>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers","title":"ENUM_Schedulers","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.CONSTANT","title":"CONSTANT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONSTANT = {\n    \"name\": \"Constant Scheduler\",\n    \"class\": ConstantLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.COSINE_ANNEALING","title":"COSINE_ANNEALING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COSINE_ANNEALING = {\n    \"name\": \"Cosine Annealing Scheduler\",\n    \"class\": CosineAnnealingLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.COSINE_ANNEALING_WARM_RESTARTS","title":"COSINE_ANNEALING_WARM_RESTARTS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COSINE_ANNEALING_WARM_RESTARTS = {\n    \"name\": \"Cosine Annealing with Warm Restarts\",\n    \"class\": CosineAnnealingWarmRestarts,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.CYCLIC","title":"CYCLIC  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CYCLIC = {'name': 'Cyclic Scheduler', 'class': CyclicLR}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.EXPONENTIAL","title":"EXPONENTIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXPONENTIAL = {\n    \"name\": \"Exponential Scheduler\",\n    \"class\": ExponentialLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.LAMBDA","title":"LAMBDA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LAMBDA = {'name': 'Lambda Scheduler', 'class': LambdaLR}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.LINEAR","title":"LINEAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LINEAR = {'name': 'Linear Scheduler', 'class': LinearLR}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.MULTIPLICATIVE","title":"MULTIPLICATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MULTIPLICATIVE = {\n    \"name\": \"Multiplicative Scheduler\",\n    \"class\": MultiplicativeLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.ONE_CYCLE","title":"ONE_CYCLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ONE_CYCLE = {\n    \"name\": \"One Cycle Scheduler\",\n    \"class\": OneCycleLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.POLYNOMIAL","title":"POLYNOMIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POLYNOMIAL = {\n    \"name\": \"Polynomial Scheduler\",\n    \"class\": PolynomialLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.REDUCE_ON_PLATEAU","title":"REDUCE_ON_PLATEAU  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REDUCE_ON_PLATEAU = {\n    \"name\": \"Reduce on Plateau Scheduler\",\n    \"class\": ReduceLROnPlateau,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.SEQUENTIAL","title":"SEQUENTIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SEQUENTIAL = {\n    \"name\": \"Sequential Scheduler\",\n    \"class\": SequentialLR,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_schedulers/#AI4SurrogateModelling.src.enums.enums_schedulers.ENUM_Schedulers.STEP","title":"STEP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STEP = {'name': 'Step Scheduler', 'class': StepLR}\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/","title":"Module <code>src.enums.enums_tabular_stats</code>","text":""},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats","title":"AI4SurrogateModelling.src.enums.enums_tabular_stats","text":"<p>Contains enumerators and function hub related to the calculation of statistics related to tabular databases.</p>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular","title":"ENUM_StatisticsCode_Tabular","text":"<p>               Bases: <code>Enum</code></p> <p>Contains the enumerators specifying which statistic and how should it  be calculated.</p> Each enumerator has a value with 5 entries <ul> <li>value[0]: string code of the statistic, used for io and dependency            calculation.</li> <li>value[1]: determines the type of the statistic, used for io.</li> <li>value[2]: a string description of the statistic, used for logging.</li> <li>value[3]: a function to calculate the statistic.</li> <li>value[4]: a dictionary containing required parameters. Contains a            dependency field.</li> </ul>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.CORRELATION_PEARSON","title":"CORRELATION_PEARSON  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CORRELATION_PEARSON = (\n    \"correlation_pearson\",\n    COL,\n    \"Pearson's Correlation Matrices\",\n    calculate_statistic_correlation_pearson,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.COVARIANCE","title":"COVARIANCE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COVARIANCE = (\n    \"covariance\",\n    COL,\n    \"Covariances\",\n    calculate_statistic_covariance,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.HISTOGRAM","title":"HISTOGRAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HISTOGRAM = (\n    \"histogram\",\n    COL,\n    \"Histogram\",\n    calculate_statistic_histogram,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.IQR","title":"IQR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IQR = ('iqr', COL, 'IQR', calculate_statistic_iqr, {})\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.KURTOSIS","title":"KURTOSIS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>KURTOSIS = (\n    \"kurtosis\",\n    COL,\n    \"Kurtosis\",\n    calculate_statistic_kurtosis,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.MAX","title":"MAX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX = (\n    \"max\",\n    COL,\n    \"Maximal values\",\n    calculate_statistic_max,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.MEAN","title":"MEAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEAN = (\n    \"mean\",\n    COL,\n    \"Mean values\",\n    calculate_statistic_mean,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.MEDIAN","title":"MEDIAN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEDIAN = (\n    \"median\",\n    COL,\n    \"Median values\",\n    calculate_statistic_median,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.MIN","title":"MIN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN = (\n    \"min\",\n    COL,\n    \"Minimal values\",\n    calculate_statistic_min,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = ('none', NONE, '', None, None)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.P25","title":"P25  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>P25 = (\n    \"percentile_25\",\n    COL,\n    \"25% percentiles\",\n    calculate_statistic_percentile,\n    {\"p\": 25},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.P50","title":"P50  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>P50 = (\n    \"percentile_50\",\n    COL,\n    \"50% percentiles\",\n    calculate_statistic_percentile,\n    {\"p\": 50},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.P75","title":"P75  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>P75 = (\n    \"percentile_75\",\n    COL,\n    \"75% percentiles\",\n    calculate_statistic_percentile,\n    {\"p\": 75},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.PAIR_HISTOGRAM","title":"PAIR_HISTOGRAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PAIR_HISTOGRAM = (\n    \"pair-histogram\",\n    COL,\n    \"Pair-wise Distribution Histogram\",\n    calculate_statistic_histogram_pair_wise,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.PCA","title":"PCA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PCA = (\n    \"pca\",\n    COL,\n    \"Principal Component Analysis\",\n    calculate_statistic_pca,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.RANKS","title":"RANKS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RANKS = (\n    \"ranks\",\n    ROW,\n    \"Ranks\",\n    calculate_statistic_ranks,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.SKEWNESS","title":"SKEWNESS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKEWNESS = (\n    \"skewness\",\n    COL,\n    \"Skewness\",\n    calculate_statistic_skewness,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.STD","title":"STD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STD = (\n    \"std\",\n    COL,\n    \"Standard deviations\",\n    calculate_statistic_std,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.VARIANCE","title":"VARIANCE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VARIANCE = (\n    \"var\",\n    COL,\n    \"Variances\",\n    calculate_statistic_variance,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsCode_Tabular.ZSCORE","title":"ZSCORE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ZSCORE = (\n    \"z-score\",\n    ROW,\n    \"Z-Scores\",\n    calculate_statistic_zscore,\n    {},\n)\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsType","title":"ENUM_StatisticsType","text":"<p>               Bases: <code>Enum</code></p> <p>Determines the type of the statistic. - ROW: calculates the statistics for each entry in the database. - COL: calculates the statistics for each column of the database.</p>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsType.COL","title":"COL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COL = 2\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsType.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 0\n</code></pre>"},{"location":"api/API%20Reference/src/enums/enums_tabular_stats/#AI4SurrogateModelling.src.enums.enums_tabular_stats.ENUM_StatisticsType.ROW","title":"ROW  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ROW = 1\n</code></pre>"},{"location":"api/API%20Reference/src/export/export_config_rules/","title":"Module <code>src.export.export_config_rules</code>","text":""},{"location":"api/API%20Reference/src/export/export_config_rules/#AI4SurrogateModelling.src.export.export_config_rules","title":"AI4SurrogateModelling.src.export.export_config_rules","text":"<p>A module containing functionality exporting the configuration file specifications into a markdown file.</p>"},{"location":"api/API%20Reference/src/export/export_config_rules/#AI4SurrogateModelling.src.export.export_config_rules.export","title":"export","text":"<pre><code>export(*, config: BaseModel, tgt_fn: str)\n</code></pre> <p>Takes the supplied configuration model and stores its markdown        specification to the supplied file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseModel</code> <p>A configuration specification.</p> required <code>tgt_fn</code> <code>str</code> <p>A filename of the resulting markdown file.</p> required Source code in <code>AI4SurrogateModelling/src/export/export_config_rules.py</code> <pre><code>def export(\n    *,\n    config: BaseModel,\n    tgt_fn: str,\n):\n    \"\"\"Takes the supplied configuration model and stores its markdown\\\n        specification to the supplied file.\n\n    Args:\n        config (BaseModel): A configuration specification.\n        tgt_fn (str): A filename of the resulting markdown file.\n    \"\"\"\n    if mpi.get_rank() == 0:\n        markdown_doc = schema_to_markdown(config)\n\n        with open(tgt_fn, \"w\") as f:\n            f.write(markdown_doc)\n\n    msg = f\"\u2705 Config spec written to {tgt_fn}\"\n    Logger.info(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/export/export_config_rules/#AI4SurrogateModelling.src.export.export_config_rules.schema_to_markdown","title":"schema_to_markdown","text":"<pre><code>schema_to_markdown(\n    model: type[BaseModel], level: int = 1\n) -&gt; str\n</code></pre> <p>Takes the supplied model schema and constructs a markdown string to be stored to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[BaseModel]</code> <p>Model schema to be analyzed.</p> required <code>level</code> <code>int</code> <p>Recursion level for indentation purposes. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representing the markdown file.</p> Source code in <code>AI4SurrogateModelling/src/export/export_config_rules.py</code> <pre><code>def schema_to_markdown(model: type[BaseModel], level: int = 1) -&gt; str:\n    \"\"\"Takes the supplied model schema and constructs a markdown string to be\n    stored to a file.\n\n    Args:\n        model (type[BaseModel]): Model schema to be analyzed.\n        level (int): Recursion level for indentation purposes. Defaults to 1.\n\n    Returns:\n        str: A string representing the markdown file.\n    \"\"\"\n    lines = [f\"{'#' * level} `{model.__name__}`\", \"\"]\n\n    for field_name, field in model.model_fields.items():\n        # Resolve type name\n        typ = field.annotation\n        origin = get_origin(typ)\n        args = get_args(typ)\n\n        if origin is list:\n            type_name = f\"List[{args[0].__name__ if hasattr(args[0], '__name__') else args[0]}]\"\n        elif origin is dict:\n            key_type, val_type = args\n            val_name = (\n                val_type.__name__ if hasattr(val_type, \"__name__\") else val_type\n            )\n            type_name = f\"Dict[{key_type.__name__}, {val_name}]\"\n        else:\n            type_name = typ.__name__ if hasattr(typ, \"__name__\") else str(typ)\n\n        default = \"required\" if field.is_required() else field.default\n        desc = field.description or \"\"\n\n        lines.append(f\"{'#' * (level+1)} `{field_name}`\")\n        lines.append(f\"- **Type**: `{type_name}`\")\n        lines.append(f\"- **Default**: `{default}`\")\n        if desc:\n            lines.append(f\"- **Description**: {desc}\")\n        lines.append(\"\")\n\n        # Recurse into sub-models\n        if isinstance(typ, type) and issubclass(typ, BaseModel):\n            lines.append(schema_to_markdown(typ, level + 2))\n\n        elif origin in (list, dict) and args:\n            # If dict/list contains a BaseModel as value, recurse\n            for arg in args:\n                if isinstance(arg, type) and issubclass(arg, BaseModel):\n                    lines.append(schema_to_markdown(arg, level + 2))\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/","title":"Module <code>src.export.plots.tabular</code>","text":""},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular","title":"AI4SurrogateModelling.src.export.plots.tabular","text":"<p>summary</p>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting","title":"Plotting","text":"<p>summary</p>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.__calculate_histograms_2d_asymmetric","title":"__calculate_histograms_2d_asymmetric  <code>staticmethod</code>","text":"<pre><code>__calculate_histograms_2d_asymmetric(\n    indices1: list[int],\n    indices2: list[int],\n    bin_edges_list1: list[ndarray],\n    bin_edges_list2: list[ndarray],\n    data1: ndarray,\n    data2: ndarray,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __calculate_histograms_2d_asymmetric(\n    indices1: list[int],\n    indices2: list[int],\n    bin_edges_list1: list[np.ndarray],\n    bin_edges_list2: list[np.ndarray],\n    data1: np.ndarray,\n    data2: np.ndarray,\n):\n    n1, n2, nvalues_max_2d = len(indices1), len(indices2), 0\n\n    data_raw = [[None for _ in range(n2)] for _ in range(n1)]\n\n    for i1, j1 in enumerate(indices1):\n        for i2, j2 in enumerate(indices2):\n\n            data_local = np.histogram2d(\n                x=data1[:, j1],\n                y=data2[:, j2],\n                bins=[\n                    bin_edges_list1[i1],\n                    bin_edges_list2[i2],\n                ],\n            )[0]\n\n            data_global = mpi.reduce(data_local, mpi.OP.SUM, root=0)\n            data_raw[i1][i2] = data_global\n\n            nvalues_max_2d = max(nvalues_max_2d, np.max(data_global))\n\n    return data_raw, nvalues_max_2d\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.__calculate_histograms_2d_symmetric","title":"__calculate_histograms_2d_symmetric  <code>staticmethod</code>","text":"<pre><code>__calculate_histograms_2d_symmetric(\n    indices1: list[int],\n    indices2: list[int],\n    bin_edges_list1: list[ndarray],\n    bin_edges_list2: list[ndarray],\n    data1: ndarray,\n    data2: ndarray,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __calculate_histograms_2d_symmetric(\n    indices1: list[int],\n    indices2: list[int],\n    bin_edges_list1: list[np.ndarray],\n    bin_edges_list2: list[np.ndarray],\n    data1: np.ndarray,\n    data2: np.ndarray,\n):\n    n1, n2, nvalues_max_2d = len(indices1), len(indices2), 0\n\n    data_raw = [[None for _ in range(n2)] for _ in range(n1)]\n\n    for i1, j1 in enumerate(indices1):\n        for i2, j2 in enumerate(indices2):\n            if i2 &gt;= i1:\n                continue\n\n            data_local = np.histogram2d(\n                x=data1[:, j1],\n                y=data2[:, j2],\n                bins=[\n                    bin_edges_list1[i1],\n                    bin_edges_list2[i2],\n                ],\n            )[0]\n\n            data_global = mpi.reduce(data_local, mpi.OP.SUM, root=0)\n            data_raw[i1][i2] = data_global\n\n            nvalues_max_2d = max(nvalues_max_2d, np.max(data_global))\n\n    return data_raw, nvalues_max_2d\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.__export_box_plot_helper","title":"__export_box_plot_helper  <code>staticmethod</code>","text":"<pre><code>__export_box_plot_helper(\n    categories: list[str],\n    values_min: ndarray,\n    values_q1: ndarray,\n    values_median: ndarray,\n    values_q3: ndarray,\n    values_max: ndarray,\n    target_dir: str,\n    prefix: str,\n    indices: list[int],\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __export_box_plot_helper(\n    categories: list[str],\n    values_min: np.ndarray,\n    values_q1: np.ndarray,\n    values_median: np.ndarray,\n    values_q3: np.ndarray,\n    values_max: np.ndarray,\n    target_dir: str,\n    prefix: str,\n    indices: list[int],\n):\n    # Convert summary statistics to the format required by seaborn\n    box_data = []\n    ncategories = len(indices)\n    # fig_height = max(2, ncategories * 1.5)\n\n    image_filenames = []\n\n    if mpi.get_rank() == 0:\n        for i in indices:\n            box_data.append(\n                {\n                    \"Category\": categories[i],\n                    \"Min\": values_min[i],\n                    \"Q1\": values_q1[i],\n                    \"Median\": values_median[i],\n                    \"Q3\": values_q3[i],\n                    \"Max\": values_max[i],\n                }\n            )\n\n        for j in mpi.tqdm(\n            range(len(indices)), desc=f\"Exporting Box Plots for: {prefix}\"\n        ):\n            fig, ax = plt.subplots(figsize=(12, 2))\n            i = indices[j]\n            box = ax.bxp(\n                [\n                    {\n                        \"whislo\": values_min[i],  # Minimum\n                        \"q1\": values_q1[i],  # First quartile\n                        \"med\": values_median[i],  # Median\n                        \"q3\": values_q3[i],  # Third quartile\n                        \"whishi\": values_max[i],  # Maximum\n                        \"fliers\": [],  # No outliers as we only have summaries\n                    }\n                ],\n                positions=[j],\n                showfliers=False,\n                vert=False,\n                widths=0.6,\n                patch_artist=True,  # Enables custom box face colors\n                boxprops=dict(\n                    edgecolor=\"black\", linewidth=1.5\n                ),  # Box color\n                medianprops=dict(\n                    color=\"red\", linewidth=2\n                ),  # Median line color\n                capprops=dict(\n                    color=\"black\", linewidth=1.5\n                ),  # Whisker cap color\n                whiskerprops=dict(\n                    color=\"black\", linewidth=1.5\n                ),  # Whisker color\n            )\n            for patch in box[\"boxes\"]:\n                patch.set_facecolor(\"#6495ED\")\n\n            # Set category labels\n            # ax.set_yticks(range(ncategories))\n            # ax.set_yticklabels([categories[i] for i in indices])\n            ax.set_title(f\"Boxplot of {prefix} '{categories[i]}' values\")\n            ax.set_xlabel(\"Values\")\n\n            plt.tight_layout()  # to fix labeling cut off\n            fn = f\"{target_dir}/{prefix}_{i}.png\"\n            plt.savefig(fn)\n            plt.close()\n            image_filenames.append(fn)\n            # image_filenames.append(os.path.abspath(fn))\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.__export_pair_plot_helper","title":"__export_pair_plot_helper  <code>staticmethod</code>","text":"<pre><code>__export_pair_plot_helper(\n    compact,\n    bin_edges_list_1,\n    bin_edges_list_2,\n    weights_list_1,\n    weights_list_2,\n    labels_1,\n    labels_2,\n    density_matrix_list,\n    indices1,\n    indices2,\n    max_y_lim_1d,\n    max_y_lim_2d,\n    bin_centers_list_1,\n    bin_centers_list_2,\n    target_dir,\n    prefix,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __export_pair_plot_helper(\n    compact,\n    bin_edges_list_1,\n    bin_edges_list_2,\n    weights_list_1,\n    weights_list_2,\n    labels_1,\n    labels_2,\n    density_matrix_list,\n    indices1,\n    indices2,\n    max_y_lim_1d,\n    max_y_lim_2d,\n    bin_centers_list_1,\n    bin_centers_list_2,\n    target_dir,\n    prefix,\n):\n    if mpi.get_rank() &gt; 0:\n        return\n\n    image_filenames = []\n\n    if compact:\n        nrows = len(indices2) + 1\n        ncols = len(indices1) + 2\n        width_ratios = [4] * (ncols - 2) + [1, 1]\n        height_ratios = [1] + [4] * (nrows - 1)\n        gs = gridspec.GridSpec(\n            nrows=nrows,\n            ncols=ncols,\n            width_ratios=width_ratios,\n            height_ratios=height_ratios,\n            wspace=0.00,\n            hspace=0.00,\n        )\n\n        sns.set_theme(style=\"white\")\n\n        fig = plt.figure(figsize=(sum(width_ratios), sum(height_ratios)))\n        plt.title(\"Density Plots\")\n        plt.axis(\"off\")\n\n    valid_indices = []\n    for index1, i1 in enumerate(indices1):\n        for index2, i2 in enumerate(indices2):\n            density_matrix = density_matrix_list[index1][index2]\n            if density_matrix is not None:\n                valid_indices.append([(index1, index2), (i1, i2)])\n\n    for I in mpi.tqdm(valid_indices, desc=f\"Generating pair plots\"):\n        index1 = I[0][0]\n        index2 = I[0][1]\n        i1 = I[1][0]\n        i2 = I[1][1]\n\n        density_matrix = density_matrix_list[index1][index2]\n\n        x_edges = bin_edges_list_1[index1]\n        y_edges = bin_edges_list_2[index2]\n        hist_x = weights_list_1[index1]\n        hist_y = weights_list_2[index2]\n        labels_x = labels_1[indices1[index1]]\n        labels_y = labels_2[indices2[index2]]\n\n        if not compact:\n            sns.set_theme(style=\"white\")\n            plt.title(\"Density Plot\")\n            plt.axis(\"off\")\n            include_bottom = True\n            include_left = True\n            width_ratios = [4, 1, 1]\n            height_ratios = [1, 4]\n            gs = gridspec.GridSpec(\n                nrows=2,\n                ncols=2,\n                width_ratios=width_ratios,\n                height_ratios=height_ratios,\n                wspace=0.00,\n                hspace=0.00,\n            )\n            fig = plt.figure(\n                figsize=(sum(width_ratios), sum(height_ratios))\n            )\n            ax_main = plt.subplot(gs[1, 0])\n            ax_top = plt.subplot(gs[0, 0], sharex=ax_main)\n            ax_right = plt.subplot(gs[1, 1], sharey=ax_main)\n\n        else:\n            include_bottom = False\n            include_left = False\n            ax_top = None\n            ax_right = None\n            ax_main = plt.subplot(gs[nrows - 1 - index2, index1])\n            if index2 == len(indices2) - 1:\n                ax_top = plt.subplot(gs[0, index1], sharex=ax_main)\n            if index1 == len(indices1) - 1:\n                ax_right = plt.subplot(\n                    gs[nrows - 1 - index2, -1], sharey=ax_main\n                )\n            if index2 == 0:\n                include_bottom = True\n            if index1 == 0:\n                include_left = True\n\n        ax_main.imshow(\n            density_matrix.T,\n            origin=\"lower\",\n            aspect=\"auto\",\n            extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]],\n            # vmin=0, vmax=max_y_lim_2d,\n            cmap=\"rocket\",\n        )\n        for spine in ax_main.spines.values():\n            #     spine.set_visible(False)\n            spine.set_edgecolor(\"cyan\")  # Set spine color\n            spine.set_linewidth(\n                1 * max(nrows, ncols)\n            )  # Set spine thickness\n            # spine.set_linestyle('--')         # Optional: dashed style\n\n        # Histogram on the top (X-axis)\n        if ax_top is not None:\n            ax_top.bar(\n                (x_edges[1:] + x_edges[:-1]) * 0.5,\n                hist_x,\n                width=np.diff(x_edges),\n                color=\"black\",\n                alpha=0.8,\n            )\n            ax_top.set_xlim(x_edges[0], x_edges[-1])\n            ax_top.set_ylim(0, max_y_lim_1d)\n            ax_top.tick_params(\n                axis=\"x\",\n                which=\"both\",\n                bottom=False,\n                top=False,\n                labelbottom=False,\n            )\n            ax_top.set_yticks([])  # Remove y-ticks for cleanliness\n            for spine in ax_top.spines.values():\n                spine.set_visible(False)\n\n        if ax_right is not None:\n            ax_right.barh(\n                (y_edges[1:] + y_edges[:-1]) * 0.5,\n                hist_y,\n                height=np.diff(y_edges),\n                color=\"black\",\n                alpha=0.8,\n            )\n            ax_right.set_ylim(y_edges[0], y_edges[-1])\n            ax_right.set_xlim(0, max_y_lim_1d)\n            ax_right.tick_params(\n                axis=\"y\",\n                which=\"both\",\n                left=False,\n                right=False,\n                labelleft=False,\n            )\n            ax_right.set_xticks([])  # Remove y-ticks for cleanliness\n            for spine in ax_right.spines.values():\n                spine.set_visible(False)\n\n        if include_bottom:\n            ax_main.set_xlabel(labels_x)\n            ax_main.set_xticks(\n                [float(f\"{v:0.2f}\") for v in bin_centers_list_1[index1]]\n            )  # X-axis ticks at column indices\n        else:\n            ax_main.set_xticks([])\n\n        if include_left:\n            ax_main.set_ylabel(labels_y)\n            ax_main.set_yticks(\n                [float(f\"{v:0.2f}\") for v in bin_centers_list_2[index2]]\n            )  # X-axis ticks at column indices\n        else:\n            ax_main.set_yticks([])\n\n        if not compact:\n            plt.tight_layout()\n            fn = f\"{target_dir}/{prefix}_{i1:03d}_{i2:03d}.png\"\n            plt.savefig(fn)\n            image_filenames.append(fn)\n            plt.close()\n\n    if compact:\n        plt.tight_layout()\n        fn = f\"{target_dir}/{prefix}.png\"\n        plt.savefig(fn)\n        image_filenames.append(fn)\n        plt.close()\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.__export_value_distribution_helper","title":"__export_value_distribution_helper  <code>staticmethod</code>","text":"<pre><code>__export_value_distribution_helper(\n    target_dir: str,\n    prefix: str,\n    nbins: int,\n    data: ndarray,\n    data_labels: list[str],\n    indices: list[int],\n    values_min: list[float],\n    values_max: list[float],\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>str</code> <p>description</p> required <code>prefix</code> <code>str</code> <p>description</p> required <code>nbins</code> <code>int</code> <p>description</p> required <code>data</code> <code>ndarray</code> <p>description</p> required <code>data_labels</code> <code>list[str]</code> <p>description</p> required <code>indices</code> <code>list[int]</code> <p>description</p> required <code>values_min</code> <code>list[float]</code> <p>description</p> required <code>values_max</code> <code>list[float]</code> <p>description</p> required Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __export_value_distribution_helper(\n    target_dir: str,\n    prefix: str,\n    nbins: int,\n    data: np.ndarray,\n    data_labels: list[str],\n    indices: list[int],\n    values_min: list[float],\n    values_max: list[float],\n):\n    \"\"\"_summary_\n\n    Args:\n        target_dir (str): _description_\n        prefix (str): _description_\n        nbins (int): _description_\n        data (np.ndarray): _description_\n        data_labels (list[str]): _description_\n        indices (list[int]): _description_\n        values_min (list[float]): _description_\n        values_max (list[float]): _description_\n    \"\"\"\n\n    bin_edges_list, weights_list, bin_centers_list, max_y_lim = (\n        Plotting.calculate_histograms(\n            indices=indices,\n            values_min=values_min,\n            values_max=values_max,\n            data=data,\n            nbins=nbins,\n        )\n    )\n\n    image_filenames = []\n\n    if mpi.get_rank() == 0:\n        for j in mpi.tqdm(\n            range(len(indices)),\n            desc=f\"Exporting Value Distributions for: {prefix}\",\n        ):\n            i = indices[j]\n            plt.figure(figsize=(8, 8))\n            value_range = (values_min[i], values_max[i])\n            bin_edges = bin_edges_list[j]\n            weights = weights_list[j]\n            bin_centers = bin_centers_list[j]\n\n            plt.hist(\n                x=bin_centers,\n                bins=bin_edges,\n                weights=weights,\n                edgecolor=\"black\",\n            )\n\n            plt.xlim(value_range[0], value_range[1])\n            # plt.ylim(0, max_y_lim)\n\n            plt.ylabel(\"% of samples\")\n            plt.xlabel(\"Value\")\n\n            plt.title(f\"Distribution of '{data_labels[i]}'\")\n\n            plt.tight_layout()\n            fn = f\"{target_dir}/{nbins:03d}_{prefix}_{i:03d}.png\"\n            plt.savefig(fn)\n            plt.close()\n            image_filenames.append(fn)\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.calculate_cummulative_distribution","title":"calculate_cummulative_distribution  <code>staticmethod</code>","text":"<pre><code>calculate_cummulative_distribution(\n    *, data: array, nbins: int\n) -&gt; tuple[array, array]\n</code></pre> <p>Takes the input flattened numpy array and calculates a cummulative value distribution curve with 'nbins' values at the x-axis. The curve represents the answer to a question \"What portion of data (y axis) has value v (x axis) or lower?\"</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>A flattened numpy array containing the values.</p> required <code>nbins</code> <code>int</code> <p>Resolution of the function</p> required <p>Returns:</p> Type Description <code>tuple[array, array]</code> <p>tuple[np.array, np.array]: x and y values</p> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef calculate_cummulative_distribution(\n    *,\n    data: np.array,\n    nbins: int,\n) -&gt; tuple[np.array, np.array]:\n    \"\"\"Takes the input flattened numpy array and calculates a cummulative\n    value distribution curve with 'nbins' values at the x-axis. The curve\n    represents the answer to a question \"What portion of data (y axis) has\n    value v (x axis) or lower?\"\n\n    Args:\n        data (np.array): A flattened numpy array containing the values.\n        nbins (int): Resolution of the function\n\n    Returns:\n        tuple[np.array, np.array]: x and y values\n    \"\"\"\n    if data is None:\n        msg = \"Data must not be None\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    data = np.asarray(data).ravel()\n    data = data[np.isfinite(data)]\n\n    if data.size == 0:\n        msg = \"Data must contain at least one finite number\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    if nbins &lt;= 1:\n        msg = \"nbins must be greater than 1\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    data_sorted = np.sort(data)\n    data_min, data_max = data_sorted[0], data_sorted[-1]\n\n    x_values = np.linspace(data_min, data_max, nbins, endpoint=True)\n    counts = np.searchsorted(data_sorted, x_values, side=\"right\")\n    y_values = counts / data_sorted.size\n\n    return x_values, y_values\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.calculate_histograms","title":"calculate_histograms  <code>staticmethod</code>","text":"<pre><code>calculate_histograms(\n    *,\n    predefined_min: float = None,\n    predefined_max: float = None,\n    data: ndarray,\n    nbins: int\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef calculate_histograms(\n    *,\n    predefined_min: float = None,\n    predefined_max: float = None,\n    data: np.ndarray,\n    nbins: int,\n):\n    if predefined_min is not None and predefined_max is not None:\n        value_range = (predefined_min, predefined_max)\n\n    if len(data.shape) &lt;= 1:\n        if predefined_min is None or predefined_max is None:\n            min_local = data.min()\n            max_local = data.max()\n            min_global = mpi.allreduce(min_local, mpi.OP.MIN)\n            max_global = mpi.allreduce(max_local, mpi.OP.MAX)\n            value_range = (min_global, max_global)\n\n        counts, bin_edges = np.histogram(\n            data, bins=nbins, range=value_range\n        )\n        counts = mpi.reduce(counts, mpi.OP.SUM, root=0)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) * 0.5\n        # print(counts)\n        weights = counts / np.sum(counts)\n\n        return bin_centers, weights\n\n    if predefined_min is None or predefined_max is None:\n        min_local = data.min(axis=0)\n        max_local = data.max(axis=0)\n        min_global = mpi.allreduce(min_local, mpi.OP.MIN)\n        max_global = mpi.allreduce(max_local, mpi.OP.MAX)\n        value_range = [\n            (min_global[i], max_global[i]) for i in range(data.shape[1])\n        ]\n    else:\n        value_range = [(predefined_min, predefined_max)] * data.shape[1]\n\n    bin_centers = np.zeros((data.shape[1], nbins))\n    weights = np.zeros((data.shape[1], nbins))\n    for i in range(data.shape[1]):\n        counts, bin_edges = np.histogram(\n            data[:, i], bins=nbins, range=value_range[i]\n        )\n        counts = mpi.reduce(counts, mpi.OP.SUM, root=0)\n        bin_centers[i] = (bin_edges[:-1] + bin_edges[1:]) * 0.5\n        weights[i] = counts / sum(counts)\n\n    return bin_centers, weights\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_box_plot","title":"export_box_plot  <code>staticmethod</code>","text":"<pre><code>export_box_plot(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_box_plot(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    data_keys = list(database.get_data_keys())\n    if indices is None:\n        indices = {k: None for k in data_keys}\n    image_filenames = {k: [] for k in data_keys}\n\n    values_min = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.MIN,\n    )\n    values_q1 = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.P25,\n    )\n    values_median = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.MEDIAN,\n    )\n    values_q3 = database.get_statistic(\n        data_keys=data_keys, stat_code=ENUM_StatisticsCode_Tabular.P75\n    )\n    values_max = database.get_statistic(\n        data_keys=data_keys, stat_code=ENUM_StatisticsCode_Tabular.MAX\n    )\n\n    for key in data_keys:\n        dim = database.get_dim(key=key)\n        if dim &lt;= 0:\n            continue\n\n        # data = np.array(database.get_data(key=key))\n        if indices[key] is None:\n            indices[key] = [i for i in range(dim)]\n\n        image_filenames[key] += Plotting.__export_box_plot_helper(\n            categories=database.metadata_get_value(\"labels\")[key],\n            values_min=values_min[key],\n            values_q1=values_q1[key],\n            values_median=values_median[key],\n            values_q3=values_q3[key],\n            values_max=values_max[key],\n            target_dir=target_dir,\n            prefix=key,\n            indices=indices[key],\n        )\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_correlation_matrix","title":"export_correlation_matrix  <code>staticmethod</code>","text":"<pre><code>export_correlation_matrix(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_correlation_matrix(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    data_keys = sorted(list(database.get_data_keys()))\n    dimensions = {k: database.get_dim(k) for k in data_keys}\n    labels = database.metadata_get_value(\"labels\")\n    if indices is None:\n        indices = {k: None for k in data_keys}\n    for key in data_keys:\n        if indices[key] is None:\n            indices[key] = [i for i in range(dimensions[key])]\n\n    stats_correlation_pearson = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.CORRELATION_PEARSON,\n    )\n\n    image_filenames = {}\n\n    for idx_1, key_1 in enumerate(data_keys):\n        for key_2 in data_keys[idx_1:]:\n            image_filenames[f\"{key_1}-{key_2}\"] = None\n\n    if mpi.get_rank() == 0:\n        for idx_1, key_1 in enumerate(data_keys):\n            for key_2 in data_keys[idx_1:]:\n                key = f\"{key_1}-{key_2}\"\n                if stats_correlation_pearson[key] is None:\n                    continue\n                plt.figure(figsize=(dimensions[key_1], dimensions[key_2]))\n                sns.heatmap(\n                    stats_correlation_pearson[key],\n                    annot=True,\n                    cmap=\"coolwarm\",\n                    xticklabels=labels[key_1],\n                    yticklabels=labels[key_2],\n                )\n                plt.title(\"Correlation Matrix\")\n                plt.tight_layout()\n                fn = f\"{target_dir}/{key}.png\"\n                plt.savefig(fn)\n                image_filenames[key] = fn\n                plt.close()\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_covariance_matrix","title":"export_covariance_matrix  <code>staticmethod</code>","text":"<pre><code>export_covariance_matrix(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_covariance_matrix(\n    database: DatabaseTabular,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    data_keys = sorted(list(database.get_data_keys()))\n    dimensions = {k: database.get_dim(k) for k in data_keys}\n    labels = database.metadata_get_value(\"labels\")\n    if indices is None:\n        indices = {k: None for k in data_keys}\n    for key in data_keys:\n        if indices[key] is None:\n            indices[key] = [i for i in range(dimensions[key])]\n\n    stats_covariance = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.COVARIANCE,\n    )\n\n    image_filenames = {}\n\n    for idx_1, key_1 in enumerate(data_keys):\n        for key_2 in data_keys[idx_1:]:\n            image_filenames[f\"{key_1}-{key_2}\"] = None\n    if mpi.is_master():\n        for idx_1, key_1 in enumerate(data_keys):\n            for key_2 in data_keys[idx_1:]:\n                key = f\"{key_1}-{key_2}\"\n                if stats_covariance[key] is None:\n                    continue\n                plt.figure(figsize=(dimensions[key_1], dimensions[key_2]))\n                sns.heatmap(\n                    stats_covariance[key],\n                    annot=True,\n                    cmap=\"coolwarm\",\n                    xticklabels=labels[key_1],\n                    yticklabels=labels[key_2],\n                )\n                plt.title(\"Covariance Matrix\")\n                plt.tight_layout()\n                fn = f\"{target_dir}/{key}.png\"\n                plt.savefig(fn)\n                image_filenames[key] = fn\n                plt.close()\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_pair_plots","title":"export_pair_plots  <code>staticmethod</code>","text":"<pre><code>export_pair_plots(\n    database: DatabaseTabular,\n    nbins: int,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n    compact: bool = False,\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>nbins</code> <code>int</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> <code>compact</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_pair_plots(\n    database: DatabaseTabular,\n    nbins: int,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n    compact: bool = False,\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        nbins (int): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n        compact (bool, optional): _description_. Defaults to False.\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    data_keys = sorted(list(database.get_data_keys()))\n    dimensions = {k: database.get_dim(k) for k in data_keys}\n    labels = database.metadata_get_value(\"labels\")\n    if indices is None:\n        indices = {k: None for k in data_keys}\n    for key in data_keys:\n        if indices[key] is None:\n            indices[key] = [i for i in range(dimensions[key])]\n\n    image_filenames = {}\n    for idx_1, key_1 in enumerate(data_keys):\n        for key_2 in data_keys[idx_1:]:\n            image_filenames[f\"{key_1}-{key_2}\"] = None\n\n    values_min = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.MIN,\n    )\n    values_max = database.get_statistic(\n        data_keys=data_keys, stat_code=ENUM_StatisticsCode_Tabular.MAX\n    )\n\n    bin_edges = {}\n    weights = {}\n    bin_centers = {}\n    max_y_lim = {}\n    data = {}\n    for key in data_keys:\n        if dimensions[key] &gt; 0:\n            data[key] = np.array(database.get_data(key=key))\n            (\n                bin_edges[key],\n                weights[key],\n                bin_centers[key],\n                max_y_lim[key],\n            ) = Plotting.calculate_histograms(\n                indices=indices[key],\n                values_min=values_min[key],\n                values_max=values_max[key],\n                data=data[key],\n                nbins=nbins,\n            )\n\n    data_raw = {}\n    max_y_lim = {}\n    for idx, key_1 in enumerate(data_keys):\n        if dimensions[key_1] &lt;= 0:\n            continue\n        key = f\"{key_1}-{key_1}\"\n        data_raw[key], max_y_lim[key] = (\n            Plotting.__calculate_histograms_2d_symmetric(\n                indices1=indices[key_1],\n                indices2=indices[key_1],\n                bin_edges_list1=bin_edges[key_1],\n                bin_edges_list2=bin_edges[key_1],\n                data1=data[key_1],\n                data2=data[key_1],\n            )\n        )\n        for key_2 in data_keys[idx + 1 :]:\n            if dimensions[key_2] &lt;= 0:\n                continue\n            key = f\"{key_1}-{key_2}\"\n            data_raw[key], max_y_lim[key] = (\n                Plotting.__calculate_histograms_2d_asymmetric(\n                    indices1=indices[key_1],\n                    indices2=indices[key_2],\n                    bin_edges_list1=bin_edges[key_1],\n                    bin_edges_list2=bin_edges[key_2],\n                    data1=data[key_1],\n                    data2=data[key_2],\n                )\n            )\n\n    max_y_lim_1d = np.max([v for _, v in max_y_lim.items()])\n    image_filenames = {}\n\n    if mpi.is_master():\n        for idx, key_1 in enumerate(data_keys):\n            if dimensions[key_1] &lt;= 0:\n                continue\n            key = f\"{key_1}-{key_1}\"\n            image_filenames[key] = Plotting.__export_pair_plot_helper(\n                compact=compact,\n                bin_edges_list_1=bin_edges[key_1],\n                bin_edges_list_2=bin_edges[key_1],\n                weights_list_1=weights[key_1],\n                weights_list_2=weights[key_1],\n                labels_1=labels[key_1],\n                labels_2=labels[key_1],\n                density_matrix_list=data_raw[key],\n                indices1=indices[key_1],\n                indices2=indices[key_1],\n                max_y_lim_1d=max_y_lim_1d,\n                max_y_lim_2d=max_y_lim_1d,\n                bin_centers_list_1=bin_centers[key_1],\n                bin_centers_list_2=bin_centers[key_1],\n                target_dir=target_dir,\n                prefix=f\"{nbins:03d}_discrete_{key}\",\n            )\n\n            for key_2 in data_keys[idx + 1 :]:\n                if dimensions[key_2] &lt;= 0:\n                    continue\n                key = f\"{key_1}-{key_2}\"\n                image_filenames[key] = Plotting.__export_pair_plot_helper(\n                    compact=compact,\n                    bin_edges_list_1=bin_edges[key_1],\n                    bin_edges_list_2=bin_edges[key_2],\n                    weights_list_1=weights[key_1],\n                    weights_list_2=weights[key_2],\n                    labels_1=labels[key_1],\n                    labels_2=labels[key_2],\n                    density_matrix_list=data_raw[key],\n                    indices1=indices[key_1],\n                    indices2=indices[key_2],\n                    max_y_lim_1d=max_y_lim_1d,\n                    max_y_lim_2d=max_y_lim_1d,\n                    bin_centers_list_1=bin_centers[key_1],\n                    bin_centers_list_2=bin_centers[key_2],\n                    target_dir=target_dir,\n                    prefix=f\"{nbins:03d}_discrete_{key}\",\n                )\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_sensitivity_matrix","title":"export_sensitivity_matrix  <code>staticmethod</code>","text":"<pre><code>export_sensitivity_matrix(\n    database: DatabaseTabular,\n    input_indices: list[int] = None,\n    output_indices: list[int] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> <code>None</code> <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> <code>None</code> <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_sensitivity_matrix(\n    database: DatabaseTabular,\n    input_indices: list[int] = None,\n    output_indices: list[int] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    stats_sensitivity_polynomial_matrix = database.get_statistic(\n        ENUM_StatisticsCode_Tabular.SENSITIVITY_COEFFICIENT_POLYNOMIAL\n    )\n    stats_sensitivity_goniometric_matrix_cosine = database.get_statistic(\n        ENUM_StatisticsCode_Tabular.SENSITIVITY_COEFFICIENT_GONIOMETRIC_COSINES\n    )\n    stats_sensitivity_goniometric_matrix_sine = database.get_statistic(\n        ENUM_StatisticsCode_Tabular.SENSITIVITY_COEFFICIENT_GONIOMETRIC_SINES\n    )\n    ninputs = database.metadata_get_value(\"dim_inputs\")\n    noutputs = database.metadata_get_value(\"dim_outputs\")\n\n    labels_inputs = database.metadata_get_value(\"labels_inputs\")\n    labels_outputs = database.metadata_get_value(\"labels_outputs\")\n\n    if input_indices is None:\n        input_indices = [i for i in range(ninputs)]\n\n    if output_indices is None:\n        output_indices = [i for i in range(noutputs)]\n\n    image_filenames = {\n        \"polynomial\": None,\n        \"goniometric_sine\": None,\n        \"goniometric_cosine\": None,\n    }\n\n    if mpi.get_rank() == 0:\n\n        if stats_sensitivity_polynomial_matrix[\"max_degree\"] is not None:\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_sensitivity_polynomial_matrix[\"max_degree\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_outputs,\n                yticklabels=labels_inputs,\n            )\n            plt.title(\"Polynomial Sensitivity Coefficent Matrix\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/polynomial.png\"\n            plt.savefig(fn)\n            image_filenames[\"polynomial\"] = fn\n            plt.close()\n\n        if (\n            stats_sensitivity_goniometric_matrix_sine[\"max_period\"]\n            is not None\n        ):\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_sensitivity_goniometric_matrix_sine[\"max_period\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_outputs,\n                yticklabels=labels_inputs,\n            )\n            plt.title(\"Goniometric Sensitivity Coefficent Matrix (Sine)\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/goniometric_sine.png\"\n            plt.savefig(fn)\n            image_filenames[\"goniometric_sine\"] = fn\n            plt.close()\n\n        if (\n            stats_sensitivity_goniometric_matrix_cosine[\"max_period\"]\n            is not None\n        ):\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_sensitivity_goniometric_matrix_cosine[\"max_period\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_outputs,\n                yticklabels=labels_inputs,\n            )\n            plt.title(\"Goniometric Sensitivity Coefficent Matrix (Cosine)\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/goniometric_cosine.png\"\n            plt.savefig(fn)\n            image_filenames[\"goniometric_cosine\"] = fn\n            plt.close()\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_stability_matrix","title":"export_stability_matrix  <code>staticmethod</code>","text":"<pre><code>export_stability_matrix(\n    database: DatabaseTabular,\n    input_indices: list[int] = None,\n    output_indices: list[int] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> <code>None</code> <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> <code>None</code> <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_stability_matrix(\n    database: DatabaseTabular,\n    input_indices: list[int] = None,\n    output_indices: list[int] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    stats_stability_matrix = database.get_statistic(\n        ENUM_StatisticsCode_Tabular.STABILITY_COEFFICIENT\n    )\n    ninputs = database.metadata_get_value(\"dim_inputs\")\n    noutputs = database.metadata_get_value(\"dim_outputs\")\n\n    labels_inputs = database.metadata_get_value(\"labels_inputs\")\n    labels_outputs = database.metadata_get_value(\"labels_outputs\")\n\n    if input_indices is None:\n        input_indices = [i for i in range(ninputs)]\n\n    if output_indices is None:\n        output_indices = [i for i in range(noutputs)]\n\n    image_filenames = {\n        \"inputs-inputs\": None,\n        \"inputs-outputs\": None,\n        \"outputs-outputs\": None,\n    }\n\n    if mpi.get_rank() == 0:\n\n        if stats_stability_matrix[\"inputs-inputs\"] is not None:\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_stability_matrix[\"inputs-inputs\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_inputs,\n                yticklabels=labels_inputs,\n            )\n            plt.title(\"Stability Coefficent Matrix\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/inputs-inputs.png\"\n            plt.savefig(fn)\n            image_filenames[\"inputs-inputs\"] = fn\n            plt.close()\n\n        if stats_stability_matrix[\"inputs-outputs\"] is not None:\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_stability_matrix[\"inputs-outputs\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_outputs,\n                yticklabels=labels_inputs,\n            )\n            plt.title(\"Stability Coefficent Matrix\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/inputs-outputs.png\"\n            plt.savefig(fn)\n            image_filenames[\"inputs-outputs\"] = fn\n            plt.close()\n\n        if stats_stability_matrix[\"outputs-outputs\"] is not None:\n            plt.figure(figsize=(ninputs, ninputs))\n            sns.heatmap(\n                stats_stability_matrix[\"outputs-outputs\"],\n                annot=True,\n                cmap=\"coolwarm\",\n                xticklabels=labels_outputs,\n                yticklabels=labels_outputs,\n            )\n            plt.title(\"Stability Coefficent Matrix\")\n            plt.tight_layout()\n            fn = f\"{target_dir}/outputs-outputs.png\"\n            plt.savefig(fn)\n            image_filenames[\"outputs-outputs\"] = fn\n            plt.close()\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/plots/tabular/#AI4SurrogateModelling.src.export.plots.tabular.Plotting.export_value_distribution","title":"export_value_distribution  <code>staticmethod</code>","text":"<pre><code>export_value_distribution(\n    database: DatabaseTabular,\n    nbins: int,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>DatabaseTabular</code> <p>description</p> required <code>nbins</code> <code>int</code> <p>description</p> required <code>input_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>output_indices</code> <code>list[int]</code> <p>description. Defaults to None.</p> required <code>target_dir</code> <code>str</code> <p>description. Defaults to \"exports/tabular/plots\".</p> <code>'exports/tabular/plots'</code> Source code in <code>AI4SurrogateModelling/src/export/plots/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_value_distribution(\n    database: DatabaseTabular,\n    nbins: int,\n    indices: dict[list[int]] = None,\n    target_dir: str = \"exports/tabular/plots\",\n):\n    \"\"\"_summary_\n\n    Args:\n        database (DatabaseTabular): _description_\n        nbins (int): _description_\n        input_indices (list[int], optional): _description_. Defaults to None.\n        output_indices (list[int], optional): _description_. Defaults to None.\n        target_dir (str, optional): _description_. Defaults to \"exports/tabular/plots\".\n    \"\"\"\n    if mpi.is_master():\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n    mpi.sync()\n\n    Logger.info(f\"Calculating data for distribution plot.\")\n\n    data_keys = list(database.get_data_keys())\n    values_min = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.MIN,\n    )\n    values_max = database.get_statistic(\n        data_keys=data_keys,\n        stat_code=ENUM_StatisticsCode_Tabular.MAX,\n    )\n    if indices is None:\n        indices = {k: None for k in data_keys}\n\n    image_filenames = {k: [] for k in data_keys}\n    for key in data_keys:\n        dim = database.get_dim(key=key)\n        if dim &lt;= 0:\n            continue\n\n        data = np.array(database.get_data(key=key))\n        if indices[key] is None:\n            indices[key] = [i for i in range(dim)]\n\n        image_filenames[key] += Plotting.__export_value_distribution_helper(\n            target_dir=target_dir,\n            prefix=f\"distribution_{key}\",\n            nbins=nbins,\n            data=data,\n            indices=indices[key],\n            values_min=values_min[key] - 1e-6,\n            values_max=values_max[key] + 1e-6,\n            data_labels=database.metadata_get_value(\"labels\")[key],\n        )\n\n    return image_filenames\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/","title":"Module <code>src.export.reports.general_rajko</code>","text":""},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko","title":"AI4SurrogateModelling.src.export.reports.general_rajko","text":""},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.generate_dashboard","title":"generate_dashboard","text":"<pre><code>generate_dashboard(\n    dashboard_structure,\n    output_file=\"dashboard.html\",\n    title=\"Data Dashboard\",\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def generate_dashboard(dashboard_structure, output_file=\"dashboard.html\", title=\"Data Dashboard\"):\n    dashboard_serialized = serialize_dashboard(dashboard_structure)\n    dashboard_json = json.dumps(dashboard_serialized)\n    dashboard_embedded = json.dumps(dashboard_json)\n\n    html = rf\"\"\"&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;title&gt;{title}&lt;/title&gt;\n&lt;script src=\"https://cdn.plot.ly/plotly-latest.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.tailwindcss.com\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body class=\"h-screen flex flex-col font-sans\"&gt;\n&lt;nav id=\"navbar\" class=\"flex bg-gray-800 text-white p-3 space-x-4 overflow-x-auto\"&gt;&lt;/nav&gt;\n&lt;div class=\"flex flex-1 overflow-hidden\"&gt;\n&lt;aside id=\"sidebar\" class=\"w-60 bg-gray-100 p-3 overflow-y-auto border-r border-gray-200\"&gt;&lt;/aside&gt;\n&lt;main id=\"content\" class=\"flex-1 p-4 overflow-y-auto grid grid-cols-1 md:grid-cols-2 gap-4\"&gt;&lt;/main&gt;\n&lt;/div&gt;\n&lt;div id=\"modal\" class=\"fixed inset-0 bg-black bg-opacity-70 hidden justify-center items-center z-50\"&gt;\n  &lt;div class=\"bg-white rounded-2xl p-4 w-[90%] h-[90%] flex flex-col\"&gt;\n    &lt;div class=\"flex justify-between mb-2\"&gt;\n      &lt;button id=\"modal-close\" class=\"text-2xl text-gray-500 hover:text-black\"&gt;&amp;times;&lt;/button&gt;\n      &lt;div id=\"slider-container\" class=\"flex items-center space-x-2\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n    &lt;div id=\"modal-plot\" class=\"flex-1\"&gt;&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;script&gt;\nconst dashboard = JSON.parse({dashboard_embedded});\nlet currentCategory = null;\nlet selectedSubs = [];\n\nfunction createNavbar() {{\n  const navbar = document.getElementById(\"navbar\");\n  for (const cat in dashboard) {{\n    const btn = document.createElement(\"button\");\n    btn.textContent = cat;\n    btn.className = \"px-3 py-2 rounded hover:bg-gray-600\";\n    btn.onclick = () =&gt; selectCategory(cat, btn);\n    navbar.appendChild(btn);\n  }}\n}}\n\nfunction clearActiveButtons() {{\n  document.querySelectorAll('#navbar button').forEach(btn =&gt; btn.classList.remove('bg-gray-700'));\n}}\n\nfunction selectCategory(cat, btn) {{\n  clearActiveButtons();\n  btn.classList.add('bg-gray-700');\n  currentCategory = cat;\n  selectedSubs = [];\n  const sidebar = document.getElementById(\"sidebar\");\n  const content = document.getElementById(\"content\");\n  sidebar.innerHTML = \"\";\n  content.innerHTML = \"\";\n\n  const catData = dashboard[cat];\n  if (catData.charts) {{\n    renderChartsDirect(catData.charts);\n  }} else {{\n    for (const sub in catData) {{\n      if (sub == '_supplement_data') {{\n        continue;\n      }}\n      const wrapper = document.createElement(\"div\");\n      wrapper.className = \"flex items-center mb-2\";\n      const checkbox = document.createElement(\"input\");\n      checkbox.type = \"checkbox\";\n      checkbox.id = \"chk-\" + sub;\n      checkbox.className = \"mr-2\";\n      checkbox.onchange = () =&gt; toggleSubcategory(cat, sub, checkbox.checked);\n      const label = document.createElement(\"label\");\n      label.textContent = sub;\n      label.htmlFor = checkbox.id;\n      wrapper.appendChild(checkbox);\n      wrapper.appendChild(label);\n      sidebar.appendChild(wrapper);\n    }}\n  }}\n}}\n\nfunction toggleSubcategory(cat, sub, checked) {{\n  if (checked) selectedSubs.push(sub);\n  else selectedSubs = selectedSubs.filter(s =&gt; s !== sub);\n  updateMergedCharts(cat);\n}}\n\nfunction renderChartsDirect(charts) {{\n  const content = document.getElementById(\"content\");\n  content.innerHTML = \"\";\n  for (const chartTitle in charts) {{\n    const chartData = charts[chartTitle];\n    const div = document.createElement(\"div\");\n    div.className = \"bg-white shadow rounded-lg p-2 cursor-pointer\";\n    const header = document.createElement(\"h2\");\n    header.textContent = chartTitle;\n    header.className = \"text-center text-sm font-semibold mb-2\";\n    div.appendChild(header);\n    const chartDiv = document.createElement(\"div\");\n    chartDiv.className = \"h-64\";\n    div.appendChild(chartDiv);\n    content.appendChild(div);\n\n    if (chartData.slider_data) {{\n      if(chartData.slider_data.type===\"hist\") showHistogramPlot(chartDiv, chartData.slider_data);\n      else if(chartData.slider_data.type===\"heatmap\") showHeatmapPlot(chartDiv, chartData.slider_data);\n      div.onclick = () =&gt; showSliderModal(chartData.slider_data);\n    }} else if(chartData.figure) {{\n      const fig = JSON.parse(chartData.figure);\n      Plotly.newPlot(chartDiv, fig.data, fig.layout, {{responsive:true}});\n      div.onclick = () =&gt; showModal(fig);\n    }}\n  }}\n}}\n\nfunction updateMergedCharts(cat) {{\n  const content = document.getElementById(\"content\");\n  content.innerHTML = \"\";\n  if (selectedSubs.length===0) return;\n\n  const grouped = {{}};\n  const untagged = [];\n\n  selectedSubs.forEach(sub =&gt; {{\n    if(sub=='_supplement_data'){{\n      return;\n    }}\n    const charts = dashboard[cat][sub].charts;\n    for (const chartTitle in charts) {{\n      const chartData = charts[chartTitle];\n      if(chartData.tag &amp;&amp; chartData.figure) {{\n        const tag = chartData.tag;\n        if(!grouped[tag]) grouped[tag] = [];\n        grouped[tag].push({{sub:sub, chartData:chartData}});\n      }} else {{\n        untagged.push(chartData);\n      }}\n    }}\n  }});\n\n  for(const tag in grouped) {{\n    const figs = grouped[tag];\n    // preserve sidebar order\n    const sidebarOrder = Array.from(document.getElementById(\"sidebar\").children)\n                              .map(div =&gt; div.querySelector(\"label\").textContent)\n                              .filter(label =&gt; selectedSubs.includes(label));\n    figs.sort((a,b)=&gt; sidebarOrder.indexOf(a.sub) - sidebarOrder.indexOf(b.sub));\n\n    const type0 = JSON.parse(figs[0].chartData.figure).data[0].type;\n    let mergedFig;\n    if(type0===\"heatmap\") {{\n      const supplement_dataset = dashboard[cat]._supplement_data.find(obj =&gt; obj.tag === tag);\n      const fullMatrix = supplement_dataset.full_matrix;\n      //const fullMatrix = figs[0].chartData._full_matrix;\n      const indices = figs.map(f=&gt; f.chartData.index);\n      const newMat = indices.map(i =&gt; indices.map(j =&gt; fullMatrix[i][j]));\n      mergedFig = {{data:[{{z:newMat,type:\"heatmap\",colorscale:\"Viridis\"}}], layout:{{title:\"Merged Heatmap\"}}}};\n    }} else {{\n      // Merge non-heatmap figures with distinct colors per subcategory\n      const colorPalette = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\"];\n      mergedFig = {{data:[], layout:{{}}}};\n      for(let i=0;i&lt;figs.length;i++){{\n          const subFig = JSON.parse(figs[i].chartData.figure);\n          subFig.data.forEach(trace =&gt; {{\n              trace.marker = trace.marker || {{}};\n              trace.line = trace.line || {{}};\n                if (trace.type === \"bar\") {{\n                    trace.marker.color = colorPalette[i % colorPalette.length];\n                }} else if (trace.type === \"scatter\" &amp;&amp; trace.mode.includes(\"lines\")) {{\n                    trace.line.color = colorPalette[i % colorPalette.length];\n                }} else {{\n                    trace.marker.color = colorPalette[i % colorPalette.length];\n                }}\n              mergedFig.data.push(trace);\n          }});\n      }}\n      mergedFig.layout.title = (figs[0].chartData.figure ? JSON.parse(figs[0].chartData.figure).layout.title : \"Merged Chart\")+\" (\"+figs.length+\")\";\n\n    }}\n\n    const div = document.createElement(\"div\");\n    div.className=\"bg-white shadow rounded-lg p-2 cursor-pointer\";\n    const header = document.createElement(\"h2\");\n    header.textContent = tag;\n    header.className=\"text-center text-sm font-semibold mb-2\";\n    div.appendChild(header);\n    const chartDiv = document.createElement(\"div\");\n    chartDiv.className=\"h-64\";\n    div.appendChild(chartDiv);\n    content.appendChild(div);\n    Plotly.newPlot(chartDiv, mergedFig.data, mergedFig.layout, {{responsive:true}});\n    div.onclick = () =&gt; showModal(mergedFig);\n  }}\n\n  untagged.forEach(chartData =&gt; {{\n    console.log(chartData);\n    const div = document.createElement(\"div\");\n    div.className=\"bg-white shadow rounded-lg p-2 cursor-pointer\";\n    const chartDiv = document.createElement(\"div\");\n    chartDiv.className=\"h-64\";\n    div.appendChild(chartDiv);\n    content.appendChild(div);\n\n    if(chartData.figure){{\n        const fig = JSON.parse(chartData.figure);\n        Plotly.newPlot(chartDiv, fig.data, fig.layout, {{ responsive: true }});\n        div.onclick = () =&gt; showModal(fig);\n    }}else{{\n        if (chartData.slider_data.type === \"hist\") showHistogramPlot(chartDiv, chartData.slider_data);\n        else if (chartData.slider_data.type === \"heatmap\") showHeatmapPlot(chartDiv, chartData.slider_data);\n        div.onclick = () =&gt; showSliderModal(chartData.slider_data);\n\n    }}\n  }});\n}}\n\nfunction showModal(fig) {{\n  const modal = document.getElementById(\"modal\");\n  const modalPlot = document.getElementById(\"modal-plot\");\n  document.getElementById(\"slider-container\").innerHTML=\"\";\n  modal.classList.remove(\"hidden\");\n  modalPlot.innerHTML=\"\";\n  Plotly.newPlot(modalPlot, fig.data, fig.layout, {{responsive:true}});\n}}\n\nfunction showHistogramPlot(div, sliderData) {{\n  const counts = sliderData.finest_counts;\n  const bins = sliderData.finest_bins;\n  const histData = [{{x: bins.slice(0,-1), y: counts, type:\"bar\"}}];\n  const layout = {{title: sliderData.title}};\n  Plotly.newPlot(div, histData, layout, {{responsive:true}});\n}}\n\nfunction showHeatmapPlot(div, sliderData) {{\n  const z = sliderData.finest_matrix;\n  const trace = {{z:z, type:\"heatmap\", colorscale:\"Viridis\"}};\n  const layout = {{title: sliderData.title}};\n  Plotly.newPlot(div, [trace], layout, {{responsive:true}});\n}}\n\nfunction showSliderModal(sliderData) {{\n  const modal = document.getElementById(\"modal\");\n  const modalPlot = document.getElementById(\"modal-plot\");\n  const sliderContainer = document.getElementById(\"slider-container\");\n  modal.classList.remove(\"hidden\");\n  modalPlot.innerHTML=\"\";\n  sliderContainer.innerHTML=\"\";\n\n  const maxRes = sliderData.finest_matrix ? sliderData.finest_matrix.length : sliderData.finest_counts.length;\n\n  function plot(value) {{\n    const N_bins = value;\n    if(sliderData.type===\"hist\") {{\n      const K = Math.floor(maxRes/N_bins);\n      const counts = [];\n      const edges = [];\n      for(let i=0;i&lt;maxRes;i+=K){{ counts.push(sliderData.finest_counts.slice(i,i+K).reduce((a,b)=&gt;a+b,0)); edges.push(sliderData.finest_bins[i]); }}\n      edges.push(sliderData.finest_bins[sliderData.finest_bins.length-1]);\n      const histData=[{{x: edges.slice(0,-1), y: counts, type:\"bar\"}}];\n      Plotly.newPlot(modalPlot, histData, {{title:sliderData.title}}, {{responsive:true}});\n    }} else if(sliderData.type===\"heatmap\") {{\n      const matrix=sliderData.finest_matrix;\n      const total=matrix.length;\n      const K=Math.floor(total/N_bins);\n      const newMat=[];\n      for(let i=0;i&lt;total;i+=K){{\n        const row=[];\n        for(let j=0;j&lt;total;j+=K){{\n          let sum=0;\n          for(let di=0;di&lt;K;di++){{\n            for(let dj=0;dj&lt;K;dj++){{\n              sum+=matrix[i+di][j+dj];\n            }}\n          }}\n          row.push(sum/(K*K));\n        }}\n        newMat.push(row);\n      }}\n      Plotly.newPlot(modalPlot,[{{z:newMat,type:\"heatmap\",colorscale:\"Viridis\"}}],{{title:sliderData.title}},{{responsive:true}});\n    }}\n  }}\n\n  const slider=document.createElement(\"input\");\n  slider.type=\"range\";\n  slider.min=0;\n  slider.max=sliderData.divisors.length-1;\n  slider.value=sliderData.divisors.length-1;\n  slider.step=1;\n  slider.className=\"w-48\";\n  slider.oninput=()=&gt;{{plot(sliderData.divisors[slider.value]);}};\n  sliderContainer.innerHTML=\"Resolution: \";\n  sliderContainer.appendChild(slider);\n  plot(sliderData.divisors[slider.value]);\n}}\n\ndocument.getElementById(\"modal-close\").onclick=()=&gt;{{document.getElementById(\"modal\").classList.add(\"hidden\");}};\ndocument.getElementById(\"modal\").onclick=(e)=&gt;{{if(e.target.id===\"modal\") e.target.classList.add(\"hidden\");}};\n\ncreateNavbar();\nconst firstCat=Object.keys(dashboard)[0];\nif(firstCat) selectCategory(firstCat, document.querySelector(\"#navbar button\"));\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(html)\n    print(f\"\u2705 Dashboard written to {output_file}\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.make_bar","title":"make_bar","text":"<pre><code>make_bar(subset, label)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def make_bar(subset,label):\n    df = pd.DataFrame({\n        \"x\": list(range(len(subset))),\n        \"y\": subset,\n        \"region\": [label]*len(subset)\n    })\n    return px.bar(df, x=\"x\", y=\"y\", title=f\"Bar Chart\", barmode=\"group\", color=\"region\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.make_correlation_sub","title":"make_correlation_sub","text":"<pre><code>make_correlation_sub(index, total_subs)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def make_correlation_sub(index, total_subs):\n    K = total_subs\n    full_matrix = np.random.rand(K,K).tolist()\n    fig = {\"data\":[{\"z\":[index],\"type\":\"heatmap\"}],\"layout\":{\"title\":f\"SubHeat {index}\"}}\n    return {\"charts\":{\"Correlation\":{\"tag\":\"SubHeat\",\"figure\":json.dumps(fig),\n                                        \"index\": index-1}}}\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.make_histogram","title":"make_histogram","text":"<pre><code>make_histogram(\n    *, subset: ndarray, label: str, nbins: int, title: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def make_histogram(\n  *,\n  subset: np.ndarray,\n  label: str,\n  nbins: int,\n  title: str\n):\n    df = pd.DataFrame({\n        \"x\": subset,\n        \"region\": [label]*len(subset)\n    })\n    return px.histogram(df, x=\"x\", nbins=nbins, title=title, color=\"region\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.make_line","title":"make_line","text":"<pre><code>make_line(subset, label)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def make_line(subset,label):\n    df = pd.DataFrame({\n        \"x\": list(range(len(subset))),\n        \"y\": subset,\n        \"region\": [label]*len(subset)\n    })\n    return px.line(df, x=\"x\", y=\"y\", title=f\"{label} Line\", color=\"region\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.make_scatter","title":"make_scatter","text":"<pre><code>make_scatter(subset, label)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def make_scatter(subset,label):\n    df = pd.DataFrame({\n        \"x\": list(range(len(subset))),\n        \"y\": subset,\n        \"region\": [label]*len(subset)\n    })\n    return px.scatter(df, x=\"x\", y=\"y\", title=f\" Scatter\", color=\"region\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/general_rajko/#AI4SurrogateModelling.src.export.reports.general_rajko.serialize_dashboard","title":"serialize_dashboard","text":"<pre><code>serialize_dashboard(dashboard_structure)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/general_rajko.py</code> <pre><code>def serialize_dashboard(dashboard_structure):\n    def convert(obj):\n        if isinstance(obj, dict):\n            return {k: convert(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [convert(x) for x in obj]\n        elif hasattr(obj, \"to_plotly_json\"):\n            return pio.to_json(obj)\n        else:\n            return obj\n    return convert(dashboard_structure)\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/","title":"Module <code>src.export.reports.report_general</code>","text":""},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general","title":"AI4SurrogateModelling.src.export.reports.report_general","text":""},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining","title":"ReporterTraining","text":""},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.__add_description","title":"__add_description  <code>staticmethod</code>","text":"<pre><code>__add_description(training_progress: TrainingManager)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __add_description(\n    training_progress: TrainingManager,\n):\n    attributes = training_progress.get_attributes()\n    description = \"\"\n    for k, v in attributes.items():\n        if v[0] is not None and v[1] is not None:\n            description += f\"{v[0]:30s}: {v[1]}\\n\"\n\n    ReporterTraining.__update_body(\n        code=\"{DESCRIPTION}\",\n        value=description,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.__create_dirs","title":"__create_dirs  <code>staticmethod</code>","text":"<pre><code>__create_dirs(tgt_fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __create_dirs(\n    tgt_fn: str,\n):\n    subdir = \"image_data\"\n\n    master_dir = Path(tgt_fn).parent\n    image_subdir = f\"{master_dir}/{subdir}\"\n    if mpi.is_master():\n        os.makedirs(master_dir, exist_ok=True)\n        os.makedirs(image_subdir, exist_ok=True)\n    mpi.sync()\n\n    return subdir, image_subdir\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.__generate_image","title":"__generate_image  <code>staticmethod</code>","text":"<pre><code>__generate_image(stat, image_subdir, nepochs)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __generate_image(\n    stat,\n    image_subdir,\n    nepochs,\n):\n    if mpi.get_rank() == 0:\n\n        fn_code = stat[0]\n        stat_descriptions = stat[1]\n        stat_data = stat[2]\n        npoints = stat_data.shape[0]\n        ncurves = stat_data.shape[1]\n        unit = stat[3]\n        title = stat[4]\n\n        if npoints &gt; nepochs:\n            x = np.arange(npoints)\n        else:\n            x = np.arange(1, nepochs + 1)\n\n        fn_suffix = f\"{fn_code}.png\"\n\n        plt.figure(figsize=(10, 10))\n        for curve_idx in range(ncurves):\n            plt.plot(\n                x,\n                stat_data[:, curve_idx],\n                label=f\"{stat_descriptions[curve_idx]}\",\n            )\n\n        plt.legend()\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(f\"{unit}\")\n        plt.title(f\"{title}\")\n        plt.tight_layout()\n        plt.savefig(f\"{image_subdir}/{fn_suffix}\")\n        plt.close()\n\n    fn_suffix = mpi.broadcast(fn_suffix, root=0)\n\n    return fn_suffix\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.__load_template","title":"__load_template  <code>staticmethod</code>","text":"<pre><code>__load_template()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __load_template():\n    if mpi.get_rank() &gt; 0:\n        return\n\n    with open(\"src/export/reports/template_training.html\", \"r\") as f:\n        ReporterTraining.body = f.read()\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.__update_body","title":"__update_body  <code>staticmethod</code>","text":"<pre><code>__update_body(code: str, value: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __update_body(\n    code: str,\n    value: str,\n):\n    ReporterTraining.body = ReporterTraining.body.replace(code, value)\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.ReporterTraining.export_to_html","title":"export_to_html  <code>staticmethod</code>","text":"<pre><code>export_to_html(\n    training_manager: TrainingManager, tgt_fn: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_to_html(\n    training_manager: TrainingManager,\n    tgt_fn: str,\n):\n\n    ReporterTraining.__load_template()\n    subdir, image_subdir = ReporterTraining.__create_dirs(tgt_fn)\n\n    # 'epoch_data': {},\n    # 'attribute_data': {},\n    # 'epoch_data_labels': {},\n    # 'epoch_data_unit': {},\n    # 'epoch_data_title': {},\n    # 'attributes': {},\n\n    epoch_data = training_progress.get_epoch_data()\n    epoch_data_labels = training_progress.get_epoch_data_labels()\n    epoch_data_units = training_progress.get_epoch_data_units()\n    epoch_data_titles = training_progress.get_epoch_data_titles()\n\n    Logger.info(epoch_data)\n    Logger.info(epoch_data_labels)\n    Logger.info(epoch_data_units)\n    Logger.info(epoch_data_titles)\n\n    ReporterTraining.__add_description(training_progress)\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/report_general/#AI4SurrogateModelling.src.export.reports.report_general.create_report","title":"create_report","text":"<pre><code>create_report(*, fn: str, data: list[dict])\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/report_general.py</code> <pre><code>def create_report(\n    *,\n    fn: str,\n    data: list[dict],\n):\n    def collect_paths(tree, prefix=None):\n        result = {}\n        prefix = prefix or []\n        for k, v in tree.items():\n            if not isinstance(v, dict):\n                continue\n            if any(key in v for key in ('line_plots', 'histogram_plots', 'annotated_matrix', 'annotated_matrix_symmetric')):\n                result['/'.join(prefix + [k])] = v\n            elif isinstance(v, dict):\n                result.update(collect_paths(v, prefix + [k]))\n        return result\n\n    report_data = {}\n    for D in data:\n        report_data.update(D)\n\n    flat_map = collect_paths(report_data)\n    import pathlib\n\n    script_dir = pathlib.Path(__file__).resolve().parent\n\n    with open(f'{script_dir}/template_report_general.html', 'r') as f:\n        html = f.read()\n\n    html = html.replace('$$__DATA_MAP__$$', json.dumps(flat_map, ensure_ascii=False))\n\n    try:\n        if mpi.get_rank() == 0:\n            Path(fn).write_text(html, encoding=\"utf-8\")\n        mpi.sync()\n        Logger.info(f\"The report '{fn}' has been written successfully.\")\n        mpi.print(f\"\u2705The report '{fn}' has been written successfully.\")\n    except:\n        mpi.print(f\"\u274cWriting of the report '{fn}' has failed.\")\n        Logger.warning(f\"\u274cWriting of the report '{fn}' has failed.\")\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/","title":"Module <code>src.export.reports.tabular</code>","text":""},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular","title":"AI4SurrogateModelling.src.export.reports.tabular","text":""},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular","title":"ReporterTabular","text":""},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_data_correlations","title":"__HTML_add_data_correlations  <code>staticmethod</code>","text":"<pre><code>__HTML_add_data_correlations(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_data_correlations(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n\n    image_filenames = PT.export_correlation_matrix(\n        database=database,\n        target_dir=f\"{target_dir}/correlation\",\n    )\n\n    if image_filenames[\"inputs-inputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{CORRELATION_IMAGE_I2I}\",\n            image_filenames[\"inputs-inputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"inputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{CORRELATION_IMAGE_I2O}\",\n            image_filenames[\"inputs-outputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"outputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{CORRELATION_IMAGE_O2O}\",\n            image_filenames[\"outputs-outputs\"][len(target_dir) + 1 :],\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_data_covariances","title":"__HTML_add_data_covariances  <code>staticmethod</code>","text":"<pre><code>__HTML_add_data_covariances(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_data_covariances(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n\n    image_filenames = PT.export_covariance_matrix(\n        database=database,\n        target_dir=f\"{target_dir}/covariance\",\n    )\n\n    if image_filenames[\"inputs-inputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{COVARIANCE_IMAGE_I2I}\",\n            image_filenames[\"inputs-inputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"inputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{COVARIANCE_IMAGE_I2O}\",\n            image_filenames[\"inputs-outputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"outputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{COVARIANCE_IMAGE_O2O}\",\n            image_filenames[\"outputs-outputs\"][len(target_dir) + 1 :],\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_data_pairwise_distributions","title":"__HTML_add_data_pairwise_distributions  <code>staticmethod</code>","text":"<pre><code>__HTML_add_data_pairwise_distributions(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_data_pairwise_distributions(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n\n    image_filenames = PT.export_pair_plots(\n        database=database,\n        target_dir=f\"{target_dir}/distribution_pairwise\",\n        nbins=5,\n        compact=True,\n    )\n\n    if len(image_filenames[\"inputs-inputs\"]) &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_IMAGE_I2I}\",\n            image_filenames[\"inputs-inputs\"][0][len(target_dir) + 1 :],\n        )\n\n    if len(image_filenames[\"inputs-outputs\"]) &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_IMAGE_I2O}\",\n            image_filenames[\"inputs-outputs\"][0][len(target_dir) + 1 :],\n        )\n\n    if len(image_filenames[\"inputs-outputs\"]) &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_IMAGE_O2O}\",\n            image_filenames[\"outputs-outputs\"][0][len(target_dir) + 1 :],\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_data_sensitivity","title":"__HTML_add_data_sensitivity  <code>staticmethod</code>","text":"<pre><code>__HTML_add_data_sensitivity(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_data_sensitivity(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n    image_filenames = PT.export_sensitivity_matrix(\n        database=database,\n        target_dir=f\"{target_dir}/sensitivity_coefficients\",\n    )\n\n    if image_filenames[\"polynomial\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{SENSITIVITY_IMAGE_POLYNOMIAL}\",\n            image_filenames[\"polynomial\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"goniometric_sine\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{SENSITIVITY_IMAGE_GONIOMETRIC_SINE}\",\n            image_filenames[\"goniometric_sine\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"goniometric_cosine\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{SENSITIVITY_IMAGE_GONIOMETRIC_COSINE}\",\n            image_filenames[\"goniometric_cosine\"][len(target_dir) + 1 :],\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_data_stability_coefficients","title":"__HTML_add_data_stability_coefficients  <code>staticmethod</code>","text":"<pre><code>__HTML_add_data_stability_coefficients(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_data_stability_coefficients(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n\n    image_filenames = PT.export_stability_matrix(\n        database=database,\n        target_dir=f\"{target_dir}/stability_coefficients\",\n    )\n\n    if image_filenames[\"inputs-inputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{STABILITY_IMAGE_I2I}\",\n            image_filenames[\"inputs-inputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"inputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{STABILITY_IMAGE_I2O}\",\n            image_filenames[\"inputs-outputs\"][len(target_dir) + 1 :],\n        )\n\n    if image_filenames[\"outputs-outputs\"] is not None:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{STABILITY_IMAGE_O2O}\",\n            image_filenames[\"outputs-outputs\"][len(target_dir) + 1 :],\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_dropdown_options","title":"__HTML_add_dropdown_options  <code>staticmethod</code>","text":"<pre><code>__HTML_add_dropdown_options(\n    database: DatabaseTabular, target_dir: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_dropdown_options(\n    database: DatabaseTabular,\n    target_dir: str,\n):\n    labels = database.metadata_get_value(\"labels\")\n\n    image_filenames_distributions = PT.export_value_distribution(\n        database=database,\n        nbins=50,\n        target_dir=f\"{target_dir}/distributions\",\n    )\n    image_filenames_boxplots = PT.export_box_plot(\n        database=database,\n        target_dir=f\"{target_dir}/boxplots\",\n    )\n\n    image_filenames_distributions_inputs = image_filenames_distributions[\n        \"inputs\"\n    ]\n    image_filenames_boxplots_inputs = image_filenames_boxplots[\"inputs\"]\n    if len(image_filenames_distributions_inputs) &gt; 0:\n        data_inputs = \"\"\n        for i, label in enumerate(labels['inputs']):\n            data_inputs += f'&lt;option value=\"{image_filenames_distributions_inputs[i][len(target_dir) + 1:]}\"&gt;Input {label}&lt;/option&gt;'\n\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{OPTIONS_INPUTS_DISTRIBUTION}\",\n            data_inputs,\n        )\n\n    if len(image_filenames_boxplots_inputs) &gt; 0:\n        data_inputs = \"\"\n        for i, label in enumerate(labels['inputs']):\n            data_inputs += f'&lt;option value=\"{image_filenames_boxplots_inputs[i][len(target_dir) + 1:]}\"&gt;Input {label}&lt;/option&gt;'\n\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{OPTIONS_INPUTS_BOXPLOTS}\",\n            data_inputs,\n        )\n\n    image_filenames_distributions_outputs = image_filenames_distributions[\n        \"outputs\"\n    ]\n    image_filenames_boxplots_outputs = image_filenames_boxplots[\"outputs\"]\n    if len(image_filenames_distributions_outputs) &gt; 0:\n        data_outputs = \"\"\n        for i, label in enumerate(labels['outputs']):\n            data_outputs += f'&lt;option value=\"{image_filenames_distributions_outputs[i][len(target_dir) + 1:]}\"&gt;Output {label}&lt;/option&gt;'\n\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{OPTIONS_OUTPUTS_DISTRIBUTION}\",\n            data_outputs,\n        )\n\n    if len(image_filenames_boxplots_outputs) &gt; 0:\n        data_outputs = \"\"\n        for i, label in enumerate(labels['outputs']):\n            data_outputs += f'&lt;option value=\"{image_filenames_boxplots_outputs[i][len(target_dir) + 1:]}\"&gt;Output {label}&lt;/option&gt;'\n\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{OPTIONS_OUTPUTS_BOXPLOTS}\",\n            data_outputs,\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__HTML_add_submenu","title":"__HTML_add_submenu  <code>staticmethod</code>","text":"<pre><code>__HTML_add_submenu(database: DatabaseTabular)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __HTML_add_submenu(\n    database: DatabaseTabular,\n):\n    dim_inputs = database.get_dim('inputs')\n    dim_outputs = database.get_dim('outputs')\n\n    if dim_inputs &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL2_INPUTS}\",\n            '&lt;button class=\"tab-button active\" onclick=\"showSubTab(\\'Inputs\\')\"&gt;Inputs&lt;/button&gt;',\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_INPUTS-INPUTS}\",\n            '&lt;button class=\"tab-button active\" onclick=\"showSubTab(\\'Inputs to Inputs\\')\"&gt;Inputs to Inputs&lt;/button&gt;',\n        )\n    else:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL2_INPUTS}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_INPUTS-INPUTS}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_INPUTS-OUTPUTS}\",\n            \"\",\n        )\n\n    if dim_outputs &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL2_OUTPUTS}\",\n            '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Outputs\\')\"&gt;Outputs&lt;/button&gt;',\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_OUTPUTS-OUTPUTS}\",\n            '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Outputs to Outputs\\')\"&gt;Outputs to Outputs&lt;/button&gt;',\n        )\n    else:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL2_OUTPUTS}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_OUTPUTS-OUTPUTS}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{TABS_LEVEL3_INPUTS-OUTPUTS}\",\n            \"\",\n        )\n\n    ReporterTabular.body = ReporterTabular.body.replace(\n        \"{TABS_LEVEL3_INPUTS-OUTPUTS}\",\n        '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Inputs to Outputs\\')\"&gt;Inputs to Outputs&lt;/button&gt;',\n    )\n\n    ReporterTabular.body = ReporterTabular.body.replace(\n        \"{TABS_LEVEL4_POLYNOMIAL}\",\n        '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Polynomial\\')\"&gt;Polynomial&lt;/button&gt;',\n    )\n    ReporterTabular.body = ReporterTabular.body.replace(\n        \"{TABS_LEVEL4_GONIOMETRIC_SINE}\",\n        '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Goniometric-Sine\\')\"&gt;Goniometric-Sine&lt;/button&gt;',\n    )\n    ReporterTabular.body = ReporterTabular.body.replace(\n        \"{TABS_LEVEL4_GONIOMETRIC_COSINE}\",\n        '&lt;button class=\"tab-button\" onclick=\"showSubTab(\\'Goniometric-Cosine\\')\"&gt;Goniometric-Cosine&lt;/button&gt;',\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_boxplots","title":"__add_boxplots  <code>staticmethod</code>","text":"<pre><code>__add_boxplots(\n    database: DatabaseTabular, target_dir: str, prefix: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def __add_boxplots(\n        database: DatabaseTabular,\n        target_dir: str,\n        prefix: str,\n    ):\n\n        image_filenames = PT.export_box_plot(\n            database=database,\n            target_dir=target_dir,\n        )\n\n        dim_outputs = database.metadata_get_value(\"dim_outputs\")\n        labels_outputs = database.metadata_get_value(\"labels_outputs\")\n        if dim_outputs &gt; 0 and mpi.is_master():\n            target_string = \"\"\"\n## Output Values Box-Plots\n\"\"\"\n            for i, label in enumerate(labels_outputs):\n                target_string += f\"\"\"\nOutput **{label}**\n\n![{label}]({prefix}{image_filenames['outputs'][i]})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{BOXPLOTS_OUTPUTS}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{BOXPLOTS_OUTPUTS}\",\n                \"\",\n            )\n\n        dim_inputs = database.metadata_get_value(\"dim_inputs\")\n        labels_inputs = database.metadata_get_value(\"labels_inputs\")\n        if dim_inputs &gt; 0 and mpi.is_master():\n            target_string = \"\"\"\n## Input Values Box-Plots\n\"\"\"\n            for i, label in enumerate(labels_inputs):\n                target_string += f\"\"\"\nInput **{label}**\n\n![{label}]({prefix}{image_filenames['inputs'][i]})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{BOXPLOTS_INPUTS}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{BOXPLOTS_INPUTS}\",\n                \"\",\n            )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_correlations","title":"__add_correlations  <code>staticmethod</code>","text":"<pre><code>__add_correlations(\n    database: DatabaseTabular, target_dir: str, prefix: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def __add_correlations(\n        database: DatabaseTabular,\n        target_dir: str,\n        prefix: str,\n    ):\n\n        image_filenames = PT.export_correlation_matrix(\n            database=database,\n            target_dir=target_dir,\n        )\n\n        dim_outputs = database.metadata_get_value(\"dim_outputs\")\n        dim_inputs = database.metadata_get_value(\"dim_inputs\")\n\n        if dim_outputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Output to Output Values Correlation Matrix\n![output-output]({prefix}{image_filenames['outputs-outputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_O2O}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_O2O}\",\n                \"\",\n            )\n\n        if dim_inputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Input to Input Values Correlation Matrix\n![input-input]({prefix}{image_filenames['inputs-inputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_I2I}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_I2I}\",\n                \"\",\n            )\n\n        if dim_inputs &gt; 0 and dim_outputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Input to Output Values Correlation Matrix\n![input-output]({prefix}{image_filenames['inputs-outputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_I2O}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{CORRELATION_I2O}\",\n                \"\",\n            )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_covariances","title":"__add_covariances  <code>staticmethod</code>","text":"<pre><code>__add_covariances(\n    database: DatabaseTabular, target_dir: str, prefix: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def __add_covariances(\n        database: DatabaseTabular,\n        target_dir: str,\n        prefix: str,\n    ):\n\n        image_filenames = PT.export_covariance_matrix(\n            database=database,\n            target_dir=target_dir,\n        )\n\n        dim_outputs = database.metadata_get_value(\"dim_outputs\")\n        dim_inputs = database.metadata_get_value(\"dim_inputs\")\n\n        if dim_outputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Output to Output Values Covariance Matrix\n![output-output]({prefix}{image_filenames['outputs-outputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_O2O}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_O2O}\",\n                \"\",\n            )\n\n        if dim_inputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Input to Input Values Covariance Matrix\n![input-input]({prefix}{image_filenames['inputs-inputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_I2I}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_I2I}\",\n                \"\",\n            )\n\n        if dim_inputs &gt; 0 and dim_outputs &gt; 0 and mpi.is_master():\n            target_string = f\"\"\"\n## Input to Output Values Covariance Matrix\n![input-output]({prefix}{image_filenames['inputs-outputs']})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_I2O}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{COVARIANCE_I2O}\",\n                \"\",\n            )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_description","title":"__add_description  <code>staticmethod</code>","text":"<pre><code>__add_description(database: DatabaseTabular)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __add_description(\n    database: DatabaseTabular,\n):\n    ReporterTabular.body = ReporterTabular.body.replace(\n        \"{DESCRIPTION}\",\n        database.metadata_get_value(\"description\"),\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_distributions","title":"__add_distributions  <code>staticmethod</code>","text":"<pre><code>__add_distributions(\n    database: DatabaseTabular, target_dir: str, prefix: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def __add_distributions(\n        database: DatabaseTabular,\n        target_dir: str,\n        prefix: str,\n    ):\n\n        image_filenames = PT.export_value_distribution(\n            database=database,\n            nbins=50,\n            target_dir=target_dir,\n        )\n\n        dim_outputs = database.metadata_get_value(\"dim_outputs\")\n        labels_outputs = database.metadata_get_value(\"labels_outputs\")\n        if dim_outputs &gt; 0 and mpi.is_master():\n            target_string = \"\"\"\n## Output Values Distribution\n\"\"\"\n            for i, label in enumerate(labels_outputs):\n                target_string += f\"\"\"\nOutput **{label}**\n\n![{label}]({prefix}{image_filenames['outputs'][i]})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{DISTRIBUTION_OUTPUTS}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{DISTRIBUTION_OUTPUTS}\",\n                \"\",\n            )\n\n        dim_inputs = database.metadata_get_value(\"dim_inputs\")\n        labels_inputs = database.metadata_get_value(\"labels_inputs\")\n        if dim_inputs &gt; 0 and mpi.is_master():\n            target_string = \"\"\"\n## Input Values Distribution\n\"\"\"\n            for i, label in enumerate(labels_inputs):\n                target_string += f\"\"\"\nInput **{label}**\n\n![{label}]({prefix}{image_filenames['inputs'][i]})\n\"\"\"\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{DISTRIBUTION_INPUTS}\",\n                target_string,\n            )\n        else:\n            ReporterTabular.body = ReporterTabular.body.replace(\n                \"{DISTRIBUTION_INPUTS}\",\n                \"\",\n            )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_distributions_pairwise","title":"__add_distributions_pairwise  <code>staticmethod</code>","text":"<pre><code>__add_distributions_pairwise(\n    database: DatabaseTabular,\n    target_dir: str,\n    prefix: str,\n    compact: bool = True,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def __add_distributions_pairwise(\n        database: DatabaseTabular,\n        target_dir: str,\n        prefix: str,\n        compact: bool = True,\n    ):\n\n        image_filenames = PT.export_pair_plots(\n            database=database,\n            target_dir=target_dir,\n            nbins=10,\n            compact=compact,\n        )\n\n        dim_outputs = database.metadata_get_value(\"dim_outputs\")\n        dim_inputs = database.metadata_get_value(\"dim_inputs\")\n\n        if dim_outputs &gt; 0 and mpi.is_master():\n            if compact:\n                target_string = f\"\"\"\n## Output to Output Values Distribution Matrix\n![output-output]({prefix}{image_filenames['outputs-outputs'][0]})\n\"\"\"\n                ReporterTabular.body = ReporterTabular.body.replace(\n                    \"{DISTRIBUTION_O2O}\",\n                    target_string,\n                )\n            else:\n                Logger.warning(\n                    \"Non-compact export of mutual value distributions is not implemented yet.\"\n                )\n\n        if dim_inputs &gt; 0 and mpi.is_master():\n            if compact:\n                target_string = f\"\"\"\n## Input to Input Values Distribution Matrix\n![inputs-inputs]({prefix}{image_filenames['inputs-inputs'][0]})\n\"\"\"\n                ReporterTabular.body = ReporterTabular.body.replace(\n                    \"{DISTRIBUTION_I2I}\",\n                    target_string,\n                )\n            else:\n                Logger.warning(\n                    \"Non-compact export of mutual value distributions is not implemented yet.\"\n                )\n\n        if dim_inputs &gt; 0 and dim_outputs &gt; 0 and mpi.is_master():\n            if compact:\n                target_string = f\"\"\"\n## Input to Output Values Distribution Matrix\n![inputs-outputs]({prefix}{image_filenames['inputs-outputs'][0]})\n\"\"\"\n                ReporterTabular.body = ReporterTabular.body.replace(\n                    \"{DISTRIBUTION_I2O}\",\n                    target_string,\n                )\n            else:\n                Logger.warning(\n                    \"Non-compact export of mutual value distributions is not implemented yet.\"\n                )\n\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_I2I}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_O2O}\",\n            \"\",\n        )\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{DISTRIBUTION_I2O}\",\n            \"\",\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.__add_properties","title":"__add_properties  <code>staticmethod</code>","text":"<pre><code>__add_properties(database: DatabaseTabular)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef __add_properties(\n    database: DatabaseTabular,\n):\n    dim_inputs = database.metadata_get_value(\"dim_inputs\")\n    labels_inputs = database.metadata_get_value(\"labels_inputs\")\n    if dim_inputs &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{PROPERTIES_INPUTS}\",\n            f'Dataset contains **{dim_inputs}** inputs: *{\"*, *\".join(labels_inputs)}*',\n        )\n    else:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{PROPERTIES_INPUTS}\",\n            f\"**Dataset contains no inputs!!!**\",\n        )\n\n    dim_outputs = database.metadata_get_value(\"dim_outputs\")\n    labels_outputs = database.metadata_get_value(\"labels_outputs\")\n    if dim_outputs &gt; 0:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{PROPERTIES_OUTPUTS}\",\n            f'Dataset contains **{dim_outputs}** outputs: *{\"*, *\".join(labels_outputs)}*',\n        )\n    else:\n        ReporterTabular.body = ReporterTabular.body.replace(\n            \"{PROPERTIES_OUTPUTS}\",\n            f\"**Dataset contains no outputs!!!**\",\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.export_to_html","title":"export_to_html  <code>staticmethod</code>","text":"<pre><code>export_to_html(*, database: DatabaseTabular, tgt_fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_to_html(\n    *,\n    database: DatabaseTabular,\n    tgt_fn: str,\n):\n    report_data = {\n        'Database/Distribution/Total': None,\n        'Database/Distribution/Training': None,\n        'Database/Distribution/Testing': None,\n        'Database/Distribution/Validation': None,\n\n        'Database/Correlation/Total': None,\n        'Database/Correlation/Training': None,\n        'Database/Correlation/Testing': None,\n        'Database/Correlation/Validation': None,\n\n        'Database/Covariance/Total': None,\n        'Database/Covariance/Training': None,\n        'Database/Covariance/Testing': None,\n        'Database/Covariance/Validation': None,\n\n        'Database/Pairwise Distributions/Total': None,\n        'Database/Pairwise Distributions/Training': None,\n        'Database/Pairwise Distributions/Testing': None,\n        'Database/Pairwise Distributions/Validation': None,\n    }\n\n    labels_all = database.metadata_get_value(\"labels\")\n    data_keys = sorted(list(database.get_data_keys()))\n    dimensions = {k: database.get_dim(k) for k in data_keys}\n    nentries = np.sum([dimensions[k] for k in dimensions])\n\n    ################## DATA DISTRIBUTIONS ##################\n    nbins = 2*3*5*7*11\n    data_distribution_total = {}\n    for key in data_keys:\n        if dimensions[key] &lt;= 0:\n            continue\n\n        data = np.array(database.get_data(key=key))\n        labels = sorted(list(labels_all[key]))\n        bin_centers, weights  = PT.calculate_histograms(\n            data = data,\n            nbins = nbins,\n        )\n\n        for idx, label in enumerate(labels):\n            data_distribution_total[label] = {\n                'x': [float(v) for v in bin_centers[idx]],\n                'y': [float(w) for w in weights[idx]],\n            }\n\n    report_data[f\"Database/Distribution/Total\"] = {\n        'histogram_plots': {\n            'xlabel': 'Value',\n            'ylabel': 'Occurence [%]',\n            'data': data_distribution_total\n        },\n    }\n    ################## DATA CORRELATIONS ##################\n    stats_correlation_pearson = database.get_statistic(\n        data_keys = data_keys,\n        stat_code = ENUM_StatisticsCode_Tabular.CORRELATION_PEARSON,\n    )\n    data_correlation_total = [[0 for _ in range(nentries)] for _ in range(nentries)]\n    labels_correlations = []\n    row_index = 0\n    for key_rows in data_keys:\n        if dimensions[key_rows] &lt;= 0:\n            continue\n        labels_row = sorted(list(labels_all[key_rows]))\n        labels_correlations += labels_row\n\n        col_index = 0\n        for key_cols in data_keys:\n            if dimensions[key_cols] &lt;= 0:\n                continue\n\n            if f'{key_rows}-{key_cols}' in stats_correlation_pearson:\n                correlation_subdata = stats_correlation_pearson[f'{key_rows}-{key_cols}']\n            elif f'{key_cols}-{key_rows}' in stats_correlation_pearson:\n                correlation_subdata = stats_correlation_pearson[f'{key_cols}-{key_rows}'].T\n            else:\n                msg = f'Unknown data combination key for correraltion retrieval: \"{key_rows}\", \"{key_cols}\"'\n                Logger.warning(msg)\n                raise RuntimeError(msg)\n\n            for i in range(correlation_subdata.shape[0]):\n                for j in range(correlation_subdata.shape[1]):\n                    data_correlation_total[row_index + i][col_index + j] = float(correlation_subdata[i, j])\n\n            col_index += dimensions[key_cols]\n\n        row_index += dimensions[key_rows]\n\n    report_data[f\"Database/Correlation/Total\"] = {\n        'annotated_matrix_symmetric': {\n            'title': 'Pearson\\'s Correlation Matrix',\n            'xlabel': 'Database labels',\n            'ylabel': 'Database labels',\n            'labels': labels_correlations,\n            'data': data_correlation_total,\n            'decimals': 2,\n            'colorscale': 'Viridis',\n        },\n    }\n\n    ################## DATA COVARIANCES ##################\n    stats_covariance = database.get_statistic(\n        data_keys = data_keys,\n        stat_code = ENUM_StatisticsCode_Tabular.COVARIANCE,\n    )\n    data_covariances_total = [[0 for _ in range(nentries)] for _ in range(nentries)]\n    labels_covariances = []\n    row_index = 0\n    for key_rows in data_keys:\n        if dimensions[key_rows] &lt;= 0:\n            continue\n        labels_row = sorted(list(labels_all[key_rows]))\n        labels_covariances += labels_row\n\n        col_index = 0\n        for key_cols in data_keys:\n            if dimensions[key_cols] &lt;= 0:\n                continue\n\n            if f'{key_rows}-{key_cols}' in stats_covariance:\n                covariance_subdata = stats_covariance[f'{key_rows}-{key_cols}']\n            elif f'{key_cols}-{key_rows}' in stats_covariance:\n                covariance_subdata = stats_covariance[f'{key_cols}-{key_rows}'].T\n            else:\n                msg = f'Unknown data combination key for covariance retrieval: \"{key_rows}\", \"{key_cols}\"'\n                Logger.warning(msg)\n                raise RuntimeError(msg)\n\n            for i in range(covariance_subdata.shape[0]):\n                for j in range(covariance_subdata.shape[1]):\n                    data_covariances_total[row_index + i][col_index + j] = float(covariance_subdata[i, j])\n\n            col_index += dimensions[key_cols]\n\n        row_index += dimensions[key_rows]\n\n    report_data[f\"Database/Covariance/Total\"] = {\n        'annotated_matrix_symmetric': {\n            'title': 'Covariance Matrix',\n            'xlabel': 'Database labels',\n            'ylabel': 'Database labels',\n            'labels': labels_covariances,\n            'data': data_covariances_total,\n            'decimals': 2,\n            'colorscale': 'Viridis',\n        },\n    }\n\n    ####################### END ######################\n    report_dir = tgt_fn.rsplit('/', 1)[0]\n    mpi.make_dir(report_dir)\n    create_report(fn = tgt_fn, data = [report_data])\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.export_to_md","title":"export_to_md  <code>staticmethod</code>","text":"<pre><code>export_to_md(\n    database: DatabaseTabular,\n    tgt_fn: str,\n    prefix: str = \"../\",\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>    @staticmethod\n    @Logger.logged()\n    def export_to_md(\n        database: DatabaseTabular,\n        tgt_fn: str,\n        prefix: str = \"../\",\n    ):\n\n        target_dir = Path(tgt_fn).parent\n        if mpi.is_master():\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n        mpi.sync()\n\n        ReporterTabular.body = \"\"\"# Tabular Dataset Report\n\n{DESCRIPTION}\n\n## Basic Properties\n{PROPERTIES_INPUTS}\n\n{PROPERTIES_OUTPUTS}\n\n{BOXPLOTS_INPUTS} \n{BOXPLOTS_OUTPUTS} \n{DISTRIBUTION_INPUTS} \n{DISTRIBUTION_OUTPUTS}\n{CORRELATION_I2I}\n{CORRELATION_I2O}\n{CORRELATION_O2O}\n{COVARIANCE_I2I}\n{COVARIANCE_I2O}\n{COVARIANCE_O2O}\n{DISTRIBUTION_I2I}\n{DISTRIBUTION_I2O}\n{DISTRIBUTION_O2O}\n\"\"\"\n        Logger.info(\"Adding data to the MD file.\")\n\n        ReporterTabular.__add_description(database)\n        ReporterTabular.__add_properties(database)\n        ReporterTabular.__add_boxplots(\n            database, f\"{target_dir}/boxplots\", prefix\n        )\n        ReporterTabular.__add_distributions(\n            database, f\"{target_dir}/distribution\", prefix\n        )\n        ReporterTabular.__add_correlations(\n            database, f\"{target_dir}/correlation\", prefix\n        )\n        ReporterTabular.__add_covariances(\n            database, f\"{target_dir}/covariance\", prefix\n        )\n        ReporterTabular.__add_distributions_pairwise(\n            database,\n            f\"{target_dir}/distribution_pairwise\",\n            prefix,\n            compact=True,\n        )\n\n        if mpi.is_master():\n            with open(tgt_fn, \"w\", encoding=\"utf-8\") as file:\n                file.write(ReporterTabular.body)\n</code></pre>"},{"location":"api/API%20Reference/src/export/reports/tabular/#AI4SurrogateModelling.src.export.reports.tabular.ReporterTabular.export_to_pdf","title":"export_to_pdf  <code>staticmethod</code>","text":"<pre><code>export_to_pdf(database: DatabaseTabular, tgt_fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/export/reports/tabular.py</code> <pre><code>@staticmethod\n@Logger.logged()\ndef export_to_pdf(\n    database: DatabaseTabular,\n    tgt_fn: str,\n):\n    tmp_fn = \"tmp/tmp.md\"\n    ReporterTabular.export_to_md(\n        database=database,\n        tgt_fn=tmp_fn,\n        prefix=\"\",\n    )\n\n    if mpi.is_master():\n        pypandoc.convert_file(tmp_fn, \"pdf\", outputfile=tgt_fn)\n        if os.path.exists(\"tmp\"):\n            shutil.rmtree(\"tmp\")\n</code></pre>"},{"location":"api/API%20Reference/src/importer/database_importer/","title":"Module <code>src.importer.database_importer</code>","text":""},{"location":"api/API%20Reference/src/importer/database_importer/#AI4SurrogateModelling.src.importer.database_importer","title":"AI4SurrogateModelling.src.importer.database_importer","text":"<p>Contains the parent class for all database importers related to tabular data.</p>"},{"location":"api/API%20Reference/src/importer/database_importer/#AI4SurrogateModelling.src.importer.database_importer.Importer","title":"Importer","text":"<pre><code>Importer(*, paths_tgt: list[str])\n</code></pre> <p>Serves as a parent class for all future tabular data importers.</p> <p>Initializes the Importer with the path to the file/directory containing the database to be imported. Also retrieves the information about the MPI environment.</p> <p>Parameters:</p> Name Type Description Default <code>paths_tgt</code> <code>list[str]</code> <p>A list of paths to the source files                containing the data to be imported.</p> required Source code in <code>AI4SurrogateModelling/src/importer/database_importer.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    paths_tgt: list[str],\n) -&gt; None:\n    \"\"\"Initializes the Importer with the path to the file/directory\n    containing the database to be imported. Also retrieves the information\n    about the MPI environment.\n\n    Args:\n        paths_tgt (list[str]): A list of paths to the source files\\\n            containing the data to be imported.\n    \"\"\"\n\n    self.rank, self.world_size = mpi.get_rank(), mpi.get_world_size()\n    self.data_paths = []\n    for path_tgt in paths_tgt:\n        if path_tgt is not None:\n            if not exists(path_tgt):\n                msg = f\"Source file or path '{path_tgt}' does not exist.\"\n                Logger.warning(msg)\n            else:\n                self.data_paths.append(path_tgt)\n    Logger.info(\n        f'Importer has been initialized with data paths \"{self.data_paths}\"'\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/importer/database_importer/#AI4SurrogateModelling.src.importer.database_importer.Importer.data_paths","title":"data_paths  <code>instance-attribute</code>","text":"<pre><code>data_paths = []\n</code></pre>"},{"location":"api/API%20Reference/src/importer/database_importer/#AI4SurrogateModelling.src.importer.database_importer.Importer.get_data","title":"get_data","text":"<pre><code>get_data() -&gt; tuple[list]\n</code></pre> <p>Returns data specified by self.data_path property, throws an error if not defined by the child class.</p> Source code in <code>AI4SurrogateModelling/src/importer/database_importer.py</code> <pre><code>@Logger.logged()\ndef get_data(\n    self,\n) -&gt; tuple[list]:\n    \"\"\"Returns data specified by self.data_path property, throws an error\n    if not defined by the child class.\"\"\"\n\n    msg = \"Method 'get_data' is not implemented in the Database Importer.\"\n    Logger.info(msg)\n    mpi.abort(0, msg)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/database_importer/#AI4SurrogateModelling.src.importer.database_importer.Importer.get_importer_code","title":"get_importer_code","text":"<pre><code>get_importer_code() -&gt; str\n</code></pre> <p>Returns the user specified unique code attached to this importer. This is used in databases for simple determination of whether some data should be imported or not.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A code attached to this importer.</p> Source code in <code>AI4SurrogateModelling/src/importer/database_importer.py</code> <pre><code>@Logger.logged()\ndef get_importer_code(\n    self,\n) -&gt; str:\n    \"\"\"Returns the user specified unique code attached to this importer.\n    This is used in databases for simple determination of whether some data\n    should be imported or not.\n\n    Returns:\n        str: A code attached to this importer.\n    \"\"\"\n    msg = \"Method 'get_importer_code' is not implemented in the Database\\\n        Importer. It should be custom defined for each particular importer\"\n    Logger.info(msg)\n    mpi.abort(0, msg)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular/","title":"Module <code>src.importer.tabular.database_importer_tabular</code>","text":""},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular","title":"AI4SurrogateModelling.src.importer.tabular.database_importer_tabular","text":"<p>Contains a tabular database importer parent class.</p>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular.ImporterTabular","title":"ImporterTabular","text":"<pre><code>ImporterTabular(*, paths_tgt: str)\n</code></pre> <p>               Bases: <code>Importer</code></p> <p>Specialized Importer to load tabular data with input + output pairs.</p> <p>Calls the parental initializer and performs various other operations specific to this type of Importer.</p> <p>Parameters:</p> Name Type Description Default <code>paths_tgt</code> <code>list[str]</code> <p>A list of paths to the source files                containing the data to be imported.</p> required Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    paths_tgt: str,\n):\n    \"\"\"Calls the parental initializer and performs various other operations\n    specific to this type of Importer.\n\n    Args:\n        paths_tgt (list[str]): A list of paths to the source files\\\n            containing the data to be imported.\n    \"\"\"\n\n    super().__init__(paths_tgt=paths_tgt)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/","title":"Module <code>src.importer.tabular.database_importer_tabular_csv</code>","text":""},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv","title":"AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv","text":"<p>Contains a general purpose importer to be used for text data in comma-separated-value (CSV) format.</p>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV","title":"ImporterTabularCSV","text":"<pre><code>ImporterTabularCSV(\n    *,\n    data_files: list[str],\n    delim: str,\n    headers: int,\n    data_column_groups: dict\n)\n</code></pre> <p>               Bases: <code>ImporterTabular</code></p> <p>A general purpose CSV importer class.</p> <p>Constructs the importer object to be used later.</p> <p>Parameters:</p> Name Type Description Default <code>data_files</code> <code>list[str]</code> <p>A list of paths to load the data from.</p> required <code>delim</code> <code>str</code> <p>A delimiting character sequence to form the columns.</p> required <code>headers</code> <code>int</code> <p>Number of rows to be skipped and used as column                labels instead.</p> required <code>data_column_groups</code> <code>dict</code> <p>contains key-list[int] pairs determining                the column groups defined upon this dataset.</p> required Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_csv.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    data_files: list[str],\n    delim: str,\n    headers: int,\n    data_column_groups: dict,\n):\n    \"\"\"Constructs the importer object to be used later.\n\n    Args:\n        data_files (list[str]): A list of paths to load the data from.\n        delim (str): A delimiting character sequence to form the columns.\n        headers (int): Number of rows to be skipped and used as column\\\n            labels instead.\n        data_column_groups (dict): contains key-list[int] pairs determining\\\n            the column groups defined upon this dataset.\n    \"\"\"\n    super().__init__(paths_tgt=data_files)\n    self.headers = headers\n    self.data_column_groups = data_column_groups\n    self.data_code = \"_\".join(\n        [v.replace(\"/\", \".\") for v in self.data_paths]\n    )\n    self.delim = delim\n\n    self.ncolumns = 0\n    for column_group in self.data_column_groups:\n        self.ncolumns = max(\n            self.ncolumns, max(self.data_column_groups[column_group]) + 1\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.data_code","title":"data_code  <code>instance-attribute</code>","text":"<pre><code>data_code = join(\n    [(replace(\"/\", \".\")) for v in (data_paths)]\n)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.data_column_groups","title":"data_column_groups  <code>instance-attribute</code>","text":"<pre><code>data_column_groups = data_column_groups\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.delim","title":"delim  <code>instance-attribute</code>","text":"<pre><code>delim = delim\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.headers","title":"headers  <code>instance-attribute</code>","text":"<pre><code>headers = headers\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.ncolumns","title":"ncolumns  <code>instance-attribute</code>","text":"<pre><code>ncolumns = max(\n    ncolumns, max(data_column_groups[column_group]) + 1\n)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.get_data","title":"get_data","text":"<pre><code>get_data() -&gt; tuple[dict]\n</code></pre> <p>Loads and processes the data associated with this importer. All the data is loaded on one process.</p> <p>Must be called by ALL MPI processes.</p> <p>Returns:</p> Type Description <code>tuple[dict]</code> <p>tuple[dict]: Returns 2 dictionaries in the following order: - data, contains the groups and corresponding data - labels, contains the groups and corresponding data</p> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_csv.py</code> <pre><code>@Logger.logged()\ndef get_data(\n    self,\n) -&gt; tuple[dict]:\n    \"\"\"Loads and processes the data associated with this importer.\n    All the data is loaded on one process.\n\n    Must be called by ALL MPI processes.\n\n    Returns:\n        tuple[dict]: Returns 2 dictionaries in the following order:\n            - data, contains the groups and corresponding data\n            - labels, contains the groups and corresponding data\n    \"\"\"\n\n    output_data = {k: [] for k in self.data_column_groups}\n    output_labels = {}\n\n    if mpi.get_rank() == 0:\n        data = []\n\n        for data_path in self.data_paths:\n            Logger.info(f'Loading data from file: \"{data_path}\"')\n            data.append(\n                np.loadtxt(\n                    data_path,\n                    delimiter=self.delim,\n                    skiprows=self.headers,\n                )\n            )\n        data = np.vstack(data)\n\n        if self.headers == 1:\n            with open(data_path, \"r\") as f:\n                header_data = f.readline().strip().split(self.delim)\n\n        else:\n            header_data = [f\"Column_{i:03d}\" for i in range(self.ncolumns)]\n\n        for k, v in self.data_column_groups.items():\n            output_labels[k] = [f\"{header_data[i]}\" for i in v]\n\n            output_data[k] = [\n                np.array([data[i, j] for j in v])\n                for i in range(data.shape[0])\n            ]\n\n    output_labels = mpi.broadcast(output_labels, root=0)\n\n    return output_data, output_labels\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_csv/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_csv.ImporterTabularCSV.get_importer_code","title":"get_importer_code","text":"<pre><code>get_importer_code() -&gt; str\n</code></pre> <p>Returns the user specified unique code attached to this importer. This is used in databases for simple determination of whether some data should be imported or not.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A code attached to this importer.</p> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_csv.py</code> <pre><code>@Logger.logged()\ndef get_importer_code(\n    self,\n) -&gt; str:\n    \"\"\"Returns the user specified unique code attached to this importer.\n    This is used in databases for simple determination of whether some data\n    should be imported or not.\n\n    Returns:\n        str: A code attached to this importer.\n    \"\"\"\n    return self.data_code\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/","title":"Module <code>src.importer.tabular.database_importer_tabular_dummy</code>","text":""},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy","title":"AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy","text":"<p>Contains a dummy importer class for experimentation.</p>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy","title":"ImporterTabularDummy","text":"<pre><code>ImporterTabularDummy(\n    *,\n    data_inputs: list,\n    data_outputs: list,\n    labels_inputs: list,\n    labels_outputs: list,\n    path_tgt: str = None\n)\n</code></pre> <p>               Bases: <code>ImporterTabular</code></p> <p>Takes already generated data and constructs an artifial importer on these data points. Used for testing of associated importer functionality.</p> <p>Constructs this dummy importer.</p> <p>Parameters:</p> Name Type Description Default <code>data_inputs</code> <code>list</code> <p>A list of ALL input data points.</p> required <code>data_outputs</code> <code>list</code> <p>A list of ALL output data points.</p> required <code>labels_inputs</code> <code>list</code> <p>A list of labels associated with the input                columns.</p> required <code>labels_outputs</code> <code>list</code> <p>A list of labels associated with the output                columns.</p> required <code>path_tgt</code> <code>str</code> <p>Unnecessary in this object. Defaults to None.</p> <code>None</code> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_dummy.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    data_inputs: list,\n    data_outputs: list,\n    labels_inputs: list,\n    labels_outputs: list,\n    path_tgt: str = None,\n):\n    \"\"\"Constructs this dummy importer.\n\n    Args:\n        data_inputs (list): A list of ALL input data points.\n        data_outputs (list): A list of ALL output data points.\n        labels_inputs (list): A list of labels associated with the input\\\n            columns.\n        labels_outputs (list): A list of labels associated with the output\\\n            columns.\n        path_tgt (str): Unnecessary in this object. Defaults to None.\n    \"\"\"\n    self.data_inputs = data_inputs\n    self.data_outputs = data_outputs\n    self.labels_inputs = labels_inputs\n    self.labels_outputs = labels_outputs\n    self.nentries = len(self.data_inputs)\n\n    super().__init__(path_tgt)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.data_inputs","title":"data_inputs  <code>instance-attribute</code>","text":"<pre><code>data_inputs = data_inputs\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.data_outputs","title":"data_outputs  <code>instance-attribute</code>","text":"<pre><code>data_outputs = data_outputs\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.labels_inputs","title":"labels_inputs  <code>instance-attribute</code>","text":"<pre><code>labels_inputs = labels_inputs\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.labels_outputs","title":"labels_outputs  <code>instance-attribute</code>","text":"<pre><code>labels_outputs = labels_outputs\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.nentries","title":"nentries  <code>instance-attribute</code>","text":"<pre><code>nentries = len(data_inputs)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_dummy/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_dummy.ImporterTabularDummy.get_data","title":"get_data","text":"<pre><code>get_data() -&gt; tuple[list]\n</code></pre> <p>Loads and processes the data associated with this importer. All the data is loaded on one process.</p> <p>Must be called by ALL MPI processes.</p> <p>Returns:</p> Type Description <code>tuple[list]</code> <p>tuple[list]: Returns 4 lists in the following order: - input data - output data - input labels - output labels</p> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_dummy.py</code> <pre><code>@Logger.logged()\ndef get_data(\n    self,\n) -&gt; tuple[list]:\n    \"\"\"Loads and processes the data associated with this importer.\n    All the data is loaded on one process.\n\n    Must be called by ALL MPI processes.\n\n    Returns:\n        tuple[list]: Returns 4 lists in the following order:\n            - input data\n            - output data\n            - input labels\n            - output labels\n    \"\"\"\n\n    data_inputs = [\n        self.data_inputs[i]\n        for i in range(mpi.get_rank(), self.nentries, mpi.get_world_size())\n    ]\n    data_outputs = [\n        self.data_outputs[i]\n        for i in range(mpi.get_rank(), self.nentries, mpi.get_world_size())\n    ]\n    return (\n        data_inputs,\n        data_outputs,\n        self.labels_inputs,\n        self.labels_outputs,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/","title":"Module <code>src.importer.tabular.database_importer_tabular_random</code>","text":""},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random","title":"AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random","text":"<p>Contains an importer class generating random data for experimentation.</p>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom","title":"ImporterTabularRandom","text":"<pre><code>ImporterTabularRandom(\n    *, nentries: int, data_column_groups: dict\n)\n</code></pre> <p>               Bases: <code>ImporterTabular</code></p> <p>Importer class generating random tabular data for experimentation.</p> <p>Constructs this random tabular data importer.</p> <p>Parameters:</p> Name Type Description Default <code>nentries</code> <code>int</code> <p>Number of rows to be generated.</p> required <code>data_column_groups</code> <code>dict</code> <p>contains key-list[int] pairs determining                the column groups defined upon this dataset.</p> required <code>seed</code> <code>int</code> <p>Random number generator seed.</p> required <code>data_fn</code> <code>str</code> <p>Unused parameter. Defaults to None.</p> required Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_random.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    nentries: int,\n    data_column_groups: dict,\n):\n    \"\"\"Constructs this random tabular data importer.\n\n    Args:\n        nentries (int): Number of rows to be generated.\n        data_column_groups (dict): contains key-list[int] pairs determining\\\n            the column groups defined upon this dataset.\n        seed (int): Random number generator seed.\n        data_fn (str): Unused parameter. Defaults to None.\n    \"\"\"\n\n    self.nentries = nentries\n    self.ncolumns = 0\n    self.data_column_groups = data_column_groups\n    self.data_code = 'random_importer'\n    for column_group in self.data_column_groups:\n        self.ncolumns = max(\n            self.ncolumns, max(self.data_column_groups[column_group]) + 1\n        )\n\n    super().__init__(paths_tgt = ['random_importer'])\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.data_code","title":"data_code  <code>instance-attribute</code>","text":"<pre><code>data_code = 'random_importer'\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.data_column_groups","title":"data_column_groups  <code>instance-attribute</code>","text":"<pre><code>data_column_groups = data_column_groups\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.ncolumns","title":"ncolumns  <code>instance-attribute</code>","text":"<pre><code>ncolumns = max(\n    ncolumns, max(data_column_groups[column_group]) + 1\n)\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.nentries","title":"nentries  <code>instance-attribute</code>","text":"<pre><code>nentries = nentries\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.get_data","title":"get_data","text":"<pre><code>get_data() -&gt; tuple[list]\n</code></pre> <p>Generates and returns the random data. Uses \"numpy.random.rand\".</p> <p>Must be called by ALL MPI processes.</p> <p>Returns:</p> Type Description <code>tuple[list]</code> <p>tuple[list]: Returns 4 lists in the following order: - input data - output data - input labels - output labels</p> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_random.py</code> <pre><code>@Logger.logged()\ndef get_data(\n    self,\n) -&gt; tuple[list]:\n    \"\"\"Generates and returns the random data. Uses \"numpy.random.rand\".\n\n    Must be called by ALL MPI processes.\n\n    Returns:\n        tuple[list]: Returns 4 lists in the following order:\n            - input data\n            - output data\n            - input labels\n            - output labels\n    \"\"\"\n\n    Logger.info(\n        f\"Initializing random Tabular Database with {self.nentries:d} entries\"\n    )\n\n    output_data = {}\n    output_labels = {}\n\n    if mpi.get_rank() == 0:\n        data = np.random.rand(self.nentries, self.ncolumns)\n\n        header_data = [f\"Column_{i:03d}\" for i in range(self.ncolumns)]\n\n        for k, v in self.data_column_groups.items():\n            output_labels[k] = [f\"{header_data[i]}\" for i in v]\n\n            output_data[k] = [\n                np.array([data[i, j] for j in v])\n                for i in range(data.shape[0])\n            ]\n\n    output_labels = mpi.broadcast(output_labels, root=0)\n\n    return output_data, output_labels\n</code></pre>"},{"location":"api/API%20Reference/src/importer/tabular/database_importer_tabular_random/#AI4SurrogateModelling.src.importer.tabular.database_importer_tabular_random.ImporterTabularRandom.get_importer_code","title":"get_importer_code","text":"<pre><code>get_importer_code() -&gt; str\n</code></pre> <p>Returns the user specified unique code attached to this importer. This is used in databases for simple determination of whether some data should be imported or not.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A code attached to this importer.</p> Source code in <code>AI4SurrogateModelling/src/importer/tabular/database_importer_tabular_random.py</code> <pre><code>@Logger.logged()\ndef get_importer_code(\n    self,\n) -&gt; str:\n    \"\"\"Returns the user specified unique code attached to this importer.\n    This is used in databases for simple determination of whether some data\n    should be imported or not.\n\n    Returns:\n        str: A code attached to this importer.\n    \"\"\"\n    return self.data_code\n</code></pre>"},{"location":"api/API%20Reference/src/main/data/","title":"Module <code>src.main.data</code>","text":""},{"location":"api/API%20Reference/src/main/data/#AI4SurrogateModelling.src.main.data","title":"AI4SurrogateModelling.src.main.data","text":"<p>Data related end point of the main script. Takes the relevant subparts of the configuration file and performs database related operations.</p>"},{"location":"api/API%20Reference/src/main/model/","title":"Module <code>src.main.model</code>","text":""},{"location":"api/API%20Reference/src/main/model/#AI4SurrogateModelling.src.main.model","title":"AI4SurrogateModelling.src.main.model","text":"<p>Model specification related end point of the main script. Takes the relevant subparts of the configuration file and configures model assemblies.</p>"},{"location":"api/API%20Reference/src/main/training/","title":"Module <code>src.main.training</code>","text":""},{"location":"api/API%20Reference/src/main/training/#AI4SurrogateModelling.src.main.training","title":"AI4SurrogateModelling.src.main.training","text":"<p>Model training related end point of the main script. Takes the relevant subparts of the configuration file and performs training related operations.</p>"},{"location":"api/API%20Reference/src/main/utilization/","title":"Module <code>src.main.utilization</code>","text":""},{"location":"api/API%20Reference/src/main/utilization/#AI4SurrogateModelling.src.main.utilization","title":"AI4SurrogateModelling.src.main.utilization","text":"<p>Model utilization related end point of the main script. Takes the relevant subparts of the configuration file and performs utilization related operations.</p>"},{"location":"api/API%20Reference/src/model/graph/","title":"Module <code>src.model.graph</code>","text":""},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph","title":"AI4SurrogateModelling.src.model.graph","text":""},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Edge","title":"Edge","text":"<pre><code>Edge(\n    *,\n    vertex_src: int,\n    vertex_tgt: int,\n    layer: ENUM_TransferLayers\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/graph.py</code> <pre><code>def __init__(\n    self,\n    *,\n    vertex_src: int,\n    vertex_tgt: int,\n    layer: TransferLayer,\n):\n    self.vertex_src = vertex_src\n    self.vertex_tgt = vertex_tgt\n    self.layer = layer\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Edge.layer","title":"layer  <code>instance-attribute</code>","text":"<pre><code>layer = layer\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Edge.vertex_src","title":"vertex_src  <code>instance-attribute</code>","text":"<pre><code>vertex_src = vertex_src\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Edge.vertex_tgt","title":"vertex_tgt  <code>instance-attribute</code>","text":"<pre><code>vertex_tgt = vertex_tgt\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Vertex","title":"Vertex","text":"<pre><code>Vertex(\n    *,\n    dim: int,\n    activation: ENUM_Activations,\n    bias: bool,\n    name: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/graph.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dim: int,\n    activation: Activation,\n    bias: bool,\n    name: str,\n):\n    self.dim = dim\n    self.activation = activation\n    self.bias = bias\n    self.name = name\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Vertex.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = activation\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Vertex.bias","title":"bias  <code>instance-attribute</code>","text":"<pre><code>bias = bias\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Vertex.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim = dim\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.Vertex.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.contains_cycle","title":"contains_cycle","text":"<pre><code>contains_cycle(\n    *, neighborhood: list[list[int]], u: int, v: int\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/graph.py</code> <pre><code>def contains_cycle(\n    *,\n    neighborhood: list[list[int]],\n    u: int,\n    v: int,\n):\n    # is there a path from v to u?\n    active_vertices = set([v])\n    traversed_vertices = set([v])\n\n    while len(active_vertices) &gt; 0:\n        active_vertices_new = set()\n\n        for w in active_vertices:\n            if w == u:\n                return True\n\n            for nw in neighborhood[w]:\n\n                if nw not in traversed_vertices:\n                    active_vertices_new.add(nw)\n                    traversed_vertices.add(nw)\n\n        active_vertices = active_vertices_new\n\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/model/graph/#AI4SurrogateModelling.src.model.graph.get_outward_vertex_groups","title":"get_outward_vertex_groups","text":"<pre><code>get_outward_vertex_groups(\n    *,\n    edges_outward: list[set[int]],\n    edges_inward: list[set[int]],\n    source_vertex: int,\n    target_vertex: int\n) -&gt; list[set[int]]\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/graph.py</code> <pre><code>def get_outward_vertex_groups(\n    *,\n    edges_outward: list[set[int]],\n    edges_inward: list[set[int]],\n    source_vertex: int,\n    target_vertex: int,\n) -&gt; list[set[int]]:\n    nvertices = len(edges_outward)\n\n    vertex2layerIndex_map = np.zeros(nvertices, dtype=int)\n    vertex2layerIndex_map.fill(-1)\n\n    active_layer = set([source_vertex])\n    vertex2layerIndex_map[source_vertex] = 0\n\n    layer_idx = 0\n    while len(active_layer) &gt; 0:\n        layer_idx += 1\n\n        active_layer_next = set()\n\n        for u in active_layer:\n            for v in edges_outward[u]:\n                if vertex2layerIndex_map[v] &lt; layer_idx:\n                    vertex2layerIndex_map[v] = layer_idx\n                    active_layer_next.add(v)\n\n        active_layer = active_layer_next\n\n    layers = [set() for _ in range(layer_idx)]\n    for u, layer_idx in enumerate(vertex2layerIndex_map):\n        layers[layer_idx].add(u)\n\n    return layers\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/","title":"Module <code>src.model.model_assembler</code>","text":""},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler","title":"AI4SurrogateModelling.src.model.model_assembler","text":""},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler","title":"ModelAssembler","text":"<pre><code>ModelAssembler()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def __init__(\n    self,\n):\n    self.tree_vertices = []\n    self.tree_edges = []\n    self.tree_calculated = True\n    self.root_model = None\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.root_model","title":"root_model  <code>instance-attribute</code>","text":"<pre><code>root_model = None\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.tree_calculated","title":"tree_calculated  <code>instance-attribute</code>","text":"<pre><code>tree_calculated = True\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.tree_edges","title":"tree_edges  <code>instance-attribute</code>","text":"<pre><code>tree_edges = []\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.tree_vertices","title":"tree_vertices  <code>instance-attribute</code>","text":"<pre><code>tree_vertices = []\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def __getitem__(\n    self,\n    item,\n):\n    if isinstance(item, str):\n        if item in self.assembled_models_id2model.keys():\n            return self.assembled_models_id2model[item]\n        else:\n            return self.root_model\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.add_model","title":"add_model","text":"<pre><code>add_model(\n    *,\n    parent: dict = None,\n    unique_id: str,\n    model_class: str,\n    parameters=dict\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def add_model(\n    self,\n    *,\n    parent: dict = None,\n    unique_id: str,\n    model_class: str,\n    parameters=dict,\n):\n    self.tree_calculated = False\n    self.tree_vertices.append(\n        (\n            unique_id,\n            model_class,\n            convert_dictionary_to_tuple(parameters),\n            convert_dictionary_to_tuple(parent),\n        )\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.add_transfer","title":"add_transfer","text":"<pre><code>add_transfer(\n    *,\n    unique_id: str,\n    model_class: str,\n    parameters: dict,\n    parent: dict\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>@f_not_implemented\ndef add_transfer(\n    self,\n    *,\n    unique_id: str,\n    model_class: str,\n    parameters: dict,\n    parent: dict,\n):\n    self.tree_edges.append(\n        (\n            unique_id,\n            model_class,\n            convert_dictionary_to_tuple(parameters),\n            convert_dictionary_to_tuple(parent),\n        )\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.assemble","title":"assemble","text":"<pre><code>assemble(*, dtype: ENUM_Dtypes)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def assemble(\n    self,\n    *,\n    dtype: Dtypes,\n):\n    # TODO add edges aswell\n    if not self.tree_calculated:\n        self.assemble_tree()\n\n    Logger.warning(f\"Edges downward: {self.edges_downward}\")\n    Logger.warning(f\"Edges upward: {self.edges_upward}\")\n    Logger.warning(\n        f\"Vertex processing groups: {self.vertex_processing_groups}\"\n    )\n\n    self.assembled_models_id2model = {}\n    assembled_models_parent_param2id = {}\n\n    for vertex_group in self.vertex_processing_groups:\n        for vertex in vertex_group:\n            vertex_id = self.get_vertex_id(vertex)\n            vertex_model_class = self.get_vertex_model_class(vertex)\n            vertex_model_class_parameters = (\n                self.get_vertex_model_class_parameters(vertex)\n            )\n            parent_id = self.get_parent_id(vertex)\n            parent_2_child_param_name = self.get_parent_2_child_param_name(\n                vertex\n            )\n            Logger.warning(f\"Vertex: {vertex}\")\n            Logger.warning(f\"  ID: {vertex_id}\")\n            Logger.warning(f\"  Class: {vertex_model_class}\")\n            Logger.warning(f\"  Parameters: {vertex_model_class_parameters}\")\n            Logger.warning(f\"  Parent ID: {parent_id}\")\n            Logger.warning(\n                f\"  Parent parameter name: {parent_2_child_param_name}\"\n            )\n\n            parameters = {}\n\n            for pkey, pvalue in vertex_model_class_parameters.items():\n                if pvalue == \"model\":\n                    parameters[pkey] = self.assembled_models_id2model[\n                        assembled_models_parent_param2id[vertex_id][pkey]\n                    ]\n                else:\n                    parameters[pkey] = pvalue\n\n            parameters[\"name\"] = vertex_id\n            model = vertex_model_class(**parameters).to(dtype.value)\n            self.assembled_models_id2model[vertex_id] = model\n\n            if parent_id is not None:\n                if parent_id not in assembled_models_parent_param2id.keys():\n                    assembled_models_parent_param2id[parent_id] = {}\n                assembled_models_parent_param2id[parent_id][\n                    parent_2_child_param_name\n                ] = vertex_id\n\n    self.root_model = self.assembled_models_id2model[\n        self.get_vertex_id(self.root_vertex_idx)\n    ]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.assemble_tree","title":"assemble_tree","text":"<pre><code>assemble_tree()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def assemble_tree(\n    self,\n):\n    self.nvertices = len(self.tree_vertices)\n    self.vertex2index_map = {}\n    for vertex in self.tree_vertices:\n        self.vertex2index_map[get_vertex_id(vertex)] = len(\n            self.vertex2index_map\n        )\n\n    self.root_vertex_idx = -1\n    for vertex in self.tree_vertices:\n        vertex_parent = get_vertex_parent(vertex)\n        if vertex_parent == None:\n            self.root_vertex_idx = self.vertex2index_map[\n                get_vertex_id(vertex)\n            ]\n            break\n\n    self.edges_downward = [set() for _ in range(self.nvertices)]\n    self.edges_upward = [set() for _ in range(self.nvertices)]\n    for vertex in self.tree_vertices:\n        vertex_parent = convert_tuple_to_dictionary(\n            get_vertex_parent(vertex)\n        )\n        if vertex_parent is None:\n            continue\n\n        vertex_parent_id = vertex_parent[\"parent_unique_id\"]\n\n        u = self.vertex2index_map[get_vertex_id(vertex)]\n        v = self.vertex2index_map[vertex_parent_id]\n        self.edges_upward[u].add(v)\n        self.edges_downward[v].add(u)\n\n    processed_vertices = set()\n    unprocessed_vertices = set(range(self.nvertices))\n    self.vertex_processing_groups = []\n    while len(unprocessed_vertices) &gt; 0:\n        group = set()\n\n        for u in unprocessed_vertices:\n            neighbours_downward = self.edges_downward[u].difference(\n                processed_vertices\n            )\n\n            if len(neighbours_downward) == 0:\n                group.add(u)\n\n        self.vertex_processing_groups.append(group)\n        processed_vertices |= group\n        unprocessed_vertices -= group\n\n    self.tree_calculated = True\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.clear_assembled_data","title":"clear_assembled_data","text":"<pre><code>clear_assembled_data()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def clear_assembled_data(\n    self,\n):\n    self.tree_calculated = False\n    self.assembled_models_id2model = {}\n    self.root_model = None\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.get_parent_2_child_param_name","title":"get_parent_2_child_param_name","text":"<pre><code>get_parent_2_child_param_name(vertex_index)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_parent_2_child_param_name(\n    self,\n    vertex_index,\n):\n    parent_info = self.tree_vertices[vertex_index][3]\n    if parent_info == None:\n        return None\n\n    parent_info = convert_tuple_to_dictionary(parent_info)\n    return parent_info[\"parent_param_name\"]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.get_parent_id","title":"get_parent_id","text":"<pre><code>get_parent_id(vertex_index)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_parent_id(\n    self,\n    vertex_index,\n):\n    parent_info = self.tree_vertices[vertex_index][3]\n    if parent_info == None:\n        return None\n\n    parent_info = convert_tuple_to_dictionary(parent_info)\n    return parent_info[\"parent_unique_id\"]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.get_vertex_id","title":"get_vertex_id","text":"<pre><code>get_vertex_id(vertex_index)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_id(\n    self,\n    vertex_index,\n):\n    return self.tree_vertices[vertex_index][0]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.get_vertex_model_class","title":"get_vertex_model_class","text":"<pre><code>get_vertex_model_class(vertex_index)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_model_class(\n    self,\n    vertex_index,\n):\n    class_str = self.tree_vertices[vertex_index][1].lower()\n\n    if class_str == \"mlp\":\n        return MultiLayerPerceptron\n\n    if class_str == \"autoencoder\":\n        return AutoEncoder\n\n    return mpi.abort(0, f'Unknown Model code: \"{class_str}\"')\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.ModelAssembler.get_vertex_model_class_parameters","title":"get_vertex_model_class_parameters","text":"<pre><code>get_vertex_model_class_parameters(vertex_index)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_model_class_parameters(\n    self,\n    vertex_index,\n):\n    return convert_tuple_to_dictionary(self.tree_vertices[vertex_index][2])\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.convert_dictionary_to_tuple","title":"convert_dictionary_to_tuple","text":"<pre><code>convert_dictionary_to_tuple(value)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def convert_dictionary_to_tuple(value):\n\n    if not isinstance(value, dict):\n        return value\n\n    out = []\n\n    for k, v in value.items():\n        out.append((k, convert_dictionary_to_tuple(v)))\n\n    return tuple(out)\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.convert_parameters_to_tuple","title":"convert_parameters_to_tuple","text":"<pre><code>convert_parameters_to_tuple(parameters)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def convert_parameters_to_tuple(parameters):\n    out = []\n    for k, v in parameters.items():\n        out.append((k, v))\n\n    return tuple(out)\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.convert_parent_to_tuple","title":"convert_parent_to_tuple","text":"<pre><code>convert_parent_to_tuple(parent)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def convert_parent_to_tuple(parent):\n    if parent is None:\n        return None\n    return (parent[\"parent_unique_id\"], parent[\"parent_param_name\"])\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.convert_tuple_to_dictionary","title":"convert_tuple_to_dictionary","text":"<pre><code>convert_tuple_to_dictionary(value)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def convert_tuple_to_dictionary(value):\n\n    if not isinstance(value, tuple):\n        return value\n\n    output = {}\n\n    for T in value:\n        k = T[0]\n        v = T[1]\n        output[k] = convert_tuple_to_dictionary(v)\n\n    return output\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.get_vertex_class","title":"get_vertex_class","text":"<pre><code>get_vertex_class(vertex)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_class(vertex):\n    return vertex[1]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.get_vertex_id","title":"get_vertex_id","text":"<pre><code>get_vertex_id(vertex)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_id(vertex):\n    return vertex[0]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.get_vertex_parameters","title":"get_vertex_parameters","text":"<pre><code>get_vertex_parameters(vertex)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_parameters(vertex):\n    return vertex[2]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_assembler/#AI4SurrogateModelling.src.model.model_assembler.get_vertex_parent","title":"get_vertex_parent","text":"<pre><code>get_vertex_parent(vertex)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_assembler.py</code> <pre><code>def get_vertex_parent(vertex):\n    return vertex[3]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/","title":"Module <code>src.model.model_base</code>","text":""},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base","title":"AI4SurrogateModelling.src.model.model_base","text":"<p>Contains the base pytorch model class and functions related to general manipulation with pytorch models.</p>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase","title":"ModelBase","text":"<pre><code>ModelBase(**kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>The base pytorch model from which all other pytorch models should be derived.</p> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n):\n    super(ModelBase, self).__init__()\n    self.config = ModelConfiguration.parse_configuration_recursive(\n        dict(kwargs)\n    )\n    self.config[\"__model_class_path__\"] = (\n        self.__class__.__module__ + \".\" + self.__class__.__name__\n    )\n\n    if \"object_uid\" not in self.config:\n        msg = f'Model initialization does not have \"object_uid\" set,'\n        \"saving/loading of the model WILL break\"\n        Logger.warning(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = parse_configuration_recursive(dict(kwargs))\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(*, data: dict) -&gt; dict\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    *,\n    data: dict,\n) -&gt; dict:\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_biases_group_stats","title":"get_biases_group_stats","text":"<pre><code>get_biases_group_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_biases_group_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_biases_groups","title":"get_biases_groups","text":"<pre><code>get_biases_groups()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_biases_groups(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_biases_list","title":"get_biases_list","text":"<pre><code>get_biases_list()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_biases_list(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_biases_list_stats","title":"get_biases_list_stats","text":"<pre><code>get_biases_list_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_biases_list_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_gradient","title":"get_gradient","text":"<pre><code>get_gradient() -&gt; Tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def get_gradient(\n    self,\n) -&gt; torch.Tensor:\n    gradients = []\n    for param in self.parameters():\n        if param.requires_grad:\n            if param.grad is not None:\n                gradients.append(param.grad.reshape(-1))\n    if len(gradients) == 0:\n        return torch.scalar_tensor(0)\n    return torch.cat(gradients)\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_input_spec","title":"get_input_spec  <code>abstractmethod</code>","text":"<pre><code>get_input_spec() -&gt; dict\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@abstractmethod\ndef get_input_spec(\n    self,\n) -&gt; dict:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_n_biases","title":"get_n_biases","text":"<pre><code>get_n_biases()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def get_n_biases(\n    self,\n):\n    n = 0\n    for name, param in self.named_parameters():\n        if param.requires_grad and \"bias\" in name:\n            n += param.numel()\n    return n\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_n_parameters","title":"get_n_parameters","text":"<pre><code>get_n_parameters()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def get_n_parameters(\n    self,\n):\n    n = 0\n    for param in self.parameters():\n        if param.requires_grad:\n            n += param.numel()\n    return n\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_n_weights","title":"get_n_weights","text":"<pre><code>get_n_weights()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def get_n_weights(\n    self,\n):\n    n = 0\n    for name, param in self.named_parameters():\n        if param.requires_grad and \"weight\" in name:\n            n += param.numel()\n    return n\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_output_spec","title":"get_output_spec  <code>abstractmethod</code>","text":"<pre><code>get_output_spec() -&gt; dict\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@abstractmethod\ndef get_output_spec(\n    self,\n) -&gt; dict:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; Tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def get_parameters(\n    self,\n) -&gt; torch.Tensor:\n    parameters = []\n    for param in self.parameters():\n        parameters.append(param.reshape(-1))\n    if len(parameters) == 0:\n        return torch.scalar_tensor(0)\n    return torch.cat(parameters)\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_parameters_group_stats","title":"get_parameters_group_stats","text":"<pre><code>get_parameters_group_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_parameters_group_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_parameters_groups","title":"get_parameters_groups","text":"<pre><code>get_parameters_groups()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_parameters_groups(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_parameters_list","title":"get_parameters_list","text":"<pre><code>get_parameters_list()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_parameters_list(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_parameters_list_stats","title":"get_parameters_list_stats","text":"<pre><code>get_parameters_list_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_parameters_list_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_shape_input","title":"get_shape_input  <code>abstractmethod</code>","text":"<pre><code>get_shape_input()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@abstractmethod\ndef get_shape_input(\n    self,\n):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_shape_output","title":"get_shape_output  <code>abstractmethod</code>","text":"<pre><code>get_shape_output()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@abstractmethod\ndef get_shape_output(\n    self,\n):\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_weights_group_stats","title":"get_weights_group_stats","text":"<pre><code>get_weights_group_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_weights_group_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_weights_groups","title":"get_weights_groups","text":"<pre><code>get_weights_groups()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_weights_groups(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_weights_list","title":"get_weights_list","text":"<pre><code>get_weights_list()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_weights_list(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.get_weights_list_stats","title":"get_weights_list_stats","text":"<pre><code>get_weights_list_stats()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@f_not_implemented\ndef get_weights_list_stats(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@staticmethod\ndef load_model(\n    *,\n    fn: str,\n):\n    def get_class_from_path(path):\n        module_name, class_name = path.rsplit(\".\", 1)\n        module = importlib.import_module(module_name)\n        return getattr(module, class_name)\n\n    def construct_models_recursive(\n        submodel_configs: dict, submodels: dict, obj: Any\n    ):\n\n        if isinstance(\n            obj, (int, float, bool, str, torch.dtype, type(None))\n        ):\n            return obj\n\n        if isinstance(obj, list):\n            parsed_obj = []\n            for item in obj:\n                parsed_obj.append(\n                    construct_models_recursive(\n                        submodel_configs, submodels, item\n                    )\n                )\n            return parsed_obj\n\n        if isinstance(obj, tuple):\n            parsed_obj = []\n            for item in obj:\n                parsed_obj.append(\n                    construct_models_recursive(\n                        submodel_configs, submodels, item\n                    )\n                )\n            return tuple(parsed_obj)\n\n        if isinstance(obj, dict):\n            if \"object_uid\" in obj:\n                # Logger.warning(f'OBJ: {obj}')\n\n                # this config corresponds to a new model\n                model_uid = obj[\"object_uid\"]\n                model_class_path = obj[\"__model_class_path__\"]\n                model_class = get_class_from_path(model_class_path)\n                if model_uid in submodels:\n                    return submodels[model_uid]\n\n                submodels[model_uid] = None\n                submodel_configs[model_uid] = {}\n                for k, v in obj.items():\n                    if k in (\"object_uid\", \"__model_class_path__\"):\n                        continue\n                    submodel_configs[model_uid][k] = (\n                        construct_models_recursive(\n                            submodel_configs, submodels, v\n                        )\n                    )\n                    submodel_configs[model_uid][\"object_uid\"] = model_uid\n\n                submodels[model_uid] = unparse_model(\n                    model_class(**submodel_configs[model_uid])\n                )\n                return submodels[model_uid]\n            else:\n                out = {}\n                for k, v in obj.items():\n                    out[k] = construct_models_recursive(\n                        submodel_configs, submodels, v\n                    )\n                return out\n\n    checkpoint = torch.load(fn, weights_only=False)\n    config = checkpoint[\"config\"]  # class ModelConfiguration\n\n    submodel_configs = {}\n    submodels = {}\n    construct_models_recursive(submodel_configs, submodels, config)\n    submodels[config[\"object_uid\"]].load_state_dict(\n        checkpoint[\"model_state_dict\"]\n    )\n    return parse_model(submodels[config[\"object_uid\"]])\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.parse_activation","title":"parse_activation","text":"<pre><code>parse_activation(activation_code: str) -&gt; Module\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def parse_activation(\n    self,\n    activation_code: str,\n) -&gt; nn.Module:\n    code_to_activation = {\n        \"none\": None,\n        \"identity\": None,\n        \"relu\": nn.ReLU(),\n        \"gelu\": nn.GELU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"tanh\": nn.Tanh(),\n        \"sin\": torch.sin,\n        \"cos\": torch.cos,\n        \"silu\": nn.SiLU(),\n    }\n\n    \"\"\"Takes the string representation of the activation function and \n    returns a callable pytorch object.\n\n    Args:\n        activation_code (str): The activation code to be parsed.\n\n    Returns:\n        nn.Module: A callable pytorch module or a function.\n    \"\"\"\n    code = activation_code.lower()\n    if code in code_to_activation:\n        return code_to_activation[code]\n\n    if code.startswith(\"softmax\"):\n        dim = int(code.split(\"_\")[-1])\n        return nn.Softmax(dim)\n\n    if code.startswith(\"softmin\"):\n        dim = int(code.split(\"_\")[-1])\n        return nn.Softmin(dim)\n\n    if code.startswith(\"layer_norm\"):\n        dim = int(code.split(\"_\")[-1])\n        return nn.LayerNorm(dim)\n\n    if code.startswith(\"dropout\"):\n        p = float(code.split(\"_\")[-1])\n        return nn.Dropout(p)\n\n    raise ValueError(f\"Unknown activation function code: {activation_code}\")\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.parse_activation_string","title":"parse_activation_string","text":"<pre><code>parse_activation_string(activation_string)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def parse_activation_string(\n    self,\n    activation_string,\n):\n    if activation_string is None:\n        return None\n\n    code = activation_string.lower().strip()\n    if code not in ModelBase.code_to_activation.keys():\n        mpi.abort(0, f'Unknown activation function code: \"{code}\"')\n    return ModelBase.code_to_activation[code]\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelBase.save_model","title":"save_model","text":"<pre><code>save_model(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>def save_model(\n    self,\n    *,\n    fn: str,\n):\n    if mpi.get_rank() == 0:\n        data_to_save = {\n            \"model_state_dict\": unparse_model(self).state_dict(),\n            \"config\": self.config,\n        }\n        torch.save(data_to_save, fn)\n        # Logger.warning(f'SAVING_MODEL: {self.config}')\n    mpi.sync()\n</code></pre>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelConfiguration","title":"ModelConfiguration","text":"<p>A class used to parse and store model configuration parameters.</p>"},{"location":"api/API%20Reference/src/model/model_base/#AI4SurrogateModelling.src.model.model_base.ModelConfiguration.parse_configuration_recursive","title":"parse_configuration_recursive  <code>staticmethod</code>","text":"<pre><code>parse_configuration_recursive(obj: Any) -&gt; Any\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/model_base.py</code> <pre><code>@staticmethod\ndef parse_configuration_recursive(obj: Any) -&gt; Any:\n    # print(obj)\n    if isinstance(\n        obj, torch.nn.parallel.distributed.DistributedDataParallel\n    ):\n        return ModelConfiguration.parse_configuration_recursive(\n            unparse_model(obj)\n        )\n\n    if isinstance(obj, ModelBase):\n        return obj.config\n\n    if isinstance(obj, (int, float, bool, str, torch.dtype, type(None))):\n        return obj\n\n    if isinstance(obj, dict):\n        parsed_config = {}\n        for key, value in obj.items():\n            parsed_config[key] = (\n                ModelConfiguration.parse_configuration_recursive(value)\n            )\n        return parsed_config\n\n    if isinstance(obj, list):\n        parsed_config = []\n        for item in obj:\n            parsed_config.append(\n                ModelConfiguration.parse_configuration_recursive(item)\n            )\n        return parsed_config\n\n    if isinstance(obj, tuple):\n        parsed_config = []\n        for item in obj:\n            parsed_config.append(\n                ModelConfiguration.parse_configuration_recursive(item)\n            )\n        return tuple(parsed_config)\n    raise ValueError(f\"Unsupported configuration type: {type(obj)}.\")\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/aux/","title":"Module <code>src.model.models.aux</code>","text":""},{"location":"api/API%20Reference/src/model/models/aux/#AI4SurrogateModelling.src.model.models.aux","title":"AI4SurrogateModelling.src.model.models.aux","text":""},{"location":"api/API%20Reference/src/model/models/aux/#AI4SurrogateModelling.src.model.models.aux.parse_model","title":"parse_model","text":"<pre><code>parse_model(model)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/aux.py</code> <pre><code>def parse_model(model):\n    # output = torch.compile(model.to(torch.float32).to(mpi._device))\n    if mpi._nGPUs &gt; 0:\n        return DDP(\n            model,\n            device_ids=[mpi._local_rank],\n            output_device=mpi._local_rank,\n        )\n    else:\n        return DDP(model)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/aux/#AI4SurrogateModelling.src.model.models.aux.unparse_model","title":"unparse_model","text":"<pre><code>unparse_model(model)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/aux.py</code> <pre><code>def unparse_model(model):\n    if isinstance(model, DDP):\n        return unparse_model(model.module)\n\n    if isinstance(model, torch._dynamo.eval_frame.OptimizedModule):\n        return unparse_model(model._orig_mod)\n\n    return model\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/model_wrapper/","title":"Module <code>src.model.models.model_wrapper</code>","text":""},{"location":"api/API%20Reference/src/model/models/model_wrapper/#AI4SurrogateModelling.src.model.models.model_wrapper","title":"AI4SurrogateModelling.src.model.models.model_wrapper","text":""},{"location":"api/API%20Reference/src/model/models/model_wrapper/#AI4SurrogateModelling.src.model.models.model_wrapper.parallel_model_class","title":"parallel_model_class","text":"<pre><code>parallel_model_class(model)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/model_wrapper.py</code> <pre><code>def parallel_model_class(model):\n\n    @functools.wraps(model)\n    def decorator(*args, **kwargs):\n        kwargs_new = {}\n        for k, v in kwargs.items():\n            kwargs_new[k] = unparse_model(v)\n\n        output = model(*args, **kwargs_new)\n        return parse_model(output)\n\n    return decorator\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/","title":"Module <code>src.model.models.autoencoders.ae_base</code>","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/#AI4SurrogateModelling.src.model.models.autoencoders.ae_base","title":"AI4SurrogateModelling.src.model.models.autoencoders.ae_base","text":"<p>Contains modules and functions related to general work with auto encoders.</p>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/#AI4SurrogateModelling.src.model.models.autoencoders.ae_base.AEBase","title":"AEBase","text":"<pre><code>AEBase(**kwargs)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> <p>Serves as the parent class for all future autoencoding models.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_base.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n) -&gt; None:\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/#AI4SurrogateModelling.src.model.models.autoencoders.ae_base.AEBase.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(x: Tensor) -&gt; Tensor\n</code></pre> <p>Decodes the supplied latent embedding and returns the decoded data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Embedding to be decoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Decoded embeddings.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_base.py</code> <pre><code>@abstractmethod\ndef decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decodes the supplied latent embedding and returns the decoded data.\n\n    Args:\n        x (torch.Tensor): Embedding to be decoded.\n\n    Returns:\n        torch.Tensor: Decoded embeddings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/#AI4SurrogateModelling.src.model.models.autoencoders.ae_base.AEBase.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(x: Tensor) -&gt; Tensor\n</code></pre> <p>Encodes the supplied data and returns the latent embedding.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Data to be encoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Encoded inputs.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_base.py</code> <pre><code>@abstractmethod\ndef encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encodes the supplied data and returns the latent embedding.\n\n    Args:\n        x (torch.Tensor): Data to be encoded.\n\n    Returns:\n        torch.Tensor: Encoded inputs.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_base/#AI4SurrogateModelling.src.model.models.autoencoders.ae_base.AEBase.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(data: dict) -&gt; dict\n</code></pre> <p>Takes the provided data encodes them to the latent embedding space then decodes this latent representation to get the decoded data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains a pair of values specified by the encoding and                 decoding keys.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_base.py</code> <pre><code>@abstractmethod\ndef forward(self, data: dict) -&gt; dict:\n    \"\"\"Takes the provided data encodes them to the latent embedding space\n    then decodes this latent representation to get the decoded data.\n\n    Returns:\n        dict: Contains a pair of values specified by the encoding and \\\n            decoding keys.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/","title":"Module <code>src.model.models.autoencoders.ae_simple</code>","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple","title":"AI4SurrogateModelling.src.model.models.autoencoders.ae_simple","text":"<p>Contains a simple autoencoding module for supervised learning on labeled data, and related functions.</p>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple","title":"AESimple","text":"<pre><code>AESimple(\n    *, decoder: ModelBase, encoder: ModelBase, name: str\n)\n</code></pre> <p>               Bases: <code>AEBase</code></p> <p>The very basic version of an autoencoder. Takes as inputs two already constructed models, one for encoding the inputs, the other for decoding.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_simple.py</code> <pre><code>def __init__(\n    self,\n    *,\n    decoder: ModelBase,\n    encoder: ModelBase,\n    name: str,\n) -&gt; None:\n    super().__init__(\n        name=name,\n        decoder=decoder,\n        encoder=encoder,\n    )\n\n    if encoder.get_shape_output() != decoder.get_shape_input():\n        mpi.abort(\n            0,\n            f\"Inconsistent encoder/decoder transition: {encoder.get_shape_output()} -&gt; {decoder.get_shape_input()}\",\n        )\n\n    if encoder.get_shape_input() != decoder.get_shape_output():\n        mpi.abort(\n            0,\n            f\"Inconsistent encoder/decoder basis: {encoder.get_shape_input()} -&gt; {decoder.get_shape_output()}\",\n        )\n\n    self.encoder = encoder\n    self.decoder = decoder\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = decoder\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = encoder\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple.decode","title":"decode","text":"<pre><code>decode(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Decodes the latent embeddings contained in the provided data dictionary and returns the results in another dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>dict</code> <p>A dictionary containing the encoded data.</p> required <code>data_out</code> <code>dict</code> <p>A dictionary containing the decoded data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains the decoded data.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_simple.py</code> <pre><code>def decode(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Decodes the latent embeddings contained in the provided data\n    dictionary and returns the results in another dictionary.\n\n    Args:\n        data_in (dict): A dictionary containing the encoded data.\n        data_out (dict): A dictionary containing the decoded data.\n\n    Returns:\n        dict: Contains the decoded data.\n    \"\"\"\n    return self.decoder(data_in=data_in, data_out=data_out)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple.encode","title":"encode","text":"<pre><code>encode(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Encodes the provided data and returns a dictionary containing the results.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>dict</code> <p>Contains data to be encoded.</p> required <code>data_out</code> <code>dict</code> <p>Contains data to be encoded.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains the encoded data.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_simple.py</code> <pre><code>def encode(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Encodes the provided data and returns a dictionary containing the\n    results.\n\n    Args:\n        data_in (dict): Contains data to be encoded.\n        data_out (dict): Contains data to be encoded.\n\n    Returns:\n        dict: Contains the encoded data.\n    \"\"\"\n    return self.encoder(data_in=data_in, data_out=data_out)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/ae_simple/#AI4SurrogateModelling.src.model.models.autoencoders.ae_simple.AESimple.forward","title":"forward","text":"<pre><code>forward(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Takes the input data, encodes them, decodes the encodings and returns a dictionary containing both the encodings and decodings.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>dict</code> <p>A dictionary containing the data to be auto-encoded.</p> required <code>data_out</code> <code>dict</code> <p>A dictionary containing the data to be auto-encoded.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains the latent embeddings and the associated decoded                values.</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/ae_simple.py</code> <pre><code>def forward(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Takes the input data, encodes them, decodes the encodings and returns\n    a dictionary containing both the encodings and decodings.\n\n    Args:\n        data_in (dict): A dictionary containing the data to be auto-encoded.\n        data_out (dict): A dictionary containing the data to be auto-encoded.\n\n    Returns:\n        dict: Contains the latent embeddings and the associated decoded\\\n            values.\n    \"\"\"\n    if data_out is None:\n        data_out = {}\n\n    latent_embedding = self.encode(data_in=data_in, data_out=data_out)\n    self.decode(\n        data_in=latent_embedding, data_out=data_out\n    )\n\n    return data_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/","title":"Module <code>src.model.models.autoencoders.cvae</code>","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae","title":"AI4SurrogateModelling.src.model.models.autoencoders.cvae","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional","title":"VAEConditional","text":"<pre><code>VAEConditional(\n    in_channels: int,\n    num_classes: int,\n    latent_dim: int,\n    hidden_dims: list = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>VAEBase</code></p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    num_classes: int,\n    latent_dim: int,\n    hidden_dims: list = None,\n    **kwargs,\n) -&gt; None:\n    super(VAEConditional, self).__init__()\n\n    self.latent_dim = latent_dim\n\n    self.embed_class = nn.Linear(num_classes, img_size * img_size)\n    self.embed_data = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n\n    modules = []\n    if hidden_dims is None:\n        hidden_dims = [32, 64, 128, 256, 512]\n\n    in_channels += 1  # To account for the extra label channel\n    # Build Encoder\n    for h_dim in hidden_dims:\n        modules.append(\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    out_channels=h_dim,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                ),\n                nn.BatchNorm2d(h_dim),\n                nn.LeakyReLU(),\n            )\n        )\n        in_channels = h_dim\n\n    self.encoder = nn.Sequential(*modules)\n    self.fc_mu = nn.Linear(hidden_dims[-1] * 4, latent_dim)\n    self.fc_var = nn.Linear(hidden_dims[-1] * 4, latent_dim)\n\n    # Build Decoder\n    modules = []\n\n    self.decoder_input = nn.Linear(\n        latent_dim + num_classes, hidden_dims[-1] * 4\n    )\n\n    hidden_dims.reverse()\n\n    for i in range(len(hidden_dims) - 1):\n        modules.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(\n                    hidden_dims[i],\n                    hidden_dims[i + 1],\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                ),\n                nn.BatchNorm2d(hidden_dims[i + 1]),\n                nn.LeakyReLU(),\n            )\n        )\n\n    self.decoder = nn.Sequential(*modules)\n\n    self.final_layer = nn.Sequential(\n        nn.ConvTranspose2d(\n            hidden_dims[-1],\n            hidden_dims[-1],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n        ),\n        nn.BatchNorm2d(hidden_dims[-1]),\n        nn.LeakyReLU(),\n        nn.Conv2d(\n            hidden_dims[-1], out_channels=3, kernel_size=3, padding=1\n        ),\n        nn.Tanh(),\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = Sequential(*modules)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.decoder_input","title":"decoder_input  <code>instance-attribute</code>","text":"<pre><code>decoder_input = Linear(\n    latent_dim + num_classes, hidden_dims[-1] * 4\n)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.embed_class","title":"embed_class  <code>instance-attribute</code>","text":"<pre><code>embed_class = Linear(num_classes, img_size * img_size)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.embed_data","title":"embed_data  <code>instance-attribute</code>","text":"<pre><code>embed_data = Conv2d(in_channels, in_channels, kernel_size=1)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = Sequential(*modules)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.fc_mu","title":"fc_mu  <code>instance-attribute</code>","text":"<pre><code>fc_mu = Linear(hidden_dims[-1] * 4, latent_dim)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.fc_var","title":"fc_var  <code>instance-attribute</code>","text":"<pre><code>fc_var = Linear(hidden_dims[-1] * 4, latent_dim)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.final_layer","title":"final_layer  <code>instance-attribute</code>","text":"<pre><code>final_layer = Sequential(\n    ConvTranspose2d(\n        hidden_dims[-1],\n        hidden_dims[-1],\n        kernel_size=3,\n        stride=2,\n        padding=1,\n        output_padding=1,\n    ),\n    BatchNorm2d(hidden_dims[-1]),\n    LeakyReLU(),\n    Conv2d(\n        hidden_dims[-1],\n        out_channels=3,\n        kernel_size=3,\n        padding=1,\n    ),\n    Tanh(),\n)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.decode","title":"decode","text":"<pre><code>decode(z: tensor) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def decode(self, z: torch.tensor) -&gt; torch.tensor:\n    result = self.decoder_input(z)\n    result = result.view(-1, 512, 2, 2)\n    result = self.decoder(result)\n    result = self.final_layer(result)\n    return result\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.encode","title":"encode","text":"<pre><code>encode(input: tensor) -&gt; list[tensor]\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def encode(\n    self,\n    input: torch.tensor,\n) -&gt; list[torch.tensor]:\n\n    result = self.encoder(input)\n    result = torch.flatten(result, start_dim=1)\n\n    # Split the result into mu and var components\n    # of the latent Gaussian distribution\n    mu = self.fc_mu(result)\n    log_var = self.fc_var(result)\n\n    return [mu, log_var]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.forward","title":"forward","text":"<pre><code>forward(input: Tensor, **kwargs) -&gt; List[Tensor]\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def forward(self, input: Tensor, **kwargs) -&gt; List[Tensor]:\n    y = kwargs[\"labels\"].float()\n    embedded_class = self.embed_class(y)\n    embedded_class = embedded_class.view(\n        -1, self.img_size, self.img_size\n    ).unsqueeze(1)\n    embedded_input = self.embed_data(input)\n\n    x = torch.cat([embedded_input, embedded_class], dim=1)\n    mu, log_var = self.encode(x)\n\n    z = self.reparameterize(mu, log_var)\n\n    z = torch.cat([z, y], dim=1)\n    return [self.decode(z), input, mu, log_var]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.generate","title":"generate","text":"<pre><code>generate(x: Tensor, **kwargs) -&gt; Tensor\n</code></pre> <p>Given an input image x, returns the reconstructed image :param x:</p> <p>(Tensor) [B x C x H x W] :return: (Tensor) [B x C x H x W]</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def generate(self, x: Tensor, **kwargs) -&gt; Tensor:\n    \"\"\"Given an input image x, returns the reconstructed image :param x:\n\n    (Tensor) [B x C x H x W] :return: (Tensor) [B x C x H x W]\n    \"\"\"\n\n    return self.forward(x, **kwargs)[0]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.loss_function","title":"loss_function","text":"<pre><code>loss_function(*args, **kwargs) -&gt; dict\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def loss_function(self, *args, **kwargs) -&gt; dict:\n    recons = args[0]\n    input = args[1]\n    mu = args[2]\n    log_var = args[3]\n\n    kld_weight = kwargs[\n        \"M_N\"\n    ]  # Account for the minibatch samples from the dataset\n    recons_loss = F.mse_loss(recons, input)\n\n    kld_loss = torch.mean(\n        -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n    )\n\n    loss = recons_loss + kld_weight * kld_loss\n    return {\n        \"loss\": loss,\n        \"Reconstruction_Loss\": recons_loss,\n        \"KLD\": -kld_loss,\n    }\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.reparameterize","title":"reparameterize","text":"<pre><code>reparameterize(mu: Tensor, logvar: Tensor) -&gt; Tensor\n</code></pre> <p>Will a single z be enough ti compute the expectation for the loss??</p> <p>:param mu: (Tensor) Mean of the latent Gaussian :param logvar: (Tensor) Standard deviation of the latent Gaussian :return:</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def reparameterize(self, mu: Tensor, logvar: Tensor) -&gt; Tensor:\n    \"\"\"Will a single z be enough ti compute the expectation for the loss??\n\n    :param mu: (Tensor) Mean of the latent Gaussian :param logvar:\n    (Tensor) Standard deviation of the latent Gaussian :return:\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return eps * std + mu\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/cvae/#AI4SurrogateModelling.src.model.models.autoencoders.cvae.VAEConditional.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int, current_device: int, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Samples from the latent space and return the corresponding image space map.</p> <p>:param num_samples: (Int) Number of samples :param current_device: (Int) Device to run the model :return: (Tensor)</p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/cvae.py</code> <pre><code>def sample(self, num_samples: int, current_device: int, **kwargs) -&gt; Tensor:\n    \"\"\"Samples from the latent space and return the corresponding image\n    space map.\n\n    :param num_samples: (Int) Number of samples :param\n    current_device: (Int) Device to run the model :return: (Tensor)\n    \"\"\"\n    y = kwargs[\"labels\"].float()\n    z = torch.randn(num_samples, self.latent_dim)\n\n    z = z.to(current_device)\n\n    z = torch.cat([z, y], dim=1)\n    samples = self.decode(z)\n    return samples\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/","title":"Module <code>src.model.models.autoencoders.vae_base</code>","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base","title":"AI4SurrogateModelling.src.model.models.autoencoders.vae_base","text":""},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase","title":"VAEBase","text":"<pre><code>VAEBase()\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>def __init__(self) -&gt; None:\n    super(VAEBase, self).__init__()\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.decode","title":"decode","text":"<pre><code>decode(input: tensor) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>def decode(self, input: torch.tensor) -&gt; torch.tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.encode","title":"encode","text":"<pre><code>encode(input: tensor) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>def encode(self, input: torch.tensor) -&gt; torch.tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(*inputs: tensor) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>@abstractmethod\ndef forward(self, *inputs: torch.tensor) -&gt; torch.tensor:\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.generate","title":"generate","text":"<pre><code>generate(x: tensor, **kwargs) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>def generate(self, x: torch.tensor, **kwargs) -&gt; torch.tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.loss_function","title":"loss_function  <code>abstractmethod</code>","text":"<pre><code>loss_function(*inputs: tensor, **kwargs) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>@abstractmethod\ndef loss_function(self, *inputs: torch.tensor, **kwargs) -&gt; torch.tensor:\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/autoencoders/vae_base/#AI4SurrogateModelling.src.model.models.autoencoders.vae_base.VAEBase.sample","title":"sample","text":"<pre><code>sample(\n    batch_size: int, current_device: int, **kwargs\n) -&gt; tensor\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/autoencoders/vae_base.py</code> <pre><code>def sample(\n    self, batch_size: int, current_device: int, **kwargs\n) -&gt; torch.tensor:\n    raise NotImplementedError\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/","title":"Module <code>src.model.models.classifiers.classifier_simple</code>","text":""},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple","title":"AI4SurrogateModelling.src.model.models.classifiers.classifier_simple","text":""},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.ClassifierSimple","title":"ClassifierSimple","text":"<pre><code>ClassifierSimple(layers_cardinality, dim_input, nclasses)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __init__(\n    self,\n    layers_cardinality,\n    dim_input,\n    nclasses,\n):\n    super(ClassifierSimple, self).__init__()\n\n    num_layers_hidden = len(layers_cardinality)\n    layers = []\n    if num_layers_hidden &gt; 0:\n        layers.append(torch.nn.Linear(dim_input, layers_cardinality[0]))\n        layers.append(torch.nn.ReLU())\n\n        for i in range(num_layers_hidden - 1):\n            layers.append(\n                torch.nn.Linear(\n                    layers_cardinality[i], layers_cardinality[i + 1]\n                )\n            )\n            layers.append(torch.nn.ReLU())\n\n        layers.append(\n            torch.nn.Linear(\n                layers_cardinality[num_layers_hidden - 1], nclasses\n            )\n        )\n\n    else:\n        layers.append(torch.nn.Linear(dim_input, nclasses))\n\n    layers.append(torch.nn.Sigmoid())\n\n    self.model = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.ClassifierSimple.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layers)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.ClassifierSimple.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def forward(self, x):\n    return self.model(x)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion","title":"__criterion","text":"<pre><code>__criterion(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion(X, Y, W):\n    # return criterion_precision(X, Y, W)\n    # return criterion_recall(X, Y, W)\n    # return criterion_F1(X, Y, W)\n    return __criterion_SE(X, Y, W)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_F1","title":"__criterion_F1","text":"<pre><code>__criterion_F1(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_F1(X, Y, W):\n    TP = torch.sum(X * Y * W)\n    TN = torch.sum((1 - X) * (1 - Y) * W)\n    FP = torch.sum(X * (1 - Y) * W)\n    FN = torch.sum((1 - X) * Y * W)\n\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    F1 = 2 * precision * recall / (precision + recall)\n\n    return 1.0 - F1\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_SE","title":"__criterion_SE","text":"<pre><code>__criterion_SE(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_SE(X, Y, W):\n    return torch.sum((X - Y) ** 2 * W)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_accuracy","title":"__criterion_accuracy","text":"<pre><code>__criterion_accuracy(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_accuracy(X, Y, W):\n    TP = torch.sum(X * Y * W)\n    TN = torch.sum((1 - X) * (1 - Y) * W)\n    FP = torch.sum(X * (1 - Y) * W)\n    FN = torch.sum((1 - X) * Y * W)\n\n    return 1 - (TP + TN)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_binary_cross_entropy","title":"__criterion_binary_cross_entropy","text":"<pre><code>__criterion_binary_cross_entropy(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_binary_cross_entropy(X, Y, W):\n    crit = nn.BCELoss(weight=W)\n    return crit(X, Y)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_cross_entropy","title":"__criterion_cross_entropy","text":"<pre><code>__criterion_cross_entropy(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_cross_entropy(X, Y, W):\n    crit = nn.CrossEntropyLoss(weight=W)\n    return crit(X, Y)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_precision","title":"__criterion_precision","text":"<pre><code>__criterion_precision(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_precision(X, Y, W):\n    TP = torch.sum(X * Y * W)\n    TN = torch.sum((1 - X) * (1 - Y) * W)\n    FP = torch.sum(X * (1 - Y) * W)\n    FN = torch.sum((1 - X) * Y * W)\n    return 1.0 - TP / (TP + FP)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__criterion_recall","title":"__criterion_recall","text":"<pre><code>__criterion_recall(X, Y, W)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __criterion_recall(X, Y, W):\n    TP = torch.sum(X * Y * W)\n    TN = torch.sum((1 - X) * (1 - Y) * W)\n    FP = torch.sum(X * (1 - Y) * W)\n    FN = torch.sum((1 - X) * Y * W)\n    return 1.0 - TP / (TP + FN)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.__get_stats","title":"__get_stats","text":"<pre><code>__get_stats(model, dataloader, nclasses: int, desc: str)\n</code></pre> <p>For each class C[i], calculate the following values:</p> <p>TP: how many of the predicted values marked as C[i] are also expected to be C[i]? FP: how many of the predicted values marked as C[i] are NOT expected to be C[i]?</p> <p>TN: how many of the predicted values marked as NOT C[i] are NOT expected to be C[i]? FN: how many of the predicted values marked as NOT C[i] are indeed expected to be C[i]?</p> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>def __get_stats(\n    model,\n    dataloader,\n    nclasses: int,\n    desc: str,\n):\n    \"\"\"For each class C[i], calculate the following values:\n\n    TP: how many of the predicted values marked as C[i] are also expected to be C[i]?\n    FP: how many of the predicted values marked as C[i] are NOT expected to be C[i]?\n\n    TN: how many of the predicted values marked as NOT C[i] are NOT expected to be C[i]?\n    FN: how many of the predicted values marked as NOT C[i] are indeed expected to be C[i]?\n    \"\"\"\n    model.eval()\n    # the model will classify a class correctly\n    cT = np.zeros(nclasses)\n    # the model will classify a class incorrectly\n    cF = np.zeros(nclasses)\n    with torch.no_grad():\n        for data, labels in mpi.tqdm(dataloader, desc=desc):\n            labels_inferred = model(data)\n\n            for i in range(labels.shape[0]):\n                Pi = torch.sort(labels_inferred[i])[0]\n                for j in range(labels.shape[1]):\n                    if labels[i, j] &gt; 0.50:\n                        if labels_inferred[i, j] &gt;= Pi[-1]:\n                            cT[j] += 1\n                        else:\n                            cF[j] += 1\n\n    cT = mpi.allreduce(cT, mpi.OP.SUM)\n    cF = mpi.allreduce(cF, mpi.OP.SUM)\n\n    return cT, cF\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.classifier_train","title":"classifier_train","text":"<pre><code>classifier_train(\n    model,\n    training_progress: TrainingProgress,\n    dataset: DatabaseTabular,\n    learning_rate: float,\n    l2_lambda: float,\n    l1_lambda: float,\n    nepochs: int,\n    batch_size: int,\n    train_ratio: float,\n    validation_ratio: float,\n    optimizer=None,\n    scheduler=None,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>@Logger.logged()\ndef classifier_train(\n    model,\n    training_progress: TrainingProgress,\n    dataset: DatabaseTabular,\n    learning_rate: float,\n    l2_lambda: float,\n    l1_lambda: float,\n    nepochs: int,\n    batch_size: int,\n    train_ratio: float,\n    validation_ratio: float,\n    optimizer=None,\n    scheduler=None,\n):\n    if optimizer == None:\n        optimizer = torch.optim.Adam(\n            params=model.parameters(), lr=learning_rate, weight_decay=l2_lambda\n        )\n    if scheduler == None:\n        scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer, start_factor=1.0, end_factor=0.5, total_iters=30\n        )\n\n    base_LR = optimizer.param_groups[0][\"lr\"]\n\n    nclasses = dataset.get_dim_outputs()\n\n    training_progress.reset()\n\n    training_progress.set_epoch_labels(\n        \"model_biases_norm\", [\"Mean values of model |biases|\"]\n    )\n    training_progress.set_epoch_unit(\"model_biases_norm\", \"Value\")\n    training_progress.set_epoch_title(\n        \"model_biases_norm\", \"Mean values of the model biases\"\n    )\n\n    training_progress.set_epoch_labels(\n        \"model_weights_norm\", [\"Mean values of model |weights|\"]\n    )\n    training_progress.set_epoch_unit(\"model_weights_norm\", \"Value\")\n    training_progress.set_epoch_title(\n        \"model_weights_norm\", \"Mean values of the model weights\"\n    )\n\n    training_progress.set_epoch_labels(\n        \"model_biases_shift\", [\"Mean values of model bias |shifts|\"]\n    )\n    training_progress.set_epoch_unit(\"model_biases_shift\", \"Value\")\n    training_progress.set_epoch_title(\n        \"model_biases_shift\",\n        \"Mean values of the shift in model biases between epochs\",\n    )\n\n    training_progress.set_epoch_labels(\n        \"model_weights_shift\", [\"Mean values of model weights |shifts|\"]\n    )\n    training_progress.set_epoch_unit(\"model_weights_shift\", \"Value\")\n    training_progress.set_epoch_title(\n        \"model_weights_shift\",\n        \"Mean values of the shift in model weights between epochs\",\n    )\n\n    training_progress.set_epoch_labels(\n        \"learning_rates\", [\"Learning rate multiplier\"]\n    )\n    training_progress.set_epoch_unit(\"learning_rates\", \"Value\")\n    training_progress.set_epoch_title(\n        \"learning_rates\",\n        \"Learning rate multiplier of the initial learning rate\",\n    )\n\n    training_progress.set_epoch_labels(\"criterion_history\", [\"Criterion [MSE]\"])\n    training_progress.set_epoch_unit(\"criterion_history\", \"Value\")\n    training_progress.set_epoch_title(\n        \"criterion_history\", \"History of the measured metrics\"\n    )\n\n    training_progress.set_epoch_labels(\n        \"pT\",\n        [\n            f\"Probability of correct prediction [class {i}]\"\n            for i in range(nclasses)\n        ],\n    )\n    training_progress.set_epoch_unit(\"pT\", \"[%]\")\n    training_progress.set_epoch_title(\"pT\", \"Correct prediction probabilities\")\n\n    training_progress.set_epoch_labels(\n        \"pF\",\n        [\n            f\"Probability of incorrect prediction [class {i}]\"\n            for i in range(nclasses)\n        ],\n    )\n    training_progress.set_epoch_unit(\"pF\", \"[%]\")\n    training_progress.set_epoch_title(\"pF\", \"False prediction probabilities\")\n\n    training_progress.set_attribute(\n        \"learning_rate\", \"Initial learning rate\", learning_rate\n    )\n    training_progress.set_attribute(\n        \"l2_lambda\", \"L2 normalization coefficient\", l2_lambda\n    )\n    training_progress.set_attribute(\n        \"l1_lambda\", \"L1 normalization coefficient\", l1_lambda\n    )\n    training_progress.set_attribute(\"batch_size\", \"Batch size\", batch_size)\n    training_progress.set_attribute(\"train_ratio\", \"Train ratio\", train_ratio)\n    training_progress.set_attribute(\n        \"validation_ratio\", \"Validation ratio\", validation_ratio\n    )\n    training_progress.set_attribute(\"optimizer\", \"Optimizer\", optimizer)\n    training_progress.set_attribute(\"scheduler\", \"Scheduler\", scheduler)\n\n    dataloader_train, dataloader_validation = dataset.get_dataloaders(\n        train_ratio,\n        validation_ratio,\n        batch_size,\n    )\n\n    n_train_loc = len(dataloader_train.dataset)\n    n_validation_loc = len(dataloader_validation.dataset)\n    n_train_glob = mpi.allreduce(n_train_loc, mpi.OP.SUM)\n    n_validation_glob = mpi.allreduce(n_validation_loc, mpi.OP.SUM)\n    classes_n_train = mpi.allreduce(\n        np.sum(\n            [\n                dataloader_train.dataset[i][1].detach().cpu().numpy()\n                for i in range(n_train_loc)\n            ],\n            axis=0,\n        ),\n        op=mpi.OP.SUM,\n    )\n    classes_n_validation = mpi.allreduce(\n        np.sum(\n            [\n                dataloader_validation.dataset[i][1].detach().cpu().numpy()\n                for i in range(n_validation_loc)\n            ],\n            axis=0,\n        ),\n        op=mpi.OP.SUM,\n    )\n\n    W = torch.tensor(1 / classes_n_train, dtype=torch.float32).reshape(1, -1)\n    W /= torch.sum(W)\n    Logger.warning(f\"Classification weights: {W}\")\n\n    previous_biases = get_model_biases(model)\n    previous_weights = get_model_weights(model)\n    training_progress.append_epoch_value(\n        \"model_biases_norm\", np.mean(np.abs(previous_biases))\n    )\n    training_progress.append_epoch_value(\n        \"model_weights_norm\", np.mean(np.abs(previous_weights))\n    )\n    for epoch in range(nepochs):\n        current_LR = optimizer.param_groups[0][\"lr\"]\n\n        loss_validation = get_loss(\n            dataloader=dataloader_validation,\n            model=model,\n            l1_lambda=l1_lambda,\n            W=W,\n            desc=f\"Validation loss\",\n            epoch=epoch % 30,\n        )\n        loss_validation /= n_validation_glob\n\n        loss_train = get_loss(\n            dataloader=dataloader_train,\n            model=model,\n            l1_lambda=l1_lambda,\n            W=W,\n            desc=f\"Training loss ({epoch + 1:6d}/{nepochs})\",\n            epoch=epoch % 30,\n            optimizer=optimizer,\n            scheduler=scheduler,\n        )\n        loss_train /= n_train_glob\n\n        cT_train, cF_train = __get_stats(\n            model,\n            dataloader_train,\n            nclasses,\n            desc=\"Calculating classification statistics for Training dataset\",\n        )\n        pT_train = cT_train / classes_n_train\n        pF_train = cF_train / classes_n_train\n\n        cT_validation, cF_validation = __get_stats(\n            model,\n            dataloader_validation,\n            nclasses,\n            desc=\"Calculating classification statistics for Validation dataset\",\n        )\n        pT_validation = cT_validation / classes_n_validation\n        pF_validation = cF_validation / classes_n_validation\n\n        current_biases = get_model_biases(model)\n        current_weights = get_model_weights(model)\n        training_progress.append_epoch_value(\n            \"model_biases_shift\",\n            np.mean(np.abs(current_biases - previous_biases)),\n        )\n        training_progress.append_epoch_value(\n            \"model_weights_shift\",\n            np.mean(np.abs(current_weights - previous_weights)),\n        )\n        training_progress.append_epoch_value(\n            \"model_biases_norm\", np.mean(np.abs(previous_biases))\n        )\n        training_progress.append_epoch_value(\n            \"model_weights_norm\", np.mean(np.abs(previous_weights))\n        )\n\n        previous_biases = current_biases\n        previous_weights = current_weights\n\n        training_progress.append_epoch_value(\n            \"learning_rates\", np.array(current_LR / base_LR)\n        )\n\n        training_progress.append_epoch_value(\n            \"criterion_history\", np.array([loss_train, loss_validation])\n        )\n        training_progress.append_epoch_value(\n            \"pT\", np.array([pT_train, pT_validation])\n        )\n        training_progress.append_epoch_value(\n            \"pT\", np.array([pF_train, pF_validation])\n        )\n        Logger.info(\n            f\"lr[{np.log10(current_LR / base_LR):6.4f}] -&gt; Loss[train]: {loss_train / n_train_glob:20.12f}, Loss[validation]: {loss_validation / n_validation_glob:20.12f}, T[t]: {pT_train} %, F[t]: {pF_train} %, T[v]: {pT_validation} %, F[v]: {pF_validation} %\"\n        )\n        training_progress.save()\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.get_loss","title":"get_loss","text":"<pre><code>get_loss(\n    dataloader,\n    model,\n    l1_lambda: float,\n    W,\n    desc,\n    epoch,\n    optimizer=None,\n    scheduler=None,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>@Logger.logged()\ndef get_loss(\n    dataloader,\n    model,\n    l1_lambda: float,\n    W,\n    desc,\n    epoch,\n    optimizer=None,\n    scheduler=None,\n):\n    loss = 0\n    for data, labels in mpi.tqdm(dataloader, desc=desc):\n        if optimizer is not None:\n            optimizer.zero_grad()\n\n            labels_inferred = model(data)\n            l1_norm = sum(p.abs().sum() for p in model.parameters())\n            loss_batch = (\n                __criterion(labels_inferred, labels, W) + l1_lambda * l1_norm\n            ) / data.shape[0]\n            loss_batch.backward()\n            loss += loss_batch.item() * data.shape[0]\n\n            optimizer.step()\n\n            if scheduler is not None:\n                scheduler.step(epoch)\n\n        else:\n            with torch.no_grad():\n                labels_inferred = model(data)\n                loss += __criterion(labels_inferred, labels, W).item()\n\n    return loss\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.get_model_biases","title":"get_model_biases","text":"<pre><code>get_model_biases(model: Module)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>@Logger.logged()\ndef get_model_biases(model: nn.Module):\n    output = [\n        p.detach().cpu().numpy().flatten()\n        for name, p in model.named_parameters()\n        if \"bias\" in name\n    ]\n    return np.concatenate(output)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/classifiers/classifier_simple/#AI4SurrogateModelling.src.model.models.classifiers.classifier_simple.get_model_weights","title":"get_model_weights","text":"<pre><code>get_model_weights(model: Module)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/classifiers/classifier_simple.py</code> <pre><code>@Logger.logged()\ndef get_model_weights(model: nn.Module):\n    output = [\n        p.detach().cpu().numpy().flatten()\n        for name, p in model.named_parameters()\n        if \"bias\" not in name\n    ]\n    return np.concatenate(output)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/bias/","title":"Module <code>src.model.models.layers.bias</code>","text":""},{"location":"api/API%20Reference/src/model/models/layers/bias/#AI4SurrogateModelling.src.model.models.layers.bias","title":"AI4SurrogateModelling.src.model.models.layers.bias","text":"<p>Contains modules and functions related to a simple bias application to vector data.</p>"},{"location":"api/API%20Reference/src/model/models/layers/bias/#AI4SurrogateModelling.src.model.models.layers.bias.BiasLayer","title":"BiasLayer","text":"<pre><code>BiasLayer(*, dtype: dtype, n: int, name: str)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> <p>Constructs a simple bias parameter.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>datatype of the parameters</p> required <code>n</code> <code>int</code> <p>number of biases to construct</p> required <code>name</code> <code>str</code> <p>Internal name of this object, for logging and reporting                purposes.</p> required Source code in <code>AI4SurrogateModelling/src/model/models/layers/bias.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dtype: torch.dtype,\n    n: int,\n    name: str,\n):\n    \"\"\"Constructs a simple bias parameter.\n\n    Args:\n        dtype (torch.dtype): datatype of the parameters\n        n (int): number of biases to construct\n        name (str): Internal name of this object, for logging and reporting\\\n            purposes.\n    \"\"\"\n    super().__init__(name=name)\n    self.bias = nn.Parameter(torch.rand(n)).to(dtype)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/bias/#AI4SurrogateModelling.src.model.models.layers.bias.BiasLayer.bias","title":"bias  <code>instance-attribute</code>","text":"<pre><code>bias = to(dtype)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/bias/#AI4SurrogateModelling.src.model.models.layers.bias.BiasLayer.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the bias to the input data.</p> Source code in <code>AI4SurrogateModelling/src/model/models/layers/bias.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Applies the bias to the input data.\"\"\"\n    return x + self.bias\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/fourier_features/","title":"Module <code>src.model.models.layers.fourier_features</code>","text":""},{"location":"api/API%20Reference/src/model/models/layers/fourier_features/#AI4SurrogateModelling.src.model.models.layers.fourier_features","title":"AI4SurrogateModelling.src.model.models.layers.fourier_features","text":""},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/","title":"Module <code>src.model.models.layers.gaussian_noise</code>","text":""},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise","title":"AI4SurrogateModelling.src.model.models.layers.gaussian_noise","text":""},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise","title":"GaussianNoise","text":"<pre><code>GaussianNoise(\n    stds: Tensor, mus: Tensor, application_interval: int = 1\n)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>AI4SurrogateModelling/src/model/models/layers/gaussian_noise.py</code> <pre><code>def __init__(\n    self,\n    stds: torch.Tensor,\n    mus: torch.Tensor,\n    application_interval: int = 1,\n):\n    super(GaussianNoise).__init__()\n    assert application_interval &gt; 0\n\n    self.stds = stds.unsqueeze(0)\n    self.mus = mus.unsqueeze(0)\n    self.application_interval = application_interval\n    self.n = 0\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise.application_interval","title":"application_interval  <code>instance-attribute</code>","text":"<pre><code>application_interval = application_interval\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise.mus","title":"mus  <code>instance-attribute</code>","text":"<pre><code>mus = unsqueeze(0)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n = 0\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise.stds","title":"stds  <code>instance-attribute</code>","text":"<pre><code>stds = unsqueeze(0)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/gaussian_noise/#AI4SurrogateModelling.src.model.models.layers.gaussian_noise.GaussianNoise.forward","title":"forward","text":"<pre><code>forward(x: Tensor)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/layers/gaussian_noise.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n):\n\n    if self.training and self.n == 0:\n        y = (\n            x\n            + torch.randn(x.shape, dtype=x.dtype, device=self.device)\n            * self.stds\n            + self.mus\n        )\n    else:\n        y = x\n\n    self.n = (self.n + 1) % self.application_interval\n    return y\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/","title":"Module <code>src.model.models.layers.residual</code>","text":""},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual","title":"AI4SurrogateModelling.src.model.models.layers.residual","text":"<p>Residual layer wrappers that inject skip connections around submodules.</p>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer","title":"ResidualLayer","text":"<pre><code>ResidualLayer(\n    *,\n    name: str = \"Residual Layer\",\n    model: ModelBase,\n    dtype: dtype = float32,\n    object_uid: str\n)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> <p>Wraps a submodule with a residual skip connection.</p> <p>Initialize the residual wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Identifier passed to the base <code>ModelBase</code>.</p> <code>'Residual Layer'</code> <code>model</code> <code>ModelBase</code> <p>Submodule providing <code>input_key</code>, <code>output_key</code>, and shape accessors for residual alignment.</p> required Source code in <code>AI4SurrogateModelling/src/model/models/layers/residual.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str = \"Residual Layer\",\n    model: ModelBase,\n    dtype: torch.dtype = torch.float32,\n    object_uid: str,\n):\n    \"\"\"Initialize the residual wrapper.\n\n    Args:\n        name: Identifier passed to the base `ModelBase`.\n        model: Submodule providing `input_key`, `output_key`, and\n            shape accessors for residual alignment.\n    \"\"\"\n    super().__init__(\n        name=name,\n        model=model,\n        dtype=dtype,\n        object_uid=object_uid,\n    )\n    self.model = unparse_model(model)\n    self.input_keys = model.input_keys\n    self.output_keys = model.output_keys\n    self.dim_in = self.model.dim_in\n    self.dim_out = self.model.dim_out\n\n    assert len(self.output_keys) == 1\n\n    if model.get_shape_input() != model.get_shape_output():\n        self.residual_transform = nn.Linear(\n            in_features=model.get_shape_input(),\n            out_features=model.get_shape_output(),\n            bias=False,\n        ).to(dtype=dtype)\n        nn.init.orthogonal_(self.residual_transform.weight)\n    else:\n        self.residual_transform = nn.Identity()\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.dim_in","title":"dim_in  <code>instance-attribute</code>","text":"<pre><code>dim_in = dim_in\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.dim_out","title":"dim_out  <code>instance-attribute</code>","text":"<pre><code>dim_out = dim_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.input_keys","title":"input_keys  <code>instance-attribute</code>","text":"<pre><code>input_keys = input_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = unparse_model(model)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.output_keys","title":"output_keys  <code>instance-attribute</code>","text":"<pre><code>output_keys = output_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.residual_transform","title":"residual_transform  <code>instance-attribute</code>","text":"<pre><code>residual_transform = to(dtype=dtype)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/layers/residual/#AI4SurrogateModelling.src.model.models.layers.residual.ResidualLayer.forward","title":"forward","text":"<pre><code>forward(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Apply the wrapped module and add the residual projection.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>dict</code> <p>Mapping containing the tensor keyed by <code>input_key</code>.</p> required <code>data_out</code> <code>dict</code> <p>Optional output dictionary to populate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the residual output at <code>output_key</code>.</p> Source code in <code>AI4SurrogateModelling/src/model/models/layers/residual.py</code> <pre><code>def forward(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Apply the wrapped module and add the residual projection.\n\n    Args:\n        data_in: Mapping containing the tensor keyed by `input_key`.\n        data_out: Optional output dictionary to populate.\n\n    Returns:\n        dict: Dictionary containing the residual output at `output_key`.\n    \"\"\"\n    if data_out is None:\n        data_out = {}\n\n    x = torch.cat([data_in[key] for key in self.input_keys], dim=-1)\n\n    res = self.model(data_in=data_in)[self.output_keys[0]]\n\n    data_out[self.output_keys[0]] = self.residual_transform(x) + res\n\n    return data_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/","title":"Module <code>src.model.models.other.mlp</code>","text":""},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp","title":"AI4SurrogateModelling.src.model.models.other.mlp","text":"<p>Contains modules and functions related to simple Multi-Layer Perceptron models.</p>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron","title":"MultiLayerPerceptron","text":"<pre><code>MultiLayerPerceptron(\n    *,\n    dtype: dtype,\n    layer_vertices: list[int],\n    layer_activations: list[str],\n    layer_biases: list[bool],\n    name: str,\n    input_keys: list[str],\n    output_keys: list[str],\n    object_uid=str\n)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> <p>The simplest architecture representing a Multi-Layer Perceptron (MLP). MLP transforms an input vector to an output vector via a sequence of non-linear transformations.</p> <p>Constructs the MLP model.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>dtype</code> <p>datatype of the parameters</p> required <code>layer_vertices</code> <code>list[int]</code> <p>Specifies the number of neurons                in each layer. The first entry has the dimension of the input                vectors, the last entry has the dimension of the output vectors.</p> required <code>layer_activations</code> <code>list[str]</code> <p>Each layer can perform a point-wise                function on the data. This list specifies these functions for                each layer.</p> required <code>layer_biases</code> <code>list[bool]</code> <p>Holds True/False flags for each layer.                When true, the data in that layer will be shifted before                activation function application and forwarding the result to                further layers.</p> required <code>name</code> <code>str</code> <p>Internal name of the model for logging and reporting.</p> required <code>input_keys</code> <code>list[str]</code> <p>A list of data keys on which this model                should be applied.</p> required <code>output_keys</code> <code>list[str]</code> <p>Data keys which this model should output.</p> required Source code in <code>AI4SurrogateModelling/src/model/models/other/mlp.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dtype: torch.dtype,\n    layer_vertices: list[int],\n    layer_activations: list[str],\n    layer_biases: list[bool],\n    name: str,\n    input_keys: list[str],\n    output_keys: list[str],\n    object_uid=str,\n):\n    \"\"\"Constructs the MLP model.\n\n    Args:\n        dtype (torch.dtype): datatype of the parameters\n        layer_vertices (list[int]): Specifies the number of neurons\\\n            in each layer. The first entry has the dimension of the input\\\n            vectors, the last entry has the dimension of the output vectors.\n        layer_activations (list[str]): Each layer can perform a point-wise\\\n            function on the data. This list specifies these functions for\\\n            each layer.\n        layer_biases (list[bool]): Holds True/False flags for each layer.\\\n            When true, the data in that layer will be shifted before\\\n            activation function application and forwarding the result to\\\n            further layers.\n        name (str): Internal name of the model for logging and reporting.\n        input_keys (list[str]): A list of data keys on which this model\\\n            should be applied.\n        output_keys (list[str]): Data keys which this model should output.\n    \"\"\"\n    super().__init__(\n        name=name,\n        dtype=dtype,\n        layer_vertices=layer_vertices,\n        layer_activations=layer_activations,\n        layer_biases=layer_biases,\n        input_keys=input_keys,\n        output_keys=output_keys,\n        object_uid=object_uid,\n    )\n    layer_list = []\n\n    n_layers = len(layer_vertices)\n    n_activations = len(layer_activations)\n    n_biases = len(layer_biases)\n    assert n_layers &gt;= 2\n    assert n_layers == n_activations and n_layers == n_biases\n\n    self.dim_in = layer_vertices[0]\n    self.dim_out = layer_vertices[-1]\n    self.input_keys = input_keys\n    self.output_keys = output_keys\n    assert len(self.output_keys) == 1\n\n    if layer_biases[0]:\n        layer_list.append(\n            BiasLayer(n=self.dim_in, dtype=dtype, name=\"MLP (input bias)\")\n        )\n\n    for activation in layer_activations[0]:\n        if activation is not None:\n            layer_list.append(self.parse_activation(activation))\n\n    for i in range(1, len(layer_vertices)):\n        u = layer_vertices[i - 1]\n        v = layer_vertices[i]\n        a = layer_activations[i]\n\n        layer_list.append(\n            nn.Linear(\n                in_features=u,\n                out_features=v,\n                bias=layer_biases[i],\n                dtype=dtype,\n            )\n        )\n        for activation in a:\n            if activation is not None:\n                layer_list.append(self.parse_activation(activation))\n\n    # self.add_module('model', nn.Sequential(*layer_list))\n    self.model = nn.Sequential(*layer_list)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.dim_in","title":"dim_in  <code>instance-attribute</code>","text":"<pre><code>dim_in = layer_vertices[0]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.dim_out","title":"dim_out  <code>instance-attribute</code>","text":"<pre><code>dim_out = layer_vertices[-1]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.input_keys","title":"input_keys  <code>instance-attribute</code>","text":"<pre><code>input_keys = input_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Sequential(*layer_list)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.output_keys","title":"output_keys  <code>instance-attribute</code>","text":"<pre><code>output_keys = output_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.forward","title":"forward","text":"<pre><code>forward(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Takes the data and applies this model to the value with the internal input key. Saves the result into a dictionary with the internal output key.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains a single key-value.</p> Source code in <code>AI4SurrogateModelling/src/model/models/other/mlp.py</code> <pre><code>def forward(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Takes the data and applies this model to the value with the internal\n    input key. Saves the result into a dictionary with the internal output\n    key.\n\n    Returns:\n        dict: Contains a single key-value.\n    \"\"\"\n    if data_out == None:\n        data_out = {}\n\n    x = torch.cat([data_in[key] for key in self.input_keys], dim=-1)\n\n    result = self.model(x)\n\n\n\n    data_out[self.output_keys[0]] = result\n\n    return data_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.get_shape_input","title":"get_shape_input","text":"<pre><code>get_shape_input() -&gt; int\n</code></pre> <p>Returns the input dimension of this model.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Input dimension of this model.</p> Source code in <code>AI4SurrogateModelling/src/model/models/other/mlp.py</code> <pre><code>def get_shape_input(\n    self,\n) -&gt; int:\n    \"\"\"Returns the input dimension of this model.\n\n    Returns:\n        int: Input dimension of this model.\n    \"\"\"\n    return self.dim_in\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/mlp/#AI4SurrogateModelling.src.model.models.other.mlp.MultiLayerPerceptron.get_shape_output","title":"get_shape_output","text":"<pre><code>get_shape_output() -&gt; int\n</code></pre> <p>Returns the output dimension of this model.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Output dimension of this model.</p> Source code in <code>AI4SurrogateModelling/src/model/models/other/mlp.py</code> <pre><code>def get_shape_output(\n    self,\n) -&gt; int:\n    \"\"\"Returns the output dimension of this model.\n\n    Returns:\n        int: Output dimension of this model.\n    \"\"\"\n    return self.dim_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/","title":"Module <code>src.model.models.other.sequential</code>","text":""},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential","title":"AI4SurrogateModelling.src.model.models.other.sequential","text":""},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential","title":"Sequential","text":"<pre><code>Sequential(\n    *, layers: list[ModelBase], name: str, object_uid: str\n)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> <p>Constructs the Sequential model.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>list[ModelBase]</code> <p>List of layers to be applied sequentially.</p> required <code>name</code> <code>str</code> <p>Internal name of the model for logging and reporting.</p> required Source code in <code>AI4SurrogateModelling/src/model/models/other/sequential.py</code> <pre><code>def __init__(\n    self,\n    *,\n    layers: list[\"ModelBase\"],\n    name: str,\n    object_uid: str,\n):\n    \"\"\"Constructs the Sequential model.\n\n    Args:\n        layers (list[ModelBase]): List of layers to be applied sequentially.\n        name (str): Internal name of the model for logging and reporting.\n    \"\"\"\n    super().__init__(\n        name=name,\n        layers=layers,\n        object_uid=object_uid,\n    )\n    self.layers = nn.ModuleList([unparse_model(model) for model in layers])\n    self.input_keys = self.layers[0].input_keys\n    self.output_keys = self.layers[-1].output_keys\n    self.dim_in = self.layers[0].dim_in\n    self.dim_out = self.layers[-1].dim_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.dim_in","title":"dim_in  <code>instance-attribute</code>","text":"<pre><code>dim_in = dim_in\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.dim_out","title":"dim_out  <code>instance-attribute</code>","text":"<pre><code>dim_out = dim_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.input_keys","title":"input_keys  <code>instance-attribute</code>","text":"<pre><code>input_keys = input_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList(\n    [(unparse_model(model)) for model in layers]\n)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.output_keys","title":"output_keys  <code>instance-attribute</code>","text":"<pre><code>output_keys = output_keys\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/sequential/#AI4SurrogateModelling.src.model.models.other.sequential.Sequential.forward","title":"forward","text":"<pre><code>forward(*, data_in: dict, data_out: dict = None) -&gt; dict\n</code></pre> <p>Forward pass through the Sequential model.</p> <p>Parameters:</p> Name Type Description Default <code>data_in</code> <code>dict</code> <p>Input data dictionary.</p> required <code>data_out</code> <code>dict</code> <p>Output data dictionary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated output data dictionary.</p> Source code in <code>AI4SurrogateModelling/src/model/models/other/sequential.py</code> <pre><code>def forward(\n    self,\n    *,\n    data_in: dict,\n    data_out: dict = None,\n) -&gt; dict:\n    \"\"\"Forward pass through the Sequential model.\n\n    Args:\n        data_in (dict): Input data dictionary.\n        data_out (dict): Output data dictionary.\n\n    Returns:\n        dict: Updated output data dictionary.\n    \"\"\"\n\n    if data_out is None:\n        data_out = {}\n\n    for layer in self.layers:\n        data_in = layer(data_in=data_in)\n\n    data_out.update(data_in)\n    return data_out\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/","title":"Module <code>src.model.models.other.structured</code>","text":""},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured","title":"AI4SurrogateModelling.src.model.models.other.structured","text":"<p>summary</p>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet","title":"StructuredNet","text":"<pre><code>StructuredNet(\n    *,\n    layer_vertices: list[Vertex],\n    layer_edges: list[Edge],\n    name: str\n)\n</code></pre> <p>               Bases: <code>ModelBase</code></p> Source code in <code>AI4SurrogateModelling/src/model/models/other/structured.py</code> <pre><code>def __init__(\n    self,\n    *,\n    layer_vertices: list[Vertex],\n    layer_edges: list[Edge],\n    name: str,\n):\n    super().__init__(name=name)\n\n    self.nvertices = np.max([k for k, _ in layer_vertices.items()]) + 1\n    self.nedges = 0\n\n    self.vertex_labels = [None for _ in range(self.nvertices)]\n    self.vertex_dim = [0 for _ in range(self.nvertices)]\n    self.vertex_activations = [nn.Identity() for _ in range(self.nvertices)]\n\n    vertex_bias = [\n        nn.Parameter(torch.scalar_tensor(0), requires_grad=False)\n        for _ in range(self.nvertices)\n    ]\n    for k, V in layer_vertices.items():\n        self.vertex_labels[k] = V.name\n        self.vertex_dim[k] = V.dim\n        self.vertex_activations[k] = V.activation.value[\"function\"]\n        if V.bias:\n            vertex_bias[k] = nn.Parameter(torch.randn((1, V.dim)))\n    self.vertex_bias = nn.ParameterList(vertex_bias)\n\n    for idx, label in enumerate(self.vertex_labels):\n        if label is None:\n            Logger.warning(\n                f\"Supplied vertices contain vertex id with values &lt; {self.nvertices}, but vertex with id: {idx} is not being used in the graph. Consider reindexing your vertices.\"\n            )\n\n    self.vertex_neighbours_outward = [set() for _ in range(self.nvertices)]\n    self.vertex_neighbours_inward = [set() for _ in range(self.nvertices)]\n    self.outward_edge_index = {u: {} for u in range(self.nvertices)}\n    self.inward_edge_index = {u: {} for u in range(self.nvertices)}\n    edges = []\n    for E in layer_edges:\n        u = E.vertex_src\n        v = E.vertex_tgt\n\n        dim_u = self.vertex_dim[u]\n        dim_v = self.vertex_dim[v]\n\n        if dim_u * dim_v &lt;= 0:\n            Logger.warning(\n                f\"Edge between vertices V{u}[{self.vertex_labels[u]}] and V{v}[{self.vertex_labels[v]}] has NOT been added. Reason: dimension irrelevancy: V{u} has dimension {dim_u}, V{v} has dimension {dim_v}.\"\n            )\n            continue\n\n        if contains_cycle(\n            neighborhood=self.vertex_neighbours_outward, u=u, v=v\n        ):\n            Logger.warning(\n                f\"Edge between vertices V{u}[{self.vertex_labels[u]}] and V{v}[{self.vertex_labels[v]}] has NOT been added. Reason: resulting graph would contain a cycle.\"\n            )\n            continue\n\n        if v in self.vertex_neighbours_outward[u]:\n            Logger.warning(\n                f\"Edge between vertices V{u}[{self.vertex_labels[u]}] and V{v}[{self.vertex_labels[v]}] has NOT been added. Reason: an edge already exists.\"\n            )\n            continue\n\n        if E.layer == TransferLayer.LINEAR:\n            edges.append(nn.Linear(dim_u, dim_v, False))\n        else:\n            mpi.abort(\n                0,\n                f\"Unknown Transfer Layer: {E.layer} between vertices V{u}[{self.vertex_labels[u]}] and V{v}[{self.vertex_labels[v]}]\",\n            )\n\n        self.outward_edge_index[u][v] = self.nedges\n        self.inward_edge_index[v][u] = self.nedges\n        self.vertex_neighbours_outward[u].add(v)\n        self.vertex_neighbours_inward[v].add(u)\n        self.nedges += 1\n\n    self.edges = nn.ParameterList(edges)\n\n    self.input_vertex = -1\n    self.output_vertex = -1\n\n    for u, Nu in enumerate(self.vertex_neighbours_inward):\n        if len(Nu) == 0:\n            if self.input_vertex &gt;= 0:\n                Logger.warning(\n                    f\"Attempting to add additional entry point: {[(u, self.vertex_labels[u])]}\"\n                )\n            else:\n                self.input_vertex = u\n\n    for u, Nu in enumerate(self.vertex_neighbours_outward):\n        if len(Nu) == 0:\n            if self.output_vertex &gt;= 0:\n                Logger.warning(\n                    f\"Attempting to add additional exit point: {[(u, self.vertex_labels[u])]}\"\n                )\n            else:\n                self.output_vertex = u\n\n    if self.input_vertex &lt; 0:\n        Logger.warning(f\"Graph does not contain an entry point.\")\n\n    if self.output_vertex &lt; 0:\n        Logger.warning(f\"Graph does not contain an exit point.\")\n\n    self.vertex_processing_groups = get_outward_vertex_groups(\n        edges_outward=self.vertex_neighbours_outward,\n        edges_inward=self.vertex_neighbours_inward,\n        source_vertex=self.input_vertex,\n        target_vertex=self.output_vertex,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.edges","title":"edges  <code>instance-attribute</code>","text":"<pre><code>edges = ParameterList(edges)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.input_vertex","title":"input_vertex  <code>instance-attribute</code>","text":"<pre><code>input_vertex = -1\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.inward_edge_index","title":"inward_edge_index  <code>instance-attribute</code>","text":"<pre><code>inward_edge_index = {u: {} for u in (range(nvertices))}\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.nedges","title":"nedges  <code>instance-attribute</code>","text":"<pre><code>nedges = 0\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.nvertices","title":"nvertices  <code>instance-attribute</code>","text":"<pre><code>nvertices = max([k for k, _ in (items())]) + 1\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.output_vertex","title":"output_vertex  <code>instance-attribute</code>","text":"<pre><code>output_vertex = -1\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.outward_edge_index","title":"outward_edge_index  <code>instance-attribute</code>","text":"<pre><code>outward_edge_index = {u: {} for u in (range(nvertices))}\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_activations","title":"vertex_activations  <code>instance-attribute</code>","text":"<pre><code>vertex_activations = [\n    (Identity()) for _ in (range(nvertices))\n]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_bias","title":"vertex_bias  <code>instance-attribute</code>","text":"<pre><code>vertex_bias = ParameterList(vertex_bias)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_dim","title":"vertex_dim  <code>instance-attribute</code>","text":"<pre><code>vertex_dim = [0 for _ in (range(nvertices))]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_labels","title":"vertex_labels  <code>instance-attribute</code>","text":"<pre><code>vertex_labels = [None for _ in (range(nvertices))]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_neighbours_inward","title":"vertex_neighbours_inward  <code>instance-attribute</code>","text":"<pre><code>vertex_neighbours_inward = [\n    (set()) for _ in (range(nvertices))\n]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_neighbours_outward","title":"vertex_neighbours_outward  <code>instance-attribute</code>","text":"<pre><code>vertex_neighbours_outward = [\n    (set()) for _ in (range(nvertices))\n]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.vertex_processing_groups","title":"vertex_processing_groups  <code>instance-attribute</code>","text":"<pre><code>vertex_processing_groups = get_outward_vertex_groups(\n    edges_outward=vertex_neighbours_outward,\n    edges_inward=vertex_neighbours_inward,\n    source_vertex=input_vertex,\n    target_vertex=output_vertex,\n)\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.forward","title":"forward","text":"<pre><code>forward(x: Tensor)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/other/structured.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n):\n    if not isinstance(x, torch.Tensor):\n        mpi.abort(0, f\"Expected a tensor as an input, recieved: {type(x)}\")\n\n    buffer = [self.vertex_bias[u] for u in range(self.nvertices)]\n    buffer[self.input_vertex] = x + buffer[self.input_vertex]\n\n    for vertex_group in self.vertex_processing_groups:\n\n        for u in vertex_group:\n            buffer[u] = self.vertex_activations[u](buffer[u])\n\n            for Nu in self.vertex_neighbours_outward[u]:\n                buffer[Nu] = self.edges[self.outward_edge_index[u, Nu]](\n                    buffer[u]\n                )\n\n    return buffer[self.output_vertex]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.get_shape_input","title":"get_shape_input","text":"<pre><code>get_shape_input()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/other/structured.py</code> <pre><code>def get_shape_input(\n    self,\n):\n    return self.vertex_dim[self.input_vertex]\n</code></pre>"},{"location":"api/API%20Reference/src/model/models/other/structured/#AI4SurrogateModelling.src.model.models.other.structured.StructuredNet.get_shape_output","title":"get_shape_output","text":"<pre><code>get_shape_output()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/model/models/other/structured.py</code> <pre><code>def get_shape_output(\n    self,\n):\n    return self.vertex_dim[self.output_vertex]\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/aux/","title":"Module <code>src.runtime_state.aux</code>","text":""},{"location":"api/API%20Reference/src/runtime_state/aux/#AI4SurrogateModelling.src.runtime_state.aux","title":"AI4SurrogateModelling.src.runtime_state.aux","text":"<p>A module containing auxilliary functions for the runtime state class.</p>"},{"location":"api/API%20Reference/src/runtime_state/aux/#AI4SurrogateModelling.src.runtime_state.aux.dtype_map","title":"dtype_map  <code>module-attribute</code>","text":"<pre><code>dtype_map = {\n    \"float16\": float16,\n    \"float32\": float32,\n    \"float64\": float64,\n    \"int32\": int32,\n    \"int64\": int64,\n    \"bool\": bool,\n}\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/aux/#AI4SurrogateModelling.src.runtime_state.aux.is_object_lazy","title":"is_object_lazy","text":"<pre><code>is_object_lazy(obj: Any) -&gt; bool\n</code></pre> <p>If the object has been decorated with the mark_lazy_function decorator, returns True.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if lazy, False otherwise.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/aux.py</code> <pre><code>def is_object_lazy(obj: Any) -&gt; bool:\n    \"\"\"If the object has been decorated with the mark_lazy_function decorator,\n    returns True.\n\n    Args:\n        obj (Any): Object to be checked.\n\n    Returns:\n        bool: True if lazy, False otherwise.\n    \"\"\"\n    if getattr(obj, \"__config_is_lazy__\", False):\n        return True\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/aux/#AI4SurrogateModelling.src.runtime_state.aux.mark_lazy_function","title":"mark_lazy_function","text":"<pre><code>mark_lazy_function(obj: Any) -&gt; Any\n</code></pre> <p>Adds an attribute to the provided object, for future checking in runtime state.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object for which to add the attribute</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The wrapped function</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/aux.py</code> <pre><code>def mark_lazy_function(obj: Any) -&gt; Any:\n    \"\"\"Adds an attribute to the provided object, for future checking in runtime\n    state.\n\n    Args:\n        obj (Any): Object for which to add the attribute\n\n    Returns:\n        Any: The wrapped function\n    \"\"\"\n    obj.__config_is_lazy__ = True\n    return obj\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/aux/#AI4SurrogateModelling.src.runtime_state.aux.parse_parameter","title":"parse_parameter","text":"<pre><code>parse_parameter(value: Any) -&gt; Any\n</code></pre> <p>Takes the input and if it is a string representing a tuple or a list, returns a tuple, or list containing integer, float or string values. Otherwise returns the input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>An object to be parsed.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A list, tuple, string or the input.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/aux.py</code> <pre><code>def parse_parameter(\n    value: Any,\n) -&gt; Any:\n    \"\"\"Takes the input and if it is a string representing a tuple or a list,\n    returns a tuple, or list containing integer, float or string values.\n    Otherwise returns the input.\n\n    Args:\n        value (Any): An object to be parsed.\n\n    Returns:\n        Any: A list, tuple, string or the input.\n    \"\"\"\n    # return value\n    if not isinstance(value, str):\n        return value\n    if value in dtype_map:\n        return dtype_map[value]\n    if value.lower().strip() == \"none\":\n        return None\n\n    def convert(x):\n        if x == \"True\":\n            return True\n\n        if x == \"False\":\n            return False\n\n        # try int\n        try:\n            return int(x)\n        except ValueError:\n            pass\n        # try float\n        try:\n            return float(x)\n        except ValueError:\n            pass\n        # fallback string\n        return x\n\n    value_ = value.strip()\n    if not (value_.startswith(\"(\") or value_.startswith(\"[\")):\n        return convert(value)  # not a sequence, return as-is\n\n    # remove brackets\n    inner = value_[1:-1].strip()\n    if not inner:\n        return [] if value_.startswith(\"[\") else ()\n\n    parts = [x.strip() for x in inner.split(\",\")]\n\n    parsed = [convert(p) for p in parts]\n    return parsed if value_.startswith(\"[\") else tuple(parsed)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/parameters/","title":"Module <code>src.runtime_state.parameters</code>","text":""},{"location":"api/API%20Reference/src/runtime_state/parameters/#AI4SurrogateModelling.src.runtime_state.parameters","title":"AI4SurrogateModelling.src.runtime_state.parameters","text":"<p>A module containing classes and functions related to parametrization and sampling from the parameter space.</p>"},{"location":"api/API%20Reference/src/runtime_state/parameters/#AI4SurrogateModelling.src.runtime_state.parameters.Parametrization","title":"Parametrization","text":"<pre><code>Parametrization(parameter_key: str, parameter_code: str)\n</code></pre> <p>A simple wrapper class around parameters passed to the runtime through  the configuration files. Its purpose is to decode the parameter key and  construct auxilliary structure for efficient random sampling within the  supplied space. The parameters can be either:</p> <pre><code>- a range or a union of ranges, defined with the keyword        PARAMETER_RANGE and followed by the list of intervals, i.e.\n    \"PARAMETER_RANGE: [(a0, b0), (a1, b1), (a2, b2)]\"\n    The constructor post-processes the ranges and joins intervals which            are overlapping or touching.\n\n- a set of choices, defined with the keyword PARAMETER_CHOICE and        followed by a list of values, i.e.\n    \"PARAMETER_CHOICE: [a, b, c]\"\n</code></pre> <p>Post processes the supplied strings and constructs all auxilliary structures.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_key</code> <code>str</code> <p>Key of the parameter to be fererenced down                the pipeline.</p> required <code>parameter_code</code> <code>str</code> <p>A string representation of the parameter                value.</p> required Source code in <code>AI4SurrogateModelling/src/runtime_state/parameters.py</code> <pre><code>def __init__(\n    self,\n    parameter_key: str,\n    parameter_code: str,\n):\n    \"\"\"Post processes the supplied strings and constructs all auxilliary\n    structures.\n\n    Args:\n        parameter_key (str): Key of the parameter to be fererenced down\\\n            the pipeline.\n        parameter_code (str): A string representation of the parameter\\\n            value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/parameters/#AI4SurrogateModelling.src.runtime_state.parameters.Parametrization.sample","title":"sample","text":"<pre><code>sample(value: Any = None) -&gt; Any\n</code></pre> <p>Performs random sampling. If the user supplies the value, raises an error if the value is not correct.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Value to be checked. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The sampled value.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/parameters.py</code> <pre><code>def sample(self, value: Any = None) -&gt; Any:\n    \"\"\"Performs random sampling. If the user supplies the value, raises an\n    error if the value is not correct.\n\n    Args:\n        value (Any): Value to be checked. Defaults to None.\n\n    Returns:\n        Any: The sampled value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/","title":"Module <code>src.runtime_state.runtime_state</code>","text":""},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state","title":"AI4SurrogateModelling.src.runtime_state.runtime_state","text":"<p>Contains classes and functions related to the state of the runtime. Serves as a bridge between the provided configurations and the instantiated objects/actions to be taken.</p>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action","title":"Action","text":"<pre><code>Action(\n    *,\n    object_uid: str = None,\n    f: Any,\n    parameters: dict[str, Any] = {},\n    action_type: str,\n    lazy: bool\n)\n</code></pre> <p>Serves as a wrapper around an action to be taken. An action can be an object initialization or a function call.</p> <p>Depending on which inputs are None, determines what action should be  performed.</p> <ul> <li>Object initialization and a member function call?</li> <li>A function call?</li> </ul> <p>The supplied parameters are used to override the default parameters of the object construction/function call.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>An object with the function                f as an attribute. Defaults to None.</p> <code>None</code> <code>f</code> <code>Any</code> <p>The function to be called, if object_uid refers to a                configured object, calls the function as the member of the                object class.</p> required <code>parameters</code> <code>dict[str, Any]</code> <p>Parameters to be passed into                the function call, this overrides the default parameters                supplied elsewhere in the configuration hierarchy. Defaults to                {}.</p> <code>{}</code> <code>action_type</code> <code>str</code> <p>the type of action to be defined, possible                values: - 'invalid' - 'class.static_function(parameters)' - 'global_function(parameters)' - 'member_function(object, **parameters)'</p> required <code>lazy</code> <code>bool</code> <p>A flag determining whether the associated objects                should be initialized automatically before (False) the function                call or explicitly (True) by the called function.</p> required Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def __init__(\n    self,\n    *,\n    object_uid: str = None,\n    f: Any,\n    parameters: dict[str, Any] = {},\n    action_type: str,\n    lazy: bool,\n):\n    \"\"\"Depending on which inputs are None, determines what action should be \n    performed.\n\n    - Object initialization and a member function call?\n    - A function call?\n\n    The supplied parameters are used to override the default parameters of\n    the object construction/function call.\n\n    Args:\n        object_uid (str, optional): An object with the function\\\n            f as an attribute. Defaults to None.\n        f (Any): The function to be called, if object_uid refers to a\\\n            configured object, calls the function as the member of the\\\n            object class.\n        parameters (dict[str, Any], optional): Parameters to be passed into\\\n            the function call, this overrides the default parameters\\\n            supplied elsewhere in the configuration hierarchy. Defaults to\\\n            {}.\n        action_type (str): the type of action to be defined, possible\\\n            values:\n            - 'invalid'\n            - 'class.static_function(**parameters)'\n            - 'global_function(**parameters)'\n            - 'member_function(object, **parameters)'\n        lazy (bool): A flag determining whether the associated objects\\\n            should be initialized automatically before (False) the function\\\n            call or explicitly (True) by the called function.\n    \"\"\"\n\n    if object_uid is None and f is None:\n        msg = (\n            f\"An Action cannot be empty\"\n            \"(requires either an object or a function to be called)\"\n        )\n        Logger.warning(msg)\n        raise ValueError(msg)\n\n    self.object_uid = object_uid\n    self.object_class = runtime_state.get_object_configuration(\n        object_uid=object_uid\n    )\n    self.f = f\n    self.parameters = parameters\n    self.action_type = action_type\n    self.lazy = lazy\n\n    # check for parameter correctness\n    sig = inspect.signature(self.f)\n    try:\n        if self.action_type == \"member_function(object, **parameters)\":\n            bound = sig.bind(self.object_class, **self.parameters)\n            bound.apply_defaults()  # fills in defaults for missing args\n        elif self.action_type == \"class.static_function(**parameters)\":\n            bound = sig.bind(**self.parameters)\n            bound.apply_defaults()  # fills in defaults for missing args\n        elif self.action_type == \"global_function(**parameters)\":\n            bound = sig.bind(**self.parameters)\n            bound.apply_defaults()  # fills in defaults for missing args\n        else:\n            msg = f\"Unknown action type: {self.action_type}\"\n            Logger.warning(msg)\n            raise AttributeError(msg)\n\n    except TypeError as e:\n        keys_in_SmP, keys_in_PmS = _filter_parameters_extra(\n            sig=sig, parameters=parameters\n        )\n        msg = (\n            f'Invalid \"{self.f}\" init parameters \"{list(self.parameters.keys())}\" -&gt;'\n            f'\"{e}\"\\nPossible candidates:{list(keys_in_SmP)}, '\n            f\"extra arguments: {list(keys_in_PmS)}\"\n        )\n        Logger.warning(msg)\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.action_type","title":"action_type  <code>instance-attribute</code>","text":"<pre><code>action_type = action_type\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.f","title":"f  <code>instance-attribute</code>","text":"<pre><code>f = f\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.lazy","title":"lazy  <code>instance-attribute</code>","text":"<pre><code>lazy = lazy\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.object_class","title":"object_class  <code>instance-attribute</code>","text":"<pre><code>object_class = get_object_configuration(\n    object_uid=object_uid\n)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.object_uid","title":"object_uid  <code>instance-attribute</code>","text":"<pre><code>object_uid = object_uid\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters = parameters\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of this object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing relevant information about this object in                a nicely formatted manner.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def __str__(\n    self,\n) -&gt; str:\n    \"\"\"Returns a string representation of this object.\n\n    Returns:\n        str: A string containing relevant information about this object in\\\n            a nicely formatted manner.\n    \"\"\"\n\n    if self.object_uid is not None:\n        if self.f is not None:\n            out = (\n                f\"Object: {self.object_uid}\\n\"\n                f\"Function: {self.f}\\n\"\n                f\"Parameters: {self.parameters}\"\n            )\n        else:\n            out = (\n                f\"Object: {self.object_uid}\\n\"\n                f\"Parameters: {self.parameters}\"\n            )\n    else:\n        if self.f is not None:\n            out = f\"Function: {self.f}\\n\" f\"Parameters: {self.parameters}\"\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.call","title":"call","text":"<pre><code>call(\n    *, parameters: dict[str, Any] = {}, reinit: bool\n) -&gt; Any\n</code></pre> <p>Calls the function. The supplied parameters override the member parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, Any]</code> <p>Explicit override of the function call            parameters to be used instead of the default ones.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>An instance of the class.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def call(\n    self,\n    *,\n    parameters: dict[str, Any] = {},\n    reinit: bool,\n) -&gt; Any:\n    \"\"\"Calls the function. The supplied parameters override the member\n    parameters.\n\n    Args:\n        parameters (dict[str, Any]): Explicit override of the function call\\\n        parameters to be used instead of the default ones.\n\n    Returns:\n        Any: An instance of the class.\n    \"\"\"\n    active_parameters = _sample_parameters(\n        default_parameters=self.parameters,\n        override_parameters=parameters,\n        lazy=self.lazy,\n        reinit=reinit,\n    )\n    return self._call(parameters=active_parameters, reinit=reinit)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.Action.is_parametrized","title":"is_parametrized","text":"<pre><code>is_parametrized(parameters: dict[str, Any] = {}) -&gt; bool\n</code></pre> <p>Determines if at least one member parameter is to be sampled. If the  user provides the override parameters, then the method considers the  overriden parametrization.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, Any]</code> <p>Goes through the member parameters and                if any key is also contained the the supplied dictionary, uses                the supplied value. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if at least one parameter is to be sampled, False                otherwise.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def is_parametrized(\n    self,\n    parameters: dict[str, Any] = {},\n) -&gt; bool:\n    \"\"\"Determines if at least one member parameter is to be sampled. If the \n    user provides the override parameters, then the method considers the \n    overriden parametrization.\n\n    Args:\n        parameters (dict[str, Any]): Goes through the member parameters and\\\n            if any key is also contained the the supplied dictionary, uses\\\n            the supplied value. Defaults to {}.\n\n    Returns:\n        bool: True if at least one parameter is to be sampled, False\\\n            otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization","title":"ObjectParametrization","text":"<pre><code>ObjectParametrization(\n    *, obj: Any, parameters: dict[str, Any], object_uid: str\n)\n</code></pre> <p>Serves as a wrapper around a class and its parametrization. Handles the object initialization and parameter sampling (if the parameters are to be sampled).</p> <p>Stores the object and its parametrization for future use. Processes the parameters for efficient future use. Args:     obj (Any): A class representing the object to be initialized.     parameters (dict[str, Any]): Object parametrization.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def __init__(\n    self,\n    *,\n    obj: Any,\n    parameters: dict[str, Any],\n    object_uid: str,\n):\n    \"\"\"\n    Stores the object and its parametrization for future use. Processes the\n    parameters for efficient future use.\n    Args:\n        obj (Any): A class representing the object to be initialized.\n        parameters (dict[str, Any]): Object parametrization.\n    \"\"\"\n    obj = inspect.unwrap(obj)\n    self.obj = obj\n    self.parameters = parameters\n    self.object_uid = object_uid\n\n    # check parameter validity\n    sig = inspect.signature(obj.__init__)\n    try:\n        # remove 'self' parameter from the signature binding\n        bound = sig.bind_partial(None, **parameters)\n        bound.apply_defaults()\n\n        # drop 'self' from reported arguments\n        args = dict(bound.arguments)\n        args.pop(\"self\", None)\n\n        # print(object_uid, obj, sig.parameters)\n        if \"object_uid\" in sig.parameters:\n            # print(object_uid)\n            self.parameters[\"object_uid\"] = object_uid\n        # else:\n        #     print(f'{self.object_uid} -&gt; NO')\n\n    except TypeError as e:\n        keys_in_SmP, keys_in_PmS = _filter_parameters_extra(\n            sig=sig, parameters=parameters\n        )\n        msg = (\n            f'Invalid \"{self.obj}\" init parameters \"{list(self.parameters.keys())}\" -&gt;'\n            f'\"{e}\"\\nPossible candidates:{list(keys_in_SmP)}, '\n            f\"extra arguments: {list(keys_in_PmS)}\"\n        )\n        Logger.warning(msg)\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.obj","title":"obj  <code>instance-attribute</code>","text":"<pre><code>obj = obj\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.object_uid","title":"object_uid  <code>instance-attribute</code>","text":"<pre><code>object_uid = object_uid\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.parameters","title":"parameters  <code>instance-attribute</code>","text":"<pre><code>parameters = parameters\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of this object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing relevant information about this object in                a nicely formatted manner.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def __str__(\n    self,\n) -&gt; str:\n    \"\"\"Returns a string representation of this object.\n\n    Returns:\n        str: A string containing relevant information about this object in\\\n            a nicely formatted manner.\n    \"\"\"\n    out = f\"Object: {self.obj}\\n\" f\"Parameters: {self.parameters}\"\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.get_object_class","title":"get_object_class","text":"<pre><code>get_object_class() -&gt; Any\n</code></pre> <p>Retrieves the class to be initialized.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>This object's class.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def get_object_class(\n    self,\n) -&gt; Any:\n    \"\"\"Retrieves the class to be initialized.\n\n    Returns:\n        Any: This object's class.\n    \"\"\"\n    return self.obj\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.get_object_parameters","title":"get_object_parameters","text":"<pre><code>get_object_parameters() -&gt; dict\n</code></pre> <p>Retrieves the parameters to be used in construction of this object.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parameters to be passed to the constructor.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def get_object_parameters(\n    self,\n) -&gt; dict:\n    \"\"\"Retrieves the parameters to be used in construction of this object.\n\n    Returns:\n        dict: Parameters to be passed to the constructor.\n    \"\"\"\n    return self.parameters\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.init","title":"init","text":"<pre><code>init(\n    *, parameters: dict[str, Any] = {}, reinit: bool\n) -&gt; Any\n</code></pre> <p>Initializes a new instance of the class with parameters sampled from a uniform random distribution (if any parameters are to be sampled).  The supplied parameters override the member parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, Any]</code> <p>Explicit override of the constructor                parameters to be used instead of the default ones.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>An instance of the class.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def init(\n    self,\n    *,\n    parameters: dict[str, Any] = {},\n    reinit: bool,\n) -&gt; Any:\n    \"\"\"Initializes a new instance of the class with parameters sampled from\n    a uniform random distribution (if any parameters are to be sampled). \n    The supplied parameters override the member parameters.\n\n    Args:\n        parameters (dict[str, Any]): Explicit override of the constructor\\\n            parameters to be used instead of the default ones.\n\n    Returns:\n        Any: An instance of the class.\n    \"\"\"\n    active_parameters = _sample_parameters(\n        default_parameters=self.parameters,\n        override_parameters=parameters,\n        reinit=reinit,\n    )\n    return self.obj(**active_parameters)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.ObjectParametrization.is_parametrized","title":"is_parametrized","text":"<pre><code>is_parametrized(parameters: dict[str, Any] = {}) -&gt; bool\n</code></pre> <p>Determines if at least one member parameter is to be sampled. If the  user provides the override parameters, then the method considers the  overriden parametrization.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict[str, Any]</code> <p>Goes through the member parameters and                if any key is also contained the the supplied dictionary, uses                the supplied value. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if at least one parameter is to be sampled, False                otherwise.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>def is_parametrized(\n    self,\n    parameters: dict[str, Any] = {},\n) -&gt; bool:\n    \"\"\"Determines if at least one member parameter is to be sampled. If the \n    user provides the override parameters, then the method considers the \n    overriden parametrization.\n\n    Args:\n        parameters (dict[str, Any]): Goes through the member parameters and\\\n            if any key is also contained the the supplied dictionary, uses\\\n            the supplied value. Defaults to {}.\n\n    Returns:\n        bool: True if at least one parameter is to be sampled, False\\\n            otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state","title":"runtime_state","text":"<p>Contains the following static objects which are referenced during the  program execution:</p> <pre><code>- object_configurations: holds the parameters used in constructors of        all objects.\n- objects: holds all the instantiated objects.\n- parametrized_object_uids: a list of unique identifiers of objects            requiring randomg parameter sampling\n- actions: for each object, holds a sequence of actions to be performed            with this object\n</code></pre> <p>Furthermore, contains the global configuration object related to this  runtime.</p>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.actions","title":"actions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>actions = []\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.object_configurations","title":"object_configurations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>object_configurations = {}\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.objects","title":"objects  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>objects = {}\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.operations","title":"operations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>operations = []\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.parametrized_object_uids","title":"parametrized_object_uids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parametrized_object_uids = []\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.add_action","title":"add_action  <code>staticmethod</code>","text":"<pre><code>add_action(\n    *, object_uid: str, fname: str, parameters: dict\n)\n</code></pre> <p>Adds an action associated with the specified object. The action is a class member function to be called with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>A unique identifier specifying the object.</p> required <code>fname</code> <code>str</code> <p>A name of the function to be called.</p> required <code>parameters</code> <code>dict</code> <p>Parameters to override the default function                parameters to the member function.</p> required Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef add_action(\n    *,\n    object_uid: str,\n    fname: str,\n    parameters: dict,\n):\n    \"\"\"Adds an action associated with the specified object. The action is a\n    class member function to be called with the specified parameters.\n\n    Args:\n        object_uid (str): A unique identifier specifying the object.\n        fname (str): A name of the function to be called.\n        parameters (dict): Parameters to override the default function\\\n            parameters to the member function.\n    \"\"\"\n\n    try:\n        obj = runtime_state.get_object_configuration(object_uid)\n        action_type = \"invalid\"\n        if obj is None:\n            try:\n                action_type = \"global_function(**parameters)\"\n                # test for a global function\n                module_path, function_name = fname.rsplit(\".\", 1)\n\n                module = import_module(module_path)\n                f = getattr(module, function_name)\n                is_lazy = is_object_lazy(f)\n\n                Logger.info(f\"Adding action:\")\n                Logger.info(f\"  lazy: {is_lazy}\")\n                Logger.info(f\"  '{function_name}' from: '{module_path}'\")\n                Logger.info(f\"  action parameters: {parameters}\")\n\n            except ModuleNotFoundError as e:\n                action_type = \"class.static_function(**parameters)\"\n                # split into module and a function\n                # module.Class.function\n                module_path, obj_name, function_name = fname.rsplit(\n                    \".\", 2\n                )  # split into module and a function\n                module = import_module(module_path)\n                obj = getattr(module, obj_name)\n                f = getattr(obj, function_name)\n                is_lazy = is_object_lazy(f)\n\n                Logger.info(f\"Adding action:\")\n                Logger.info(f\"  lazy: {is_lazy}\")\n                Logger.info(\n                    f\"  {function_name} from: \" f\"{module_path}.{obj_name}\"\n                )\n                Logger.info(f\"  action parameters: {parameters}\")\n\n        else:\n            action_type = \"member_function(object, **parameters)\"\n            obj = obj.get_object_class()\n            f = getattr(obj, fname)\n            is_lazy = is_object_lazy(f)\n\n            Logger.info(f\"Adding action:\")\n            Logger.info(f\"  lazy: {is_lazy}\")\n            Logger.info(f\"  object key: {object_uid}\")\n            Logger.info(f\"  object class: {obj}\")\n            Logger.info(f\"  action function: {fname}\")\n            Logger.info(f\"  action parameters: {parameters}\")\n\n        Logger.info(f'  Internal action type: \"{action_type}\"')\n\n        runtime_state.actions.append(\n            Action(\n                object_uid=object_uid,\n                f=f,\n                parameters=parameters,\n                action_type=action_type,\n                lazy=is_lazy,\n            )\n        )\n\n    except AttributeError as e:\n        Logger.warning(e)\n        raise AttributeError(e)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.add_object_configuration","title":"add_object_configuration  <code>staticmethod</code>","text":"<pre><code>add_object_configuration(\n    *, key: str, constructor: str, parameters: dict\n)\n</code></pre> <p>Given the object unique identifier, stores the class and parameters            required for initialization.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A unique identifier of the object.</p> required <code>constructor</code> <code>str</code> <p>Name of the class representing the object or a                function returing an instance of an object.</p> required <code>parameters</code> <code>dict</code> <p>Parameters for initialization.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object is already stored, raises this error.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef add_object_configuration(\n    *,\n    key: str,\n    constructor: str,\n    parameters: dict,\n):\n    \"\"\"Given the object unique identifier, stores the class and parameters\\\n        required for initialization.\n\n    Args:\n        key (str): A unique identifier of the object.\n        constructor (str): Name of the class representing the object or a\\\n            function returing an instance of an object.\n        parameters (dict): Parameters for initialization.\n\n    Raises:\n        ValueError: If the object is already stored, raises this error.\n    \"\"\"\n\n\n    if runtime_state.contains_uid(key):\n        msg = (\n            f'An object or a function with ID \"{key}\" is already'\n            \"configured. Skipping further configuration...\"\n        )\n        Logger.warning(msg)\n        return\n\n    f = None\n    try:\n        # test for class definition (package.class)\n        module_path, obj_name = constructor.rsplit(\".\", 1)\n\n        module = import_module(module_path)\n        f = getattr(module, function_name)\n\n        Logger.info(f\"Adding object definition:\")\n        Logger.info(f\"  object UID: '{key}'\")\n        Logger.info(f\"  object class: '{obj_name}' from '{module_path}'\")\n        Logger.info(f\"  object parameters: {parameters}\")\n    except Exception as e:\n        Logger.warning(f'\\nEncountered an exception during object definition when treating object as a class: {e}\\n  key: {key}\\n  constructor: {constructor}\\n  parameters: {parameters}\\n')\n        try:\n            # test for global function (package.function)\n            module_path, function_name = constructor.rsplit(\".\", 1)\n\n            module = import_module(module_path)\n            f = getattr(module, function_name)\n\n            Logger.info(f\"Adding object definition:\")\n            Logger.info(f\"  object UID: '{key}'\")\n            Logger.info(\n                f\"  object function: '{function_name}' from '{module_path}'\"\n            )\n            Logger.info(f\"  object parameters: {parameters}\")\n        except Exception as e:\n            Logger.warning(f'\\nEncountered an exception during object definition when treating object as a global function: {e}\\n  key: {key}\\n  constructor: {constructor}\\n  parameters: {parameters}\\n')\n            try:\n                # test for static function definition (package.class.static_function)\n                module_path, obj_name, f_name = constructor.rsplit(\".\", 2)\n\n                module = import_module(module_path)\n                obj = getattr(module, obj_name)\n                f = getattr(obj, f_name)\n\n                Logger.info(f\"Adding object definition:\")\n                Logger.info(f\"  object UID: '{key}'\")\n                Logger.info(\n                    f\"  object class: '{obj_name}' from '{module_path}'\"\n                )\n                Logger.info(\n                    f\"  function name: '{f_name}' from '{obj_name}'\"\n                )\n                Logger.info(f\"  object parameters: {parameters}\")\n            except Exception as e:\n                Logger.warning(f'\\nEncountered an exception during object definition when treating object as a static function: {e}\\n  key: {key}\\n  constructor: {constructor}\\n  parameters: {parameters}\\n')\n\n    if f is None:\n        msg = f\"Unable to initialize an object defined at: {constructor} with parameters: {parameters}\"\n        Logger.warning(msg)\n        raise RuntimeError(msg)\n\n    obj_parametrization = ObjectParametrization(\n        obj=f,\n        parameters=parameters,\n        object_uid=key,\n    )\n    runtime_state.object_configurations[key] = obj_parametrization\n\n    if obj_parametrization.is_parametrized():\n        runtime_state.parametrized_object_uids.append(key)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.add_operation","title":"add_operation  <code>staticmethod</code>","text":"<pre><code>add_operation(operation: dict)\n</code></pre> <p>Adds an operation dictionary to the runtime. </p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Contains two keys 'objects' and 'actions'.               - 'objects': a list of objects UIDs to be constructed in the                    operation. - 'actions': a list of integers, indices of actions to be taken.</p> required Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef add_operation(operation: dict):\n    \"\"\"Adds an operation dictionary to the runtime. \n\n    Args:\n        operation (dict): Contains two keys 'objects' and 'actions'.\\\n\n            - 'objects': a list of objects UIDs to be constructed in the\\\n                operation.\n            - 'actions': a list of integers, indices of actions to be taken.\n    \"\"\"\n\n    runtime_state.operations.append(operation)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.check_object_dependencies","title":"check_object_dependencies  <code>staticmethod</code>","text":"<pre><code>check_object_dependencies()\n</code></pre> <p>Analyzes the configured objects and actions and builds a hierarchy graph and ordering in which the objects should be initialized and actions called.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef check_object_dependencies():\n    \"\"\"Analyzes the configured objects and actions and builds a hierarchy\n    graph and ordering in which the objects should be initialized and\n    actions called.\n    \"\"\"\n\n    Logger.info(\"Checking object and action dependencies.\")\n\n    object_dependencies = {}\n    action_dependencies = {}\n    for operation in runtime_state.operations:\n        operation_object_UIDs = operation[\"objects\"]\n        operation_actions = operation[\"actions\"]\n\n        for action_index in operation_actions:\n            action = runtime_state._get_action(action_index)\n            # Logger.warning(f'{action_index}: {action}')\n\n            action_dependencies[action_index] = []\n            if action.object_uid is not None:\n                action_dependencies[action_index].append(action.object_uid)\n\n            for (\n                parent_object_uid\n            ) in runtime_state.get_objects_from_parameters(\n                action.parameters\n            ):\n                action_dependencies[action_index].append(parent_object_uid)\n            Logger.info(\n                f\"[{action_index}] Action Dependencies: {action_dependencies[action_index]}\"\n            )\n\n        for operation_object_UID in operation_object_UIDs:\n            object_dependencies[operation_object_UID] = []\n            obj = runtime_state.get_object_configuration(\n                operation_object_UID\n            )\n            for (\n                parent_object_uid\n            ) in runtime_state.get_objects_from_parameters(obj.parameters):\n                object_dependencies[operation_object_UID].append(\n                    parent_object_uid\n                )\n            Logger.info(\n                f\"[{operation_object_UID}] Object Dependencies: {object_dependencies[operation_object_UID]}\"\n            )\n\n    cyclic_dependencies = _get_cyclic_dependencies(object_dependencies)\n    if len(cyclic_dependencies) &gt; 0:\n        msg = (\n            \"The configuration files contain cyclic dependencies in\"\n            \"object construction...\"\n        )\n        Logger.warning(msg)\n        for obj_uid, dep in cyclic_dependencies.items():\n            Logger.warning(f\"Object: {obj_uid} -&gt; {dep}\")\n\n        raise RuntimeError(f\"{msg}:  {cyclic_dependencies}\")\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.clear_parametrized_objects","title":"clear_parametrized_objects  <code>staticmethod</code>","text":"<pre><code>clear_parametrized_objects()\n</code></pre> <p>Goes through the objects which were constructed with at least one sampled parameter and frees them from memory.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef clear_parametrized_objects():\n    \"\"\"Goes through the objects which were constructed with at least one\n    sampled parameter and frees them from memory.\n    \"\"\"\n\n    for key in runtime_state.parametrized_object_uids:\n        runtime_state.delete_object(key)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.construct_object","title":"construct_object  <code>staticmethod</code>","text":"<pre><code>construct_object(\n    *,\n    object_uid: str,\n    reinit: bool = False,\n    override_parameters: dict = {}\n) -&gt; Any\n</code></pre> <p>Takes the provided object_uid and constructs a new object referred to by that UID. If an object already exists, deletes it and uses the new object.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>A UID of the object to be created.</p> required <code>reinit</code> <code>bool</code> <p>If True, reinitializes the object even                though its already initialized. Defaults to True.</p> <code>False</code> <code>override_parameters</code> <code>dict</code> <p>a list of parameters which should be                added to/override the parameters provided by the configuration                files.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Returns the instantion of the object.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef construct_object(\n    *,\n    object_uid: str,\n    reinit: bool = False,\n    override_parameters: dict = {},\n) -&gt; Any:\n    \"\"\"Takes the provided object_uid and constructs a new object referred to\n    by that UID. If an object already exists, deletes it and uses the new\n    object.\n\n    Args:\n        object_uid (str): A UID of the object to be created.\n        reinit (bool, optional): If True, reinitializes the object even\\\n            though its already initialized. Defaults to True.\n        override_parameters (dict): a list of parameters which should be\\\n            added to/override the parameters provided by the configuration\\\n            files.\n\n    Returns:\n        Any: Returns the instantion of the object.\n    \"\"\"\n\n    if runtime_state.get_object(object_uid) is not None and not reinit:\n        return runtime_state.get_object(object_uid)\n\n    if runtime_state.get_object(object_uid) is not None and reinit:\n        Logger.warning(\n            f'An object with UID \"{object_uid}\" already exists. '\n            \"Creating a new instance.\"\n        )\n        runtime_state.delete_object(object_uid)\n\n    obj_config = runtime_state.get_object_configuration(object_uid)\n    Logger.info(f\"About to construct an object: {obj_config}\")\n\n    obj = obj_config.init(parameters=override_parameters, reinit=reinit)\n    runtime_state.objects[object_uid] = obj\n\n    return runtime_state.get_object(object_uid)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.contains_uid","title":"contains_uid  <code>staticmethod</code>","text":"<pre><code>contains_uid(key: str) -&gt; bool\n</code></pre> <p>Determines whether the unique ID is already being used.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>A string representing the unique identifier of                an object.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if present, otherwise False.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef contains_uid(\n    key: str,\n) -&gt; bool:\n    \"\"\"Determines whether the unique ID is already being used.\n\n    Args:\n        key (str): A string representing the unique identifier of\\\n            an object.\n\n    Returns:\n        bool: True if present, otherwise False.\n    \"\"\"\n    if key in runtime_state.object_configurations:\n        return True\n    # if key in runtime_state.function_configurations:\n    #     return True\n    return False\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.delete_object","title":"delete_object  <code>staticmethod</code>","text":"<pre><code>delete_object(object_uid: str) -&gt; None\n</code></pre> <p>Deletes the instance of the object with the specified UID.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>UID of the object to be deleted.</p> required Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef delete_object(object_uid: str) -&gt; None:\n    \"\"\"Deletes the instance of the object with the specified UID.\n\n    Args:\n        object_uid (str): UID of the object to be deleted.\n    \"\"\"\n    if object_uid in runtime_state.objects:\n        Logger.warning(\n            f\"CONF: {runtime_state.object_configurations[object_uid]}\"\n        )\n        obj = runtime_state.objects[object_uid]\n        del obj\n        runtime_state.objects[object_uid] = None\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.get_n_actions","title":"get_n_actions  <code>staticmethod</code>","text":"<pre><code>get_n_actions() -&gt; int\n</code></pre> <p>Returns the number of currently defined actions.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>An integer &gt;= 0.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef get_n_actions() -&gt; int:\n    \"\"\"Returns the number of currently defined actions.\n\n    Returns:\n        int: An integer &gt;= 0.\n    \"\"\"\n    return len(runtime_state.actions)\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.get_object","title":"get_object  <code>staticmethod</code>","text":"<pre><code>get_object(object_uid: str) -&gt; Any\n</code></pre> <p>Attempts to retrieve an initialized instance of the object with the provided unique identifier. If the object is not initialized, returns None.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>A unique identifier referencing the intialized                object.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Returns the object or None if it does not exist.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef get_object(\n    object_uid: str,\n) -&gt; Any:\n    \"\"\"Attempts to retrieve an initialized instance of the object with the\n    provided unique identifier. If the object is not initialized, returns\n    None.\n\n    Args:\n        object_uid (str): A unique identifier referencing the intialized\\\n            object.\n\n    Returns:\n        Any: Returns the object or None if it does not exist.\n    \"\"\"\n\n    if object_uid in runtime_state.objects:\n        return runtime_state.objects[object_uid]\n\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.get_object_configuration","title":"get_object_configuration  <code>staticmethod</code>","text":"<pre><code>get_object_configuration(\n    object_uid: str,\n) -&gt; ObjectParametrization\n</code></pre> <p>Retrieves the object class with the given unique identifier.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>A string representing the unique identifier of                the object to be retrieved.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the unique identifier does not exist.</p> <p>Returns:</p> Name Type Description <code>ObjectParametrization</code> <code>ObjectParametrization</code> <p>The class representing the queried object.</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef get_object_configuration(\n    object_uid: str,\n) -&gt; ObjectParametrization:\n    \"\"\"Retrieves the object class with the given unique identifier.\n\n    Args:\n        object_uid (str): A string representing the unique identifier of\\\n            the object to be retrieved.\n\n    Raises:\n        RuntimeError: If the unique identifier does not exist.\n\n    Returns:\n        ObjectParametrization: The class representing the queried object.\n    \"\"\"\n    if object_uid is None:\n        return None\n\n    if object_uid in runtime_state.object_configurations:\n        return runtime_state.object_configurations[object_uid]\n\n    msg = (\n        f\"get_object_configuration: Object with unique ID:\"\n        f'\"{object_uid}\"does not have a configuration.'\n    )\n    Logger.warning(msg)\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.get_object_dependencies","title":"get_object_dependencies  <code>staticmethod</code>","text":"<pre><code>get_object_dependencies(object_uid: str) -&gt; list\n</code></pre> <p>Retrieves the list of objects and actions to be taken before the supplied object can be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>object_uid</code> <code>str</code> <p>A unique identifier referencing the object to be                analyzed.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Contains object UIDs (str) and Action indices (int)</p> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef get_object_dependencies(\n    object_uid: str,\n) -&gt; list:\n    \"\"\"Retrieves the list of objects and actions to be taken before the\n    supplied object can be initialized.\n\n    Args:\n        object_uid (str): A unique identifier referencing the object to be\\\n            analyzed.\n\n    Returns:\n        list: Contains object UIDs (str) and Action indices (int)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.get_objects_from_parameters","title":"get_objects_from_parameters  <code>staticmethod</code>","text":"<pre><code>get_objects_from_parameters(parameters: Any) -&gt; list[str]\n</code></pre> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef get_objects_from_parameters(\n    parameters: Any,\n) -&gt; list[str]:\n    out = []\n\n    if isinstance(parameters, dict):\n        for k, v in parameters.items():\n            out += runtime_state.get_objects_from_parameters(v)\n\n        return out\n\n    if isinstance(parameters, list) or isinstance(parameters, tuple):\n        for v in parameters:\n            out += runtime_state.get_objects_from_parameters(v)\n\n        return out\n\n    if isinstance(parameters, str):\n        if runtime_state.contains_uid(parameters):\n            return [parameters]\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/runtime_state/runtime_state/#AI4SurrogateModelling.src.runtime_state.runtime_state.runtime_state.perform_action","title":"perform_action  <code>staticmethod</code>","text":"<pre><code>perform_action(\n    *, action_idx: int, parameters: dict = {}, reinit: bool\n)\n</code></pre> <p>Performs a chosen action specified by its index.</p> <p>Parameters:</p> Name Type Description Default <code>action_idx</code> <code>int</code> <p>Index of the action to be performed.</p> required <code>parameters</code> <code>dict</code> <p>Parameters to override the parameters                supplied in the action configuration. Defaults to {}.</p> <code>{}</code> Source code in <code>AI4SurrogateModelling/src/runtime_state/runtime_state.py</code> <pre><code>@staticmethod\ndef perform_action(\n    *,\n    action_idx: int,\n    parameters: dict = {},\n    reinit: bool,\n):\n    \"\"\"Performs a chosen action specified by its index.\n\n    Args:\n        action_idx (int): Index of the action to be performed.\n        parameters (dict, optional): Parameters to override the parameters\\\n            supplied in the action configuration. Defaults to {}.\n    \"\"\"\n    assert action_idx &gt;= 0 and action_idx &lt; len(runtime_state.actions)\n\n    action = runtime_state._get_action(action_idx)\n    Logger.info(\n        \"About to perform action \" f\"with index: {action_idx}\\n{action}\"\n    )\n    action.call(parameters=parameters, reinit=reinit)\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/goniometric/","title":"Module <code>src.sensitivity_analysis.goniometric</code>","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/goniometric/#AI4SurrogateModelling.src.sensitivity_analysis.goniometric","title":"AI4SurrogateModelling.src.sensitivity_analysis.goniometric","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/goniometric/#AI4SurrogateModelling.src.sensitivity_analysis.goniometric.construct_system","title":"construct_system","text":"<pre><code>construct_system(basis_values, target_values)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/goniometric.py</code> <pre><code>@Logger.logged()\ndef construct_system(\n    basis_values,\n    target_values,\n):\n    b = target_values\n\n    A = []\n    for basis_key, bvals in basis_values.items():\n        A.append(bvals)\n    A = np.concat(A, axis=1)\n\n    return A, b\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/goniometric/#AI4SurrogateModelling.src.sensitivity_analysis.goniometric.perform_sensitivity_analysis_Goniometric","title":"perform_sensitivity_analysis_Goniometric","text":"<pre><code>perform_sensitivity_analysis_Goniometric(\n    dataset: DatabaseTabular,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/goniometric.py</code> <pre><code>@Logger.logged()\ndef perform_sensitivity_analysis_Goniometric(\n    dataset: DatabaseTabular,\n):\n    x = np.array(dataset.get_input_data(), dtype=np.float32)\n    y = np.array(dataset.get_output_data(), dtype=np.float32)\n\n    Fs = {}\n\n    res = [1]\n    period_coeff = 0.25 / (2 * 2 * 2)\n    rank_prev = 0\n    idx = 0\n    while len(res) &gt; 0:\n        Fs[f\"cos_period_coefficient_{period_coeff}\"] = np.cos(x * period_coeff)\n        Fs[f\"sin_period_coefficient_{period_coeff}\"] = np.sin(x * period_coeff)\n\n        A, b = construct_system(\n            basis_values=Fs,\n            target_values=y,\n        )\n\n        (sol, res, rank, singular_values) = np.linalg.lstsq(A, b, rcond=None)\n        print(\n            f\"[{idx:6d}][Goniometric functions with period coefficient &lt;= {period_coeff:13.4f}]Rank: {rank:6d}(+{rank - rank_prev:6d}), residuals: {res:}\"\n        )\n\n        rank_prev = rank\n        period_coeff *= 2\n        idx += 1\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/gradient/","title":"Module <code>src.sensitivity_analysis.gradient</code>","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/gradient/#AI4SurrogateModelling.src.sensitivity_analysis.gradient","title":"AI4SurrogateModelling.src.sensitivity_analysis.gradient","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/gradient/#AI4SurrogateModelling.src.sensitivity_analysis.gradient.gradient_sensitivity","title":"gradient_sensitivity","text":"<pre><code>gradient_sensitivity(\n    *,\n    dataloader: DictionaryDataLoader,\n    model: ModelBase,\n    labels: dict[str, list[str]]\n) -&gt; tuple[list[list[float]], list[str], list[str]]\n</code></pre> <p>Calculates mean absolute gradients of outputs w.r.t. individual inputs.</p> <p>Iterates through a dataloader, performing a backward pass per output column (per output key) to extract dY/dX sensitivities. The resulting matrix is aggregated across MPI workers by averaging absolute gradients.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DictionaryDataLoader</code> <p>Source of evaluation batches.</p> required <code>model</code> <code>ModelBase</code> <p>Model whose gradients are analysed.</p> required <code>labels</code> <code>dict[str, list[str]]</code> <p>Mapping from key to per-column labels used to label the resulting matrix axes.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>tuple[list[list[float]], list[str], list[str]]: Matrix of sensitivities</p> <code>list[str]</code> <p>plus row/column labels.</p> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/gradient.py</code> <pre><code>def gradient_sensitivity(\n    *,\n    dataloader: DictionaryDataLoader,\n    model: ModelBase,\n    labels: dict[str, list[str]],\n) -&gt; tuple[list[list[float]], list[str], list[str]]:\n    \"\"\"Calculates mean absolute gradients of outputs w.r.t. individual inputs.\n\n    Iterates through a dataloader, performing a backward pass per output column\n    (per output key) to extract dY/dX sensitivities. The resulting matrix is\n    aggregated across MPI workers by averaging absolute gradients.\n\n    Args:\n        dataloader (DictionaryDataLoader): Source of evaluation batches.\n        model (ModelBase): Model whose gradients are analysed.\n        labels (dict[str, list[str]]): Mapping from key to per-column labels\n            used to label the resulting matrix axes.\n\n    Returns:\n        tuple[list[list[float]], list[str], list[str]]: Matrix of sensitivities\n        plus row/column labels.\n    \"\"\"\n    sensitivities: dict[str, dict[str, dict[int, list[np.ndarray]]]] = {}\n    for data in dataloader:\n        X = {k: torch.nn.Parameter(v) for k, v in data.items()}\n        Y = model(data_in=X)\n        # print(unparse_model(model).dtype)\n\n        output_items = list(Y.items())\n        for out_item_idx, (k_out, tensor_out) in enumerate(output_items):\n            if tensor_out.dim() == 1:\n                tensor_out = tensor_out.unsqueeze(-1)\n\n            n_outputs = tensor_out.shape[1]\n\n            for out_idx in range(n_outputs):\n                for x_tensor in X.values():\n                    if x_tensor.grad is not None:\n                        x_tensor.grad = None\n\n                v_sum = tensor_out[:, out_idx].sum()\n                retain_graph = not (\n                    out_item_idx == len(output_items) - 1\n                    and out_idx == n_outputs - 1\n                )\n                v_sum.backward(retain_graph=retain_graph)\n\n                for k_in, x_tensor in X.items():\n                    grad_tensor = x_tensor.grad\n                    if grad_tensor is None:\n                        continue\n\n                    if k_in not in sensitivities:\n                        sensitivities[k_in] = {}\n                    if k_out not in sensitivities[k_in]:\n                        sensitivities[k_in][k_out] = {}\n                    if out_idx not in sensitivities[k_in][k_out]:\n                        sensitivities[k_in][k_out][out_idx] = []\n\n                    sensitivities[k_in][k_out][out_idx].append(\n                        grad_tensor.detach().cpu().numpy()\n                    )\n                    x_tensor.grad = None\n\n    sensitivities_out = {}\n    k1_keys = sorted(list(sensitivities.keys()))\n    for k1 in k1_keys:\n        k2_keys = sorted(list(sensitivities[k1].keys()))\n        for k2 in k2_keys:\n            col_indices = sorted(list(sensitivities[k1][k2].keys()))\n            if not col_indices:\n                continue\n\n            if k1 not in sensitivities_out:\n                sensitivities_out[k1] = {}\n\n            col_results = []\n            for col_idx in col_indices:\n                M = sensitivities[k1][k2][col_idx]\n                if len(M) &lt;= 0:\n                    continue\n\n                M = np.vstack(M)\n                M = mpi.gather(M, root=0)\n                if mpi.get_rank() == 0:\n                    M = np.vstack(M)\n                    col_results.append(np.abs(M).mean(axis=0).reshape(-1, 1))\n\n            if mpi.get_rank() == 0 and col_results:\n                sensitivities_out[k1][k2] = np.hstack(col_results)\n\n    model_unparsed = unparse_model(model)\n    model_input_keys = model_unparsed.input_keys\n    model_output_keys = model_unparsed.output_keys\n\n    row_labels = []\n    col_labels = []\n    data_out = []\n    if mpi.get_rank() &gt; 0 or len(sensitivities_out) == 0:\n        return data_out, row_labels, col_labels\n\n    for output_key in model_output_keys:\n        output_labels = labels[output_key]\n        col_labels += output_labels\n\n    for input_key in model_input_keys:\n        input_labels = labels[input_key]\n        row_labels += input_labels\n\n    for input_key in model_input_keys:\n        for row_idx in range(len(labels[input_key])):\n            data_out_row = []\n            for output_key in model_output_keys:\n                for col_idx in range(len(labels[output_key])):\n                    data_out_row.append(\n                        float(\n                            sensitivities_out[input_key][output_key][\n                                row_idx, col_idx\n                            ]\n                        )\n                    )\n            data_out.append(data_out_row)\n\n    return data_out, row_labels, col_labels\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/morris/","title":"Module <code>src.sensitivity_analysis.morris</code>","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/morris/#AI4SurrogateModelling.src.sensitivity_analysis.morris","title":"AI4SurrogateModelling.src.sensitivity_analysis.morris","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/morris/#AI4SurrogateModelling.src.sensitivity_analysis.morris.morris_sensitivity","title":"morris_sensitivity","text":"<pre><code>morris_sensitivity(\n    *,\n    nsamples: int,\n    nlevels: int,\n    model: ModelBase,\n    labels: dict,\n    bounds: dict,\n    dtype: dtype\n) -&gt; dict\n</code></pre> <p>Evaluate Morris screening indices for the provided model outputs.</p> <p>The function defines a SALib problem from the model metadata, generates Morris trajectories, pushes them through the model without gradient tracking, and post-processes the results into per-output <code>mu_star</code> values.</p> <p>Parameters:</p> Name Type Description Default <code>nsamples</code> <code>int</code> <p>Number of trajectories (N) used by the Morris sampler.</p> required <code>nlevels</code> <code>int</code> <p>Discretization levels of the Morris grid.</p> required <code>model</code> <code>ModelBase</code> <p>Model that maps dictionary inputs to dictionary outputs.</p> required <code>labels</code> <code>dict</code> <p>Mapping from input/output keys to readable component labels used in SALib and the final report.</p> required <code>bounds</code> <code>dict</code> <p>Mapping from input keys to <code>(n, 2)</code> tensors containing lower and upper bounds for every labeled component.</p> required <code>dtype</code> <code>dtype</code> <p>Torch dtype used for tensors created from the sampled values.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>tuple[list[list[float]], list[str], list[str]]: Morris <code>mu_star</code> values arranged per output column then transposed per parameter, the flat list of input names, and the flat list of output labels. Non-root MPI ranks return <code>(None, None, None)</code>.</p> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/morris.py</code> <pre><code>def morris_sensitivity(\n    *,\n    nsamples: int,\n    nlevels: int,\n    model: ModelBase,\n    labels: dict,\n    bounds: dict,\n    dtype: torch.dtype,\n) -&gt; dict:\n    \"\"\"Evaluate Morris screening indices for the provided model outputs.\n\n    The function defines a SALib problem from the model metadata, generates\n    Morris trajectories, pushes them through the model without gradient\n    tracking, and post-processes the results into per-output ``mu_star`` values.\n\n    Args:\n        nsamples (int): Number of trajectories (N) used by the Morris sampler.\n        nlevels (int): Discretization levels of the Morris grid.\n        model (ModelBase): Model that maps dictionary inputs to dictionary\n            outputs.\n        labels (dict): Mapping from input/output keys to readable component\n            labels used in SALib and the final report.\n        bounds (dict): Mapping from input keys to ``(n, 2)`` tensors containing\n            lower and upper bounds for every labeled component.\n        dtype (torch.dtype): Torch dtype used for tensors created from the\n            sampled values.\n\n    Returns:\n        tuple[list[list[float]], list[str], list[str]]: Morris ``mu_star`` values\n            arranged per output column then transposed per parameter, the flat\n            list of input names, and the flat list of output labels. Non-root\n            MPI ranks return ``(None, None, None)``.\n    \"\"\"\n    if mpi.get_rank() &gt; 0:\n        return None, None, None\n\n    n_features = unparse_model(model).dim_in\n    keys_in = unparse_model(model).input_keys\n    keys_out = unparse_model(model).output_keys\n\n    # print(f'n_features: {n_features}')\n    problem = {\n        \"num_vars\": n_features,\n        \"names\": [],\n        \"bounds\": [],\n    }\n    for key in keys_in:\n        problem[\"names\"] += labels[key]\n        problem[\"bounds\"] += [\n            [float(bounds[key][i, 0]), float(bounds[key][i, 1])]\n            for i in range(len(labels[key]))\n        ]\n\n    morris_params = morris_sample.sample(\n        problem, N=nsamples, num_levels=nlevels, optimal_trajectories=None\n    )\n    # print(f'morris_params: {morris_params.shape}')\n    with torch.no_grad():\n        # transform param_values into the correct format\n        data_in = {}\n        shift = 0\n        for key in keys_in:\n            nloc = len(labels[key])\n            data_in[key] = torch.tensor(\n                morris_params[:, shift : shift + nloc], dtype=dtype\n            ).to(mpi._device)\n            shift += nloc\n            # print(f'key: {key} -&gt; {data_in[key].shape}')\n\n        data_out = model(data_in=data_in)\n        # transform processed outputs to the correct format\n        # Y = []\n        # for key in keys_out:\n        #     Y.append(data_out[key])\n        #     # print(f'key: {key} -&gt; {data_out[key].shape}')\n        # Y = torch.hstack(Y).detach().cpu().numpy()\n        # # print(f'Y: {Y.shape}')\n\n        # separate analysis for each output\n        labels_columns = []\n        for key in keys_out:\n            labels_columns += labels[key]\n\n        shift = 0\n        morris_indices_columns = []\n        noutputs = len(labels_columns)\n        for key in keys_out:\n            Y_key = data_out[key]\n            for output_idx in range(len(labels[key])):\n                morris_indices_tmp = morris_analyze.analyze(\n                    problem,\n                    morris_params,\n                    Y_key[:, output_idx],\n                    num_levels=nlevels,\n                    print_to_console=False,\n                )\n                morris_indices_column = [\n                    float(v) for v in morris_indices_tmp[\"mu_star\"]\n                ]\n                morris_indices_columns.append(morris_indices_column)\n\n            labels_columns += labels[key]\n            shift += noutputs\n\n    morris_indices_rows = [[] for _ in range(n_features)]\n    for row_idx in range(n_features):\n        for col_idx in range(noutputs):\n            morris_indices_rows[row_idx].append(\n                morris_indices_columns[col_idx][row_idx]\n            )\n    return morris_indices_rows, problem[\"names\"], labels_columns\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/","title":"Module <code>src.sensitivity_analysis.polynomial</code>","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial","title":"AI4SurrogateModelling.src.sensitivity_analysis.polynomial","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.PolynomialModelPytorch","title":"PolynomialModelPytorch","text":"<pre><code>PolynomialModelPytorch(coefficients, exponents)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    coefficients,\n    exponents,\n):\n    super(PolynomialModelPytorch, self).__init__()\n    self.coefficients = nn.Parameter(\n        torch.tensor(coefficients, dtype=torch.float32).unsqueeze(0)\n    )\n    self.exponents = torch.tensor(\n        exponents, dtype=torch.float32\n    ).T.unsqueeze(0)\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.PolynomialModelPytorch.coefficients","title":"coefficients  <code>instance-attribute</code>","text":"<pre><code>coefficients = Parameter(unsqueeze(0))\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.PolynomialModelPytorch.exponents","title":"exponents  <code>instance-attribute</code>","text":"<pre><code>exponents = unsqueeze(0)\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.PolynomialModelPytorch.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef forward(\n    self,\n    x,\n):\n    xhat = x.unsqueeze(-1)\n    xhat = xhat**self.exponents\n    xhat = torch.prod(xhat, dim=1).unsqueeze(-1)\n    xhat = xhat * self.coefficients\n    y = torch.sum(xhat, dim=1)\n\n    return y\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.construct_system","title":"construct_system","text":"<pre><code>construct_system(exponents, source_values, target_values)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef construct_system(\n    exponents,\n    source_values,\n    target_values,\n):\n    b = target_values\n    A = []\n    for polynomial_degree, bvals in exponents.items():\n        for exp in bvals:\n            A.append(evaluate_polynomial(exp, source_values))\n    A = np.concat(A, axis=1)\n\n    A = mpi.gather(A, root=0)\n    b = mpi.gather(b, root=0)\n\n    if mpi.get_rank() == 0:\n        return np.concat(A, axis=0), np.concat(b, axis=0)\n    return None, None\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.evaluate_polynomial","title":"evaluate_polynomial","text":"<pre><code>evaluate_polynomial(exponents, x)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef evaluate_polynomial(\n    exponents,\n    x,\n):\n    if len(x.shape) &lt;= 1:\n        return np.prod(\n            x.reshape(1, -1) ** exponents.reshape(1, -1), axis=1\n        ).reshape(-1, 1)\n\n    return np.prod(x ** exponents.reshape(1, -1), axis=1).reshape(-1, 1)\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.get_polynomial_degrees","title":"get_polynomial_degrees","text":"<pre><code>get_polynomial_degrees(n, d)\n</code></pre> <p>Generate a vector of polynomial degrees for all monomials in n dimensions with total degree at most d.</p> <p>Parameters: n (int): Number of dimensions (variables). d (int): Maximum total degree.</p> <p>Returns: numpy.ndarray: Vector containing the degrees of all monomials.</p> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef get_polynomial_degrees(\n    n,\n    d,\n):\n    \"\"\"Generate a vector of polynomial degrees for all monomials in n\n    dimensions with total degree at most d.\n\n    Parameters:\n    n (int): Number of dimensions (variables).\n    d (int): Maximum total degree.\n\n    Returns:\n    numpy.ndarray: Vector containing the degrees of all monomials.\n    \"\"\"\n    # Generate all possible exponent tuples (k1, k2, ..., kn) such that k1 + k2 + ... + kn &lt;= d\n    # For each dimension, exponents range from 0 to d\n    \"\"\"Returns a list of exponent tuples for polynomials in n variables of total degree \u2264 d.\"\"\"\n\n    exponents = {\n        0: [np.zeros(n, dtype=np.int32)],\n    }\n    exponents1 = []\n    for i in range(n):\n        exp1 = np.zeros(n, dtype=np.int32)\n        exp1[i] = 1\n        exponents1.append(exp1)\n\n    for pd in range(1, d + 1):\n        new_exponents = set()\n\n        previous_exponents = exponents[pd - 1]\n\n        for exp1 in previous_exponents:\n            for exp2 in exponents1:\n                exp3 = tuple(exp1 + exp2)\n\n                new_exponents.add(exp3)\n\n        exponents[pd] = list([np.array(v) for v in new_exponents])\n\n    return exponents\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.get_sensitivities","title":"get_sensitivities","text":"<pre><code>get_sensitivities(sol, exponents, x, y)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>def get_sensitivities(sol, exponents, x, y):\n    exponents_list = []\n    for k, v in exponents.items():\n        for exp in v:\n            exponents_list.append(exp)\n\n    poly_model = PolynomialModelPytorch(\n        coefficients=sol,\n        exponents=np.array(exponents_list),\n    )\n\n    xtorch = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n    predictions = poly_model(xtorch)\n\n    sensitivities = []\n    for i in range(predictions.shape[1]):\n        predictions_ = torch.sum(predictions[:, i])\n        grads = torch.autograd.grad(predictions_, xtorch)[0]\n        sensitivities.append(\n            torch.mean(torch.abs(grads), dim=0).detach().cpu().numpy()\n        )\n    sensitivities = np.array(sensitivities).T\n\n    return sensitivities\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.get_solution_error","title":"get_solution_error","text":"<pre><code>get_solution_error(sol, A, b, exponents, x, y)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef get_solution_error(sol, A, b, exponents, x, y):\n    reconstructed_msq = (A @ sol - b) ** 2\n    reconstructed_loss = np.mean(np.sum(reconstructed_msq, axis=1))\n\n    exponents_list = []\n    for k, v in exponents.items():\n        for exp in v:\n            exponents_list.append(exp)\n\n    poly_model = PolynomialModelPytorch(\n        coefficients=sol,\n        exponents=np.array(exponents_list),\n    )\n    with torch.no_grad():\n        reconstructed2_loss = (\n            nn.MSELoss()(\n                poly_model(torch.tensor(x, dtype=torch.float32)),\n                torch.tensor(y),\n            )\n            .cpu()\n            .numpy()\n        )\n\n    # optimizer = optim.Adam(poly_model.parameters(), lr = 1e-3)\n    # with torch.no_grad():\n    #     loss_init = nn.MSELoss()(poly_model(torch.tensor(x, dtype = torch.float32)), torch.tensor(y)).cpu().numpy()\n\n    # for epoch in range(100):\n    #     optimizer.zero_grad()  # Zero the gradients from the previous step\n\n    #     preds = poly_model(torch.tensor(x, dtype = torch.float32))\n\n    #     loss = nn.MSELoss()(preds, torch.tensor(y))\n    #     loss.backward()\n    #     optimizer.step()\n\n    # with torch.no_grad():\n    #     loss_final = nn.MSELoss()(poly_model(torch.tensor(x, dtype = torch.float32)), torch.tensor(y)).cpu().numpy()\n\n    # print(f'epoch[{epoch:6d}] loss -&gt; {np.log10(loss_init):6.3f} -&gt; {np.log10(loss_final):6.3f}')\n\n    # exp_idx = 0\n    # for i, alpha in enumerate(sol):\n    #     alpha = alpha.reshape(1, -1)\n    #     pol_eval = alpha * evaluate_polynomial(exponents_list[exp_idx], x)\n    #     reconstructed2_msq -= pol_eval\n    #     exp_idx += 1\n    # reconstructed2_msq = reconstructed2_msq ** 2\n    # reconstructed2_loss = np.mean(np.sum(reconstructed2_msq, axis = 1))\n\n    return reconstructed_loss, reconstructed2_loss\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/polynomial/#AI4SurrogateModelling.src.sensitivity_analysis.polynomial.perform_sensitivity_analysis_Polynomial","title":"perform_sensitivity_analysis_Polynomial","text":"<pre><code>perform_sensitivity_analysis_Polynomial(\n    dataset: DatabaseTabular,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/polynomial.py</code> <pre><code>@Logger.logged()\ndef perform_sensitivity_analysis_Polynomial(\n    dataset: DatabaseTabular,\n):\n    x = np.array(dataset.get_input_data(), dtype=np.float32)\n    y = np.array(dataset.get_output_data(), dtype=np.float32)\n\n    n1 = x.shape[1]\n\n    rank_prev = 0\n    poly_degree = 0\n\n    loss = 1\n    max_polynomials = 10\n    while loss &gt; 1e-6:\n\n        exponents = get_polynomial_degrees(n1, poly_degree)\n        Logger.info(\n            f\"Degree {poly_degree:3d} -&gt; {len(exponents[poly_degree]):6d} polynomials\"\n        )\n\n        A, b = construct_system(\n            exponents=exponents,\n            source_values=x,\n            target_values=y,\n        )\n\n        if A is not None and b is not None:\n            (sol, res, rank, singular_values) = np.linalg.lstsq(\n                A, b, rcond=None\n            )\n\n            if len(singular_values) &gt; max_polynomials:\n                print(singular_values)\n\n            err = get_solution_error(sol, A, b, exponents, x, y)\n\n            sensitivities = get_sensitivities(sol, exponents, x, y)\n\n            # polynomial_model = get_model(sol, range(poly_degree + 1))\n\n            Logger.info(\n                f\"[Polynomial with degree &lt;= {poly_degree:6d}]Rank: {rank:6d}(+{rank - rank_prev:6d}), loss[MSE]: {err[0]:10.7f},  {err[1]:10.7f}\"\n            )\n\n            for output_idx in range(sensitivities.shape[1]):\n                ordering = np.argsort(sensitivities[:, output_idx])\n\n                Logger.info(f\"Output: {dataset.get_output_label(output_idx)}\")\n                for i in ordering:\n                    Logger.info(\n                        f\"  Input[{dataset.get_input_label(i):30s}] -&gt; sensitivities: {sensitivities[i, output_idx]}\"\n                    )\n                Logger.info(\"\")\n\n            loss = err[0]\n            rank_prev = rank\n            poly_degree += 1\n</code></pre>"},{"location":"api/API%20Reference/src/sensitivity_analysis/sobol/","title":"Module <code>src.sensitivity_analysis.sobol</code>","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/sobol/#AI4SurrogateModelling.src.sensitivity_analysis.sobol","title":"AI4SurrogateModelling.src.sensitivity_analysis.sobol","text":""},{"location":"api/API%20Reference/src/sensitivity_analysis/sobol/#AI4SurrogateModelling.src.sensitivity_analysis.sobol.sobol_sensitivity","title":"sobol_sensitivity","text":"<pre><code>sobol_sensitivity(\n    *,\n    nsamples: int,\n    model: ModelBase,\n    labels: dict,\n    bounds: dict,\n    dtype: dtype\n) -&gt; dict\n</code></pre> <p>Compute first-order Sobol indices for each model output.</p> <p>The function builds a SALib problem definition from the provided model metadata, samples the corresponding input space via the Saltelli sampler, evaluates the supplied model without gradient tracking, and finally runs a Sobol analysis per output column.</p> <p>Parameters:</p> Name Type Description Default <code>nsamples</code> <code>int</code> <p>Number of draws used by the Saltelli sampler.</p> required <code>model</code> <code>ModelBase</code> <p>Model that consumes dictionary inputs and produces dictionary outputs.</p> required <code>labels</code> <code>dict</code> <p>Mapping from each input/output key to the ordered labels describing its scalar components.</p> required <code>bounds</code> <code>dict</code> <p>Mapping from each input key to a <code>(n, 2)</code> tensor that holds the lower/upper bound for every labeled component.</p> required <code>dtype</code> <code>dtype</code> <p>Torch dtype used for constructing the sampled tensors.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>tuple[list[list[float]], list[str], list[str]]: First-order Sobol indices per output column, the flattened list of input names, and the flattened list of output labels. All MPI ranks different from zero return <code>(None, None, None)</code>.</p> Source code in <code>AI4SurrogateModelling/src/sensitivity_analysis/sobol.py</code> <pre><code>def sobol_sensitivity(\n    *,\n    nsamples: int,\n    model: ModelBase,\n    labels: dict,\n    bounds: dict,\n    dtype: torch.dtype,\n) -&gt; dict:\n    \"\"\"Compute first-order Sobol indices for each model output.\n\n    The function builds a SALib problem definition from the provided model\n    metadata, samples the corresponding input space via the Saltelli sampler,\n    evaluates the supplied model without gradient tracking, and finally runs a\n    Sobol analysis per output column.\n\n    Args:\n        nsamples (int): Number of draws used by the Saltelli sampler.\n        model (ModelBase): Model that consumes dictionary inputs and produces\n            dictionary outputs.\n        labels (dict): Mapping from each input/output key to the ordered labels\n            describing its scalar components.\n        bounds (dict): Mapping from each input key to a ``(n, 2)`` tensor that\n            holds the lower/upper bound for every labeled component.\n        dtype (torch.dtype): Torch dtype used for constructing the sampled\n            tensors.\n\n    Returns:\n        tuple[list[list[float]], list[str], list[str]]: First-order Sobol\n            indices per output column, the flattened list of input names, and\n            the flattened list of output labels. All MPI ranks different from\n            zero return ``(None, None, None)``.\n    \"\"\"\n    if mpi.get_rank() &gt; 0:\n        return None, None, None\n\n    calc_second_order = False\n    n_features = unparse_model(model).dim_in\n    keys_in = unparse_model(model).input_keys\n    keys_out = unparse_model(model).output_keys\n    problem = {\n        \"num_vars\": n_features,\n        \"names\": [],\n        \"bounds\": [],\n    }\n    for key in keys_in:\n        problem[\"names\"] += labels[key]\n        problem[\"bounds\"] += [\n            [float(bounds[key][i, 0]), float(bounds[key][i, 1])]\n            for i in range(len(labels[key]))\n        ]\n\n    param_values = saltelli.sample(\n        problem, nsamples, calc_second_order=calc_second_order\n    )\n    with torch.no_grad():\n        # transform param_values into the correct format\n        data_in = {}\n        shift = 0\n        for key in keys_in:\n            nloc = len(labels[key])\n            data_in[key] = torch.tensor(\n                param_values[:, shift : shift + nloc], dtype=dtype\n            ).to(mpi._device)\n            shift += nloc\n\n        data_out = model(data_in=data_in)\n        # transform processed outputs to the correct format\n        Y = []\n        for key in keys_out:\n            Y.append(data_out[key])\n        Y = torch.hstack(Y).detach().cpu().numpy()\n\n        # separate analysis for each output\n        labels_columns = []\n        for key in keys_out:\n            labels_columns += labels[key]\n\n        shift = 0\n        sobol_indices_columns = []\n        noutputs = len(labels_columns)\n        for key in keys_out:\n            for output_idx in range(len(labels[key])):\n                sobol_indices_tmp = sobol.analyze(\n                    problem,\n                    Y[:, output_idx + shift],\n                    calc_second_order=calc_second_order,\n                    print_to_console=False,\n                )\n                sobol_indices_column = [\n                    max(0.0, float(v)) for v in sobol_indices_tmp[\"S1\"]\n                ]\n                sobol_indices_columns.append(sobol_indices_column)\n\n            labels_columns += labels[key]\n            shift += noutputs\n\n    sobol_indices_rows = [[] for _ in range(n_features)]\n    for row_idx in range(n_features):\n        for col_idx in range(noutputs):\n            sobol_indices_rows[row_idx].append(\n                sobol_indices_columns[col_idx][row_idx]\n            )\n    return sobol_indices_rows, problem[\"names\"], labels_columns\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/","title":"Module <code>src.training.manager</code>","text":""},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager","title":"AI4SurrogateModelling.src.training.manager","text":"<p>Contains modules and functions related to the functionality of training management, i.e.: storing/loading training statistics, histories and best performing models.</p>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager","title":"TrainingManager","text":"<pre><code>TrainingManager(\n    *, checkpoint_dir: str, reset: bool = False\n)\n</code></pre> <p>Persists training metadata, history and model artefacts.</p> <p>The manager abstracts disk I/O of checkpoints and training statistics. It can record arbitrary scalar statistics (<code>update_stats</code>), append training trajectories (<code>update_training_progress</code>), and keep track of both latest and best-performing model checkpoints.</p> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    *,\n    checkpoint_dir: str,\n    reset: bool = False,\n):\n    self.checkpoint_dir = checkpoint_dir\n\n    if checkpoint_dir is not None:\n        self.manager_fn = f\"{checkpoint_dir}/data.pkl\"\n        self.model_fn_best = f\"{checkpoint_dir}/model_best.bin\"\n        self.model_fn_latest = f\"{checkpoint_dir}/model_latest.bin\"\n\n        self.path_manager = Path(self.manager_fn)\n        self.path_model = Path(self.model_fn_best)\n\n        if reset:\n            self._reset()\n    else:\n        self.manager_fn = None\n        self.model_fn_best = None\n        self.model_fn_latest = None\n        self.path_manager = None\n        self.path_model = None\n\n    self._get_metadata_from_disk()\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.checkpoint_dir","title":"checkpoint_dir  <code>instance-attribute</code>","text":"<pre><code>checkpoint_dir = checkpoint_dir\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.manager_fn","title":"manager_fn  <code>instance-attribute</code>","text":"<pre><code>manager_fn = f'{checkpoint_dir}/data.pkl'\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.model_fn_best","title":"model_fn_best  <code>instance-attribute</code>","text":"<pre><code>model_fn_best = f'{checkpoint_dir}/model_best.bin'\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.model_fn_latest","title":"model_fn_latest  <code>instance-attribute</code>","text":"<pre><code>model_fn_latest = f'{checkpoint_dir}/model_latest.bin'\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.path_manager","title":"path_manager  <code>instance-attribute</code>","text":"<pre><code>path_manager = Path(manager_fn)\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.path_model","title":"path_model  <code>instance-attribute</code>","text":"<pre><code>path_model = Path(model_fn_best)\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.get_progress_data","title":"get_progress_data","text":"<pre><code>get_progress_data(xkey: str) -&gt; dict\n</code></pre> <p>Reformats stored history into grouped line-plot data structures.</p> <p>Parameters:</p> Name Type Description Default <code>xkey</code> <code>str</code> <p>Name of the metric to use as the common x-axis.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mapping hierarchical metric prefixes to the plot template</p> <code>dict</code> <p>consumed by the reporting layer.</p> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def get_progress_data(self, xkey: str) -&gt; dict:\n    \"\"\"Reformats stored history into grouped line-plot data structures.\n\n    Args:\n        xkey (str): Name of the metric to use as the common x-axis.\n\n    Returns:\n        dict: Mapping hierarchical metric prefixes to the plot template\n        consumed by the reporting layer.\n    \"\"\"\n    out = {}\n\n    # gather tree paths to construct the line graph groups\n    tree_paths = set()\n    tree_paths_special = set()\n    for k in self.training_runs_data[\"training_progress\"]:\n        if \"/\" not in k:\n            tree_paths_special.add(k)\n            continue\n\n        path = k.rsplit(\"/\", 1)[0]\n        tree_paths.add(path)\n\n    for path in tree_paths:\n        curve_data = {}\n\n        for k, v in self.training_runs_data[\"training_progress\"].items():\n            if not k.startswith(path):\n                continue\n\n            curve_data[k[len(path) + 1 :]] = {\n                \"x\": self.training_runs_data[\"training_progress\"][xkey],\n                \"y\": v,\n            }\n        new_data = {\n            \"line_plots\": {\n                \"xlabel\": xkey,\n                \"ylabel\": \"value\",\n                \"data\": curve_data,\n            }\n        }\n        out[path] = new_data\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.get_training_epoch","title":"get_training_epoch","text":"<pre><code>get_training_epoch() -&gt; int\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def get_training_epoch(self) -&gt; int:\n    return self.training_runs_data[\"nepochs\"]\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.get_training_time","title":"get_training_time","text":"<pre><code>get_training_time() -&gt; float\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def get_training_time(self) -&gt; float:\n    return self.training_runs_data[\"compute_time\"]\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.load_model_best","title":"load_model_best","text":"<pre><code>load_model_best() -&gt; ModelBase\n</code></pre> <p>Loads the best-scoring model checkpoint if it exists.</p> <p>Returns:</p> Type Description <code>ModelBase</code> <p>ModelBase | None: Deserialised model or <code>None</code> on failure.</p> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def load_model_best(self) -&gt; ModelBase:\n    \"\"\"Loads the best-scoring model checkpoint if it exists.\n\n    Returns:\n        ModelBase | None: Deserialised model or ``None`` on failure.\n    \"\"\"\n    try:\n        model = ModelBase.load_model(fn=self.model_fn_best)\n        return model\n    except Exception as e:\n        Logger.warning(\n            f\"Could not load the best model from {self.model_fn_best}: {e}\"\n        )\n\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.load_model_latest","title":"load_model_latest","text":"<pre><code>load_model_latest() -&gt; ModelBase\n</code></pre> <p>Loads the most recent model checkpoint if it exists.</p> <p>Returns:</p> Type Description <code>ModelBase</code> <p>ModelBase | None: Deserialised model or <code>None</code> on failure.</p> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def load_model_latest(self) -&gt; ModelBase:\n    \"\"\"Loads the most recent model checkpoint if it exists.\n\n    Returns:\n        ModelBase | None: Deserialised model or ``None`` on failure.\n    \"\"\"\n    try:\n        model = ModelBase.load_model(fn=self.model_fn_latest)\n        Logger.warning(f\"Loaded a model at: {self.model_fn_latest}\")\n        return model\n    except Exception as e:\n        Logger.warning(\n            f\"Could not load the latest model from {self.model_fn_latest}: {e}\"\n        )\n\n    return None\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.update_model_best","title":"update_model_best","text":"<pre><code>update_model_best(*, model: ModelBase, criterion: float)\n</code></pre> <p>Serialises the model as the best checkpoint when it improves.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelBase</code> <p>Candidate model to store.</p> required <code>criterion</code> <code>float</code> <p>Lower-is-better score compared against stored best.</p> required Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def update_model_best(\n    self,\n    *,\n    model: ModelBase,\n    criterion: float,\n):\n    \"\"\"Serialises the model as the best checkpoint when it improves.\n\n    Args:\n        model (ModelBase): Candidate model to store.\n        criterion (float): Lower-is-better score compared against stored best.\n    \"\"\"\n\n    if \"best_criterion\" not in self.training_runs_data:\n        self.training_runs_data[\"best_criterion\"] = float(\"inf\")\n\n    if criterion &lt; self.training_runs_data[\"best_criterion\"]:\n        self.training_runs_data[\"best_criterion\"] = criterion\n        self._update_metadata_on_disk()\n\n        mpi.make_dir(self.checkpoint_dir)\n        unparse_model(model).save_model(fn=self.model_fn_best)\n        Logger.info(\n            f\"New best model found with criterion {criterion:.6f}, saving to {self.model_fn_best}\"\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.update_model_latest","title":"update_model_latest","text":"<pre><code>update_model_latest(*, model: ModelBase)\n</code></pre> <p>Serialises the model as the latest checkpoint.</p> Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def update_model_latest(\n    self,\n    *,\n    model: ModelBase,\n):\n    \"\"\"Serialises the model as the latest checkpoint.\"\"\"\n    mpi.make_dir(self.checkpoint_dir)\n    unparse_model(model).save_model(fn=self.model_fn_latest)\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.update_stats","title":"update_stats","text":"<pre><code>update_stats(*, stats: dict)\n</code></pre> <p>Updates cached training statistics with user-provided values.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any]</code> <p>Mapping of statistic name to serialisable payload.</p> required Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def update_stats(\n    self,\n    *,\n    stats: dict,\n):\n    \"\"\"Updates cached training statistics with user-provided values.\n\n    Args:\n        stats (dict[str, Any]): Mapping of statistic name to serialisable payload.\n    \"\"\"\n\n    for key, value in stats.items():\n        self.training_runs_data[key] = copy.deepcopy(value)\n\n    self._update_metadata_on_disk()\n</code></pre>"},{"location":"api/API%20Reference/src/training/manager/#AI4SurrogateModelling.src.training.manager.TrainingManager.update_training_progress","title":"update_training_progress","text":"<pre><code>update_training_progress(\n    *, progress: HistoryProgress\n) -&gt; None\n</code></pre> <p>Appends HistoryProgress measurements and persists them.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>HistoryProgress</code> <p>History object holding batched metric series.</p> required Source code in <code>AI4SurrogateModelling/src/training/manager.py</code> <pre><code>def update_training_progress(\n    self,\n    *,\n    progress: HistoryProgress,\n) -&gt; None:\n    \"\"\"Appends HistoryProgress measurements and persists them.\n\n    Args:\n        progress (HistoryProgress): History object holding batched metric series.\n    \"\"\"\n    current_progress = self.training_runs_data[\"training_progress\"]\n    for k, v in progress.measurements.items():\n        if k not in current_progress:\n            current_progress[k] = []\n        current_progress[k] += v\n\n    self.training_runs_data[\"training_progress\"] = current_progress\n    self.training_runs_data[\"nepochs\"] = progress.measurements[\"epoch\"][-1]\n    self.training_runs_data[\"compute_time\"] = progress.measurements[\"time\"][\n        -1\n    ]\n\n    self._update_metadata_on_disk()\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/","title":"Module <code>src.training.measurement</code>","text":""},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement","title":"AI4SurrogateModelling.src.training.measurement","text":""},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement","title":"Measurement","text":"<pre><code>Measurement()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def __init__(\n    self,\n):\n    self.data = {}\n    self.iterator_data = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.iterator_data","title":"iterator_data  <code>instance-attribute</code>","text":"<pre><code>iterator_data = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.__add__","title":"__add__","text":"<pre><code>__add__(x: Measurement)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def __add__(\n    self,\n    x: \"Measurement\",\n):\n\n    x_data_set = set(x.iterator_data.keys())\n    y_data_set = set(self.iterator_data.keys())\n    z_data_set = x_data_set &amp; y_data_set\n\n    if len(z_data_set) &gt; 0:\n        mpi.abort(\n            0,\n            f\"A + B cannot be performed, some of the elements are the same: {z_data_set}\",\n        )\n\n    out = Measurement()\n    out.data = self.data.copy()\n    out.iterator_data = self.iterator_data.copy()\n\n    for k, v in x.data.items():\n        out.data[k] = v\n\n    for k, v in x.iterator_data.items():\n        out.iterator_data[k] = v\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def __getitem__(\n    self,\n    key: str,\n):\n    if not isinstance(key, str):\n        mpi.abort(\n            0,\n            f\"Measurement[key] called, key must be a string, but is: {type(key)}\",\n        )\n\n    if key not in self.iterator_data.keys():\n        mpi.abort(\n            0,\n            f\"Measurement[key] called, key: {key} has not been found in: {list(self.iterator_data.keys())}\",\n        )\n\n    return self.iterator_data[key]\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def __iter__(\n    self,\n):\n    return iter([(k, v) for k, v in self.iterator_data.items()])\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def __str__(\n    self,\n):\n    return f\"{self.data}\"\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.add_measurement","title":"add_measurement","text":"<pre><code>add_measurement(*, tree_keys: list[str], values)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def add_measurement(\n    self,\n    *,\n    tree_keys: list[str],\n    values,\n):\n    ptr = self.data\n    values_key = \"/\".join(tree_keys)\n\n    for key in tree_keys[:-1]:\n        if key not in ptr.keys():\n            ptr[key] = {}\n        ptr = ptr[key]\n\n    if isinstance(values, list):\n        ptr[tree_keys[-1]] = {}\n        ptr = ptr[tree_keys[-1]]\n\n        for idx, value in enumerate(values):\n            ptr[f\"{idx}\"] = value\n            self.iterator_data[f\"{values_key}/{idx}\"] = value\n\n        return\n\n    ptr[tree_keys[-1]] = values\n    self.iterator_data[values_key] = values\n</code></pre>"},{"location":"api/API%20Reference/src/training/measurement/#AI4SurrogateModelling.src.training.measurement.Measurement.add_prefix","title":"add_prefix","text":"<pre><code>add_prefix(prefix: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/measurement.py</code> <pre><code>def add_prefix(\n    self,\n    prefix: str,\n):\n    data_new = {prefix: self.data}\n\n    iterator_data_new = {}\n    for k, v in self.iterator_data.items():\n        iterator_data_new[f\"{prefix}/{k}\"] = v\n\n    self.data = data_new\n    self.iterator_data = iterator_data_new\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainer/","title":"Module <code>src.training.trainer</code>","text":""},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer","title":"AI4SurrogateModelling.src.training.trainer","text":""},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer.Trainer","title":"Trainer","text":""},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer.Trainer.experiment","title":"experiment  <code>staticmethod</code>","text":"<pre><code>experiment(model, dataloader)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/trainer.py</code> <pre><code>@staticmethod\ndef experiment(\n    model,\n    dataloader,\n):\n    # intermediates_forward = {}\n\n    # def hook_recursive(\n    #     M,\n    # ):\n    #     for name, P in M.named_parameters():\n    #         Logger.warning(f'{name} : {P}')\n    #     # for name, C in M.named_children():\n    #         # Logger.warning(f'{name}: {C}')\n    #         # hook_recursive(C)\n    #     # Logger.warning(M.named_children())\n    #     # Logger.warning(type(M))\n    #     # Logger.warning(M.__dict__)\n    #     # # Register hooks\n    #     # for layer in model.children():\n    #     #     for\n    #     #     layer.register_forward_hook(hook_fn_forward)\n\n    # def hook_fn_forward(module, input, output):\n\n    # # hook_recursive(M = model)\n\n    # model.register_forward_hook(hook_fn_forward)\n\n    visited_vertices = set()\n\n    def traverse_graph(grad_fn, depth=0):\n\n        if grad_fn is None or grad_fn in visited_vertices:\n            return\n\n        visited_vertices.add(grad_fn)\n\n        indent = \"  \" * depth\n        Logger.warning(f\"{indent}[{grad_fn.name()}]{dir(grad_fn)}\")\n\n        for next_fn, _ in grad_fn.next_functions:\n            if next_fn is not None:\n                traverse_graph(next_fn, depth + 1)\n\n    for x, target in dataloader:\n        # Then forward pass\n        output = model(x)\n\n        traverse_graph(output.grad_fn)\n        # Logger.warning(output.grad_fn)\n\n        break\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer.Trainer.gather_losses","title":"gather_losses  <code>staticmethod</code>","text":"<pre><code>gather_losses(\n    *,\n    training_dataloader: DataLoader,\n    validation_dataloader: DataLoader,\n    model: ModelBase,\n    criterion: CompositeLoss,\n    optimizer: CompositeOptimizer = None,\n    scheduler: CompositeScheduler = None\n) -&gt; tuple[Measurement, float]\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/trainer.py</code> <pre><code>@staticmethod\ndef gather_losses(\n    *,\n    training_dataloader: DataLoader,\n    validation_dataloader: DataLoader,\n    model: Model,\n    criterion: CompositeLoss,\n    optimizer: CompositeOptimizer = None,\n    scheduler: CompositeScheduler = None,\n) -&gt; tuple[Measurement, float]:\n\n    model.eval()\n    criterion_validation, _, monitored_loss_validation = criterion(\n        dataloader=validation_dataloader,\n        model=model,\n    )\n\n    model.train()\n    criterion_train, learning_rate_multiplier, monitored_loss_train = (\n        criterion(\n            dataloader=training_dataloader,\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n        )\n    )\n    criterion_validation.add_prefix(\"validation\")\n    criterion_train.add_prefix(\"train\")\n    criterion_data = criterion_validation + criterion_train\n\n    return (\n        criterion_data,\n        learning_rate_multiplier,\n        monitored_loss_train,\n        monitored_loss_validation,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer.Trainer.gather_metrics","title":"gather_metrics  <code>staticmethod</code>","text":"<pre><code>gather_metrics(\n    *,\n    training_dataloader: DataLoader,\n    validation_dataloader: DataLoader,\n    model: ModelBase,\n    metric: CompositeMetric\n) -&gt; Measurement\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/trainer.py</code> <pre><code>@staticmethod\ndef gather_metrics(\n    *,\n    training_dataloader: DataLoader,\n    validation_dataloader: DataLoader,\n    model: Model,\n    metric: CompositeMetric,\n) -&gt; Measurement:\n    if metric is None:\n        return None\n\n    metrics = metric(\n        training_dataloader=training_dataloader,\n        validation_dataloader=validation_dataloader,\n        model=model,\n    )\n\n    return metrics\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainer/#AI4SurrogateModelling.src.training.trainer.Trainer.train","title":"train  <code>staticmethod</code>","text":"<pre><code>train(\n    *,\n    reset: bool = False,\n    model_assembler: ModelAssembler = None,\n    optimizer_assembler: OptimizerAssembler = None,\n    scheduler_assembler: SchedulerAssembler = None,\n    loss_assembler: LossAssembler = None,\n    metric_assembler: MetricAssembler = None,\n    training_config: dict,\n    dataset_config: dict,\n    progress_save_dir: str\n) -&gt; ModelBase\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/trainer.py</code> <pre><code>@staticmethod\ndef train(\n    *,\n    reset: bool = False,\n    model_assembler: ModelAssembler = None,\n    optimizer_assembler: OptimizerAssembler = None,\n    scheduler_assembler: SchedulerAssembler = None,\n    loss_assembler: LossAssembler = None,\n    metric_assembler: MetricAssembler = None,\n    training_config: dict,\n    dataset_config: dict,\n    progress_save_dir: str,\n) -&gt; Model:\n    nepochs = training_config[\"nepochs\"]\n    dtype = training_config[\"dtype\"]\n\n    training_manager = TrainingManager(\n        reset=reset,\n        manager_save_dir=progress_save_dir,\n        model_assembler=model_assembler,\n        optimizer_assembler=optimizer_assembler,\n        scheduler_assembler=scheduler_assembler,\n        loss_assembler=loss_assembler,\n        metric_assembler=metric_assembler,\n        dtype=dtype,\n    )\n\n    model = training_manager.get_model()\n    optimizer = training_manager.get_optimizer()\n    scheduler = training_manager.get_scheduler()\n    criterion = training_manager.get_criterion()\n    metric = training_manager.get_metric()\n\n    training_dataloader, validation_dataloader = dataset_config[\n        \"dataset\"\n    ].get_dataloaders(\n        train_ratio=dataset_config[\"train_ratio\"],\n        validation_ratio=dataset_config[\"validation_ratio\"],\n        batch_size=dataset_config[\"batch_size\"],\n        dtype=dtype,\n        inputs_require_gradient=criterion.contains_objective(\n            Objectives.INPUT_GRADIENT\n        )\n        or metric.contains_objective(Objectives.INPUT_GRADIENT),\n    )\n\n    pbar = tqdm(\n        total=nepochs, desc=f\"[{mpi.get_rank():3d}]Training progress\"\n    )\n    metrics = Trainer.gather_metrics(\n        training_dataloader=training_dataloader,\n        validation_dataloader=validation_dataloader,\n        model=model,\n        metric=metric,\n    )\n    training_manager.add_metric_data(\n        metric_data=metrics,\n    )\n    best_monitored_loss = None\n    the_best_model = None\n    for epoch_index in range(nepochs):\n\n        # metrics = Trainer.gather_metrics(\n        #     training_dataloader = training_dataloader,\n        #     validation_dataloader = validation_dataloader,\n        #     model = model,\n        #     metric = metric,\n        # )\n\n        # training_manager.add_metric_data(\n        #     metric_data = metrics,\n        # )\n\n        TimeMonitor.start(\"training\")\n        (\n            criterion_data,\n            learning_rate_multiplier,\n            monitored_loss_train,\n            monitored_loss_validation,\n        ) = Trainer.gather_losses(\n            training_dataloader=training_dataloader,\n            validation_dataloader=validation_dataloader,\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            criterion=criterion,\n        )\n        time_taken_cpu, time_taken_wall = TimeMonitor.get(\"training\")\n\n        training_manager.add_criterion_data(\n            criterion_data=criterion_data,\n        )\n        training_manager.add_learning_rate(\n            lr=learning_rate_multiplier,\n        )\n        training_manager.add_time(\n            time_cpu=time_taken_cpu,\n            time_wall=time_taken_wall,\n        )\n        training_manager.step()\n\n        # Logger.warning(f'[{epoch_index:6d}] LR: {np.log10(learning_rate_multiplier):8.6f}')\n\n        if (\n            best_monitored_loss is None\n            or monitored_loss_train &lt; best_monitored_loss\n        ):\n            best_monitored_loss = monitored_loss_train\n            the_best_model = copy.deepcopy(model)\n\n        criterion_train_str = (\n            f\"{np.log10(monitored_loss_train.item()):8.5f}\"\n        )\n        criterion_validation_str = (\n            f\"{np.log10(monitored_loss_validation.item()):8.5f}\"\n        )\n\n        best_monitored_loss_str = (\n            f\"{np.log10(best_monitored_loss.item()):8.5f}\"\n        )\n        pbar.set_description(\n            f\"[{mpi.get_rank():3d}]Training progress: TRAIN({criterion_train_str}), VALIDATION({criterion_validation_str}), BEST TRAIN: {best_monitored_loss_str}\"\n        )\n\n        pbar.update(n=1)\n    pbar.close()\n\n    differences_absolute = []\n    differences_relative = []\n    tol = 1e-9\n    with torch.no_grad():\n        for x, target in training_dataloader:\n            y = the_best_model(x).reshape(-1)\n            dabs = torch.abs(y - target.reshape(-1))\n            drel = (dabs + tol) / (torch.abs(target).reshape(-1) + tol)\n\n            differences_absolute += dabs.numpy().tolist()\n            differences_relative += drel.numpy().tolist()\n\n    differences_absolute = mpi.gather(differences_absolute, root=0)\n    differences_relative = mpi.gather(differences_relative, root=0)\n    if mpi.get_rank() == 0:\n        differences_absolute = np.array(\n            [d for D in differences_absolute for d in D]\n        )\n        differences_relative = np.array(\n            [d for D in differences_relative for d in D]\n        )\n\n        abs_min = differences_absolute.min()\n        abs_max = differences_absolute.max()\n        abs_mean = differences_absolute.mean()\n\n        rel_min = differences_relative.min()\n        rel_max = differences_relative.max()\n        rel_mean = differences_relative.mean()\n\n        Logger.info(f\"Training finished...\")\n        Logger.info(f\"  Absolute differences:\")\n        Logger.info(f\"    min : {abs_min:12.8f}\")\n        Logger.info(f\"    max : {abs_max:12.8f}\")\n        Logger.info(f\"    mean: {abs_mean:12.8f}\")\n        Logger.info(f\"  Relative differences:\")\n        Logger.info(f\"    min : {100*rel_min:12.3f} [%]\")\n        Logger.info(f\"    max : {100*rel_max:12.3f} [%]\")\n        Logger.info(f\"    mean: {100*rel_mean:12.3f} [%]\")\n\n    metrics = Trainer.gather_metrics(\n        training_dataloader=training_dataloader,\n        validation_dataloader=validation_dataloader,\n        model=the_best_model,\n        metric=metric,\n    )\n    training_manager.add_metric_data(\n        metric_data=metrics,\n    )\n    # training_manager.save()\n\n    training_manager.export_to_tensorboard(\n        target_dir=f\"{training_manager.manager_save_dir}/tensorboard\",\n    )\n\n    # Trainer.experiment(\n    #     model,\n    #     training_dataloader\n    # )\n\n    return the_best_model\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/","title":"Module <code>src.training.__manager.manager_tabular</code>","text":""},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular","title":"AI4SurrogateModelling.src.training.__manager.manager_tabular","text":""},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular","title":"ManagerTabular","text":"<pre><code>ManagerTabular(\n    target_dir: str,\n    dataset: DatabaseTabular,\n    train_ratio: float,\n    validation_ratio: float,\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/__manager/manager_tabular.py</code> <pre><code>@Logger.logged()\ndef __init__(\n    self,\n    target_dir: str,\n    dataset: DatabaseTabular,\n    train_ratio: float,\n    validation_ratio: float,\n):\n\n    if train_ratio &lt;= 0 or train_ratio &gt; 1:\n        mpi.abort(\n            0,\n            f\"Train portion is: {train_ratio}, which does not lie in ({0}, {1}]\",\n        )\n\n    if validation_ratio &lt; 0 or validation_ratio &gt;= 1:\n        mpi.abort(\n            0,\n            f\"Validation portion is: {validation_ratio}, which does not lie in [{0}, {1})\",\n        )\n\n    if validation_ratio + train_ratio &gt; 1:\n        mpi.abort(\n            0,\n            f\"Validation + Training portion is: {validation_ratio + train_ratio}, which &gt; 1\",\n        )\n\n    mpi.assert_equality(train_ratio)\n    mpi.assert_equality(validation_ratio)\n\n    self.project_dir = target_dir\n    self.dataset = dataset\n    self.train_p = train_ratio\n    self.valid_p = validation_ratio\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular.project_dir","title":"project_dir  <code>instance-attribute</code>","text":"<pre><code>project_dir = target_dir\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular.train_p","title":"train_p  <code>instance-attribute</code>","text":"<pre><code>train_p = train_ratio\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular.valid_p","title":"valid_p  <code>instance-attribute</code>","text":"<pre><code>valid_p = validation_ratio\n</code></pre>"},{"location":"api/API%20Reference/src/training/__manager/manager_tabular/#AI4SurrogateModelling.src.training.__manager.manager_tabular.ManagerTabular.perform_tests","title":"perform_tests","text":"<pre><code>perform_tests(\n    ntests,\n    models: list[Module],\n    optimizers: list[Optimizer],\n    schedulers: list[LRScheduler],\n    parameter_range_batch_size: tuple[int, int],\n    parameter_ranges_models: list[\n        list[dict[str, tuple[int, int]]]\n    ],\n    parameter_ranges_optimizers: list[\n        list[dict[str, tuple[int, int]]]\n    ],\n    parameter_ranges_schedulers: list[\n        list[dict[str, tuple[int, int]]]\n    ],\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/__manager/manager_tabular.py</code> <pre><code>@Logger.logged()\ndef perform_tests(\n    self,\n    ntests,\n    models: list[nn.Module],\n    optimizers: list[Optimizer],\n    schedulers: list[LRScheduler],\n    parameter_range_batch_size: tuple[int, int],\n    parameter_ranges_models: list[list[dict[str, tuple[int, int]]]],\n    parameter_ranges_optimizers: list[list[dict[str, tuple[int, int]]]],\n    parameter_ranges_schedulers: list[list[dict[str, tuple[int, int]]]],\n):\n    failed_tests = {}\n\n    if len(failed_tests) &gt; 0:\n        Logger.info(\n            f\"Number of failed isntances: {len(failed_tests)}, reasons: {failed_tests}\"\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/","title":"Module <code>src.training.loss.loss_base</code>","text":""},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base","title":"AI4SurrogateModelling.src.training.loss.loss_base","text":"<p>Contains modules and functions related to the base class of various losses to be used during model training.</p>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase","title":"LossBase","text":"<pre><code>LossBase(\n    *,\n    model_parameters: dict = None,\n    outputs: dict = None,\n    derivatives: dict = None\n)\n</code></pre> <p>A base class containing basic functionality and configuration for loss calculations. All loss classes should derive from this class.</p> <p>The base loss class has the following capabilities: - loss on predicted values, model weights, model biases</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_parameters: dict = None,\n    outputs: dict = None,\n    derivatives: dict = None,\n):\n\n    self.coefficients = {}\n    if model_parameters is not None:\n        self.coefficients[\"model_parameters\"] = deepcopy(model_parameters)\n    else:\n        self.coefficients[\"model_parameters\"] = None\n\n    if outputs is not None:\n        self.coefficients[\"outputs\"] = deepcopy(outputs)\n    else:\n        self.coefficients[\"outputs\"] = None\n\n    if derivatives is not None:\n        self.coefficients[\"derivatives\"] = deepcopy(derivatives)\n    else:\n        self.coefficients[\"derivatives\"] = None\n\n    self._reset_losses()\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.coefficients","title":"coefficients  <code>instance-attribute</code>","text":"<pre><code>coefficients = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.loss_keys","title":"loss_keys  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>loss_keys = {'mae', 'mse', 'rmae', 'rmse'}\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    model: ModelBase = None,\n    data: dict = None,\n    predictions: dict = None,\n    total_data_size: int = 1,\n    number_of_batches: int = 1\n) -&gt; Tensor\n</code></pre> <p>Goes through all provided loss dictionaries and calculates up-to date losses scaled by the user provided factors.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelBase</code> <p>Model for which to calculate the                metrics.</p> <code>None</code> <code>data</code> <code>dict</code> <p>Data provided by the dataloader, usually                contains inputs and expected outputs.</p> <code>None</code> <code>predictions</code> <code>predictions</code> <p>Data provided via the model                evaluating the input data, i.e. predictions.</p> <code>None</code> <code>total_data_size</code> <code>int</code> <p>total number of data entries over all MPI                processes and across all batches.</p> <code>1</code> <code>number_of_batches</code> <code>int</code> <p>total number of batches.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A single differentiable scalar value.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def __call__(\n    self,\n    *,\n    model: ModelBase = None,\n    data: dict = None,\n    predictions: dict = None,\n    total_data_size: int = 1,\n    number_of_batches: int = 1,\n) -&gt; torch.Tensor:\n    \"\"\"Goes through all provided loss dictionaries and calculates up-to date\n    losses scaled by the user provided factors.\n\n    Args:\n        model (ModelBase, optional): Model for which to calculate the\\\n            metrics.\n        data (dict, optional): Data provided by the dataloader, usually\\\n            contains inputs and expected outputs.\n        predictions (predictions, optional): Data provided via the model\\\n            evaluating the input data, i.e. predictions.\n        total_data_size (int): total number of data entries over all MPI\\\n            processes and across all batches.\n        number_of_batches (int): total number of batches.\n\n    Returns:\n        torch.Tensor: A single differentiable scalar value.\n    \"\"\"\n    self._reset_losses(raw=True, accumulated=False)\n\n    self._eval_model_losses(\n        model=model, number_of_batches=number_of_batches\n    )\n    self._eval_prediction_losses(\n        data=data, predictions=predictions, total_size=total_data_size\n    )\n    self._eval_input_derivative_losses(\n        data=data, predictions=predictions, total_size=total_data_size\n    )\n\n    loss = self._gather_losses()\n    with torch.no_grad():\n        self.total_loss += loss\n\n    self.total_criterion += self.get_criterion()\n\n    return loss\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.get_accumulated_raw_losses","title":"get_accumulated_raw_losses","text":"<pre><code>get_accumulated_raw_losses() -&gt; dict\n</code></pre> <p>Goes through all the partial loss results, sums them up across all MPI processes and returns a dictionary containing non-nested loss measurements same on all MPI processes.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A non-nested dictionary containing all losses.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def get_accumulated_raw_losses(\n    self,\n) -&gt; dict:\n    \"\"\"Goes through all the partial loss results, sums them up across all\n    MPI processes and returns a dictionary containing non-nested loss\n    measurements same on all MPI processes.\n\n    Returns:\n        dict: A non-nested dictionary containing all losses.\n    \"\"\"\n\n    parsed_dictionary_tmp = parse_dictionary_data(\n        self.losses_raw_accumulated\n    )\n\n    parsed_dictionary = {}\n    for k, v in parsed_dictionary_tmp.items():\n        if k.startswith(\"outputs-\"):\n            parsed_dictionary[k[len(\"outputs-\") :]] = v\n            continue\n        if k.startswith(\"derivatives-\"):\n            parsed_dictionary[k[len(\"derivatives-\") :]] = v\n            continue\n        if k.startswith(\"model_parameters-\"):\n            parsed_dictionary[\n                f'model_parameters/{k[len(\"model_parameters-\"):]}'\n            ] = v\n            continue\n        parsed_dictionary[k] = v\n\n    keys_sorted = sorted(list(parsed_dictionary.keys()))\n    values = np.array([parsed_dictionary[k] for k in keys_sorted])\n    values = mpi.allreduce(values, mpi.OP.SUM) / mpi.get_world_size()\n\n    for idx, k in enumerate(keys_sorted):\n        parsed_dictionary[k] = values[idx]\n\n    return parsed_dictionary\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.get_criterion","title":"get_criterion","text":"<pre><code>get_criterion() -&gt; Tensor\n</code></pre> <p>Returns the UNSCALED loss associated with the prediction losses. Useful for measuring the real properties of the model.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Non-differentiable sum of all losses associated with the predictions and                expectations.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def get_criterion(\n    self,\n) -&gt; torch.Tensor:\n    \"\"\"Returns the UNSCALED loss associated with the prediction losses.\n    Useful for measuring the real properties of the model.\n\n    Returns:\n        torch.Tensor: Non-differentiable sum of all losses associated with the predictions and\\\n            expectations.\n    \"\"\"\n\n    def _gather_recursive_no_scaling(\n        loss,\n    ):\n        loss_value = 0\n\n        if isinstance(loss, dict):\n            for k, v in loss.items():\n                loss_value_loc = _gather_recursive_no_scaling(\n                    v,\n                )\n                loss_value = loss_value + loss_value_loc\n        else:\n            return loss\n\n        return loss_value\n\n    with torch.no_grad():\n        loss_outputs = _gather_recursive_no_scaling(\n            self.losses_raw[\"outputs\"],\n        )\n\n    return loss_outputs\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.get_total_criterion","title":"get_total_criterion","text":"<pre><code>get_total_criterion() -&gt; float\n</code></pre> <p>Computes the total sum of all raw losses related to the predictions category and returns it to the user.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Total criterion summed over all MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def get_total_criterion(\n    self,\n) -&gt; float:\n    \"\"\"Computes the total sum of all raw losses related to the predictions\n    category and returns it to the user.\n\n    Returns:\n        float: Total criterion summed over all MPI processes.\n    \"\"\"\n\n    total_criterion = (\n        mpi.allreduce(self.total_criterion, op=mpi.OP.SUM)\n        / mpi.get_world_size()\n    )\n    return total_criterion\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.get_total_scaled_loss","title":"get_total_scaled_loss","text":"<pre><code>get_total_scaled_loss() -&gt; float\n</code></pre> <p>Computes the total sum of all scaled losses and returns it to the user.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Total loss summed over all MPI processes.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def get_total_scaled_loss(\n    self,\n) -&gt; float:\n    \"\"\"Computes the total sum of all scaled losses and returns it to the\n    user.\n\n    Returns:\n        float: Total loss summed over all MPI processes.\n    \"\"\"\n\n    loss_total = (\n        mpi.allreduce(self.total_loss, op=mpi.OP.SUM) / mpi.get_world_size()\n    )\n    return loss_total\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.LossBase.what_requires_gradients","title":"what_requires_gradients","text":"<pre><code>what_requires_gradients() -&gt; dict\n</code></pre> <p>If any loss requires the calculation of the partial derivatives w.r.t. to any of the data, returns a dictionary containing those data keys.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>keys of database subsets which require gradients.</p> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def what_requires_gradients(\n    self,\n) -&gt; dict:\n    \"\"\"If any loss requires the calculation of the partial derivatives\n    w.r.t. to any of the data, returns a dictionary containing those data\n    keys.\n\n    Returns:\n        dict: keys of database subsets which require gradients.\n    \"\"\"\n    if \"derivatives\" not in self.coefficients:\n        return {}\n\n    out = {}\n    if not isinstance(self.coefficients[\"derivatives\"], dict):\n        return out\n\n    for derive_what, v in self.coefficients[\"derivatives\"].items():\n        for derive_with, w in v.items():\n            out[derive_with] = True\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/training/loss/loss_base/#AI4SurrogateModelling.src.training.loss.loss_base.parse_constraint","title":"parse_constraint","text":"<pre><code>parse_constraint(expr: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/loss/loss_base.py</code> <pre><code>def parse_constraint(expr: str):\n    pattern = r\"(&lt;=|&gt;=|&lt;|&gt;|=)\\s*([-+]?\\d*\\.?\\d+)\"\n    match = re.search(pattern, expr.strip())\n    if match:\n        operator, number = match.groups()\n        return operator, float(number)\n    else:\n        raise ValueError(f\"Invalid expression: {expr}\")\n</code></pre>"},{"location":"api/API%20Reference/src/training/metric/metric_base/","title":"Module <code>src.training.metric.metric_base</code>","text":""},{"location":"api/API%20Reference/src/training/metric/metric_base/#AI4SurrogateModelling.src.training.metric.metric_base","title":"AI4SurrogateModelling.src.training.metric.metric_base","text":"<p>Contains modules and functions related to the base class of various metrics to be calculated and stored during model training.</p>"},{"location":"api/API%20Reference/src/training/metric/metric_base/#AI4SurrogateModelling.src.training.metric.metric_base.MetricBase","title":"MetricBase","text":"<pre><code>MetricBase()\n</code></pre> <p>A base class containing basic functionality and configuration for metric calculations. All metric classes should derive from this class.</p> Source code in <code>AI4SurrogateModelling/src/training/metric/metric_base.py</code> <pre><code>def __init__(\n    self,\n):\n    pass\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/","title":"Module <code>src.training.optimizer.optimizer_configuration</code>","text":""},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration","title":"AI4SurrogateModelling.src.training.optimizer.optimizer_configuration","text":""},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer","title":"CompositeOptimizer","text":"<pre><code>CompositeOptimizer(\n    *,\n    optimizers: list[Optimizer],\n    epoch_activations: list[int]\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def __init__(\n    self,\n    *,\n    optimizers: list[optim.Optimizer],\n    epoch_activations: list[int],\n):\n    self.optimizers = optimizers.copy()\n    self.batch_index = 0\n    self.epoch_index = 0\n    self.current_optimizer_index = 0\n    self.param_groups = optimizers[0].param_groups\n\n    epoch_activation_period = 0\n    self.epoch_activations = []\n    for epoch_activation in epoch_activations:\n        epoch_activation_period += epoch_activation\n        self.epoch_activations.append(epoch_activation_period)\n\n    super().__init__(self.optimizers[0].param_groups, {})\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.batch_index","title":"batch_index  <code>instance-attribute</code>","text":"<pre><code>batch_index = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.current_optimizer_index","title":"current_optimizer_index  <code>instance-attribute</code>","text":"<pre><code>current_optimizer_index = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.epoch_activations","title":"epoch_activations  <code>instance-attribute</code>","text":"<pre><code>epoch_activations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.epoch_index","title":"epoch_index  <code>instance-attribute</code>","text":"<pre><code>epoch_index = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.optimizers","title":"optimizers  <code>instance-attribute</code>","text":"<pre><code>optimizers = copy()\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.param_groups","title":"param_groups  <code>instance-attribute</code>","text":"<pre><code>param_groups = param_groups\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.load_optimizer","title":"load_optimizer  <code>staticmethod</code>","text":"<pre><code>load_optimizer(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>@staticmethod\ndef load_optimizer(\n    *,\n    fn: str,\n):\n    with open(fn, \"rb\") as f:\n        optimizer = pickle.load(f)\n\n    return optimizer\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.next_epoch","title":"next_epoch","text":"<pre><code>next_epoch()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def next_epoch(\n    self,\n):\n    optimizer_index_tmp = self.current_optimizer_index\n\n    self.epoch_index += 1\n\n    if (\n        self.epoch_index\n        % self.epoch_activations[self.current_optimizer_index]\n        == 0\n    ):\n        self.current_optimizer_index += 1\n\n    if self.epoch_index % self.epoch_activations[-1] == 0:\n        self.current_optimizer_index = 0\n\n    if optimizer_index_tmp != self.current_optimizer_index:\n        Logger.info(\n            f\"Epoch: {self.epoch_index} -&gt; switching to optimizer: {self.optimizers[self.current_optimizer_index]}\"\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.save_optimizer","title":"save_optimizer","text":"<pre><code>save_optimizer(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def save_optimizer(\n    self,\n    *,\n    fn: str,\n):\n    with open(fn, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.step","title":"step","text":"<pre><code>step(closure=None)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def step(self, closure=None):\n    current_optimizer = self.optimizers[self.current_optimizer_index]\n    if self.epoch_index == 0 and self.batch_index == 0:\n        Logger.info(\n            f\"Epoch: {self.epoch_index} -&gt; using optimizer: {current_optimizer}\"\n        )\n    self.batch_index += 1\n\n    loss = current_optimizer.step(closure)\n\n    return loss\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.CompositeOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def zero_grad(\n    self,\n):\n    self.optimizers[self.current_optimizer_index].zero_grad()\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler","title":"OptimizerAssembler","text":"<pre><code>OptimizerAssembler()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def __init__(\n    self,\n):\n    self.assembled_optimizers = {}\n    self.composite_optimizer = None\n    self.optimizer_configurations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.assembled_optimizers","title":"assembled_optimizers  <code>instance-attribute</code>","text":"<pre><code>assembled_optimizers = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.composite_optimizer","title":"composite_optimizer  <code>instance-attribute</code>","text":"<pre><code>composite_optimizer = None\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.optimizer_configurations","title":"optimizer_configurations  <code>instance-attribute</code>","text":"<pre><code>optimizer_configurations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def __getitem__(\n    self,\n    item,\n):\n    if isinstance(item, str):\n        if item in self.assembled_optimizers.keys():\n            return self.assembled_optimizers[item]\n        else:\n            Logger.warning(\n                f'Unknown optimizer ID: \"{item}\", returning composite optimizer.'\n            )\n            return self.composite_optimizer\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.add_optimizer","title":"add_optimizer","text":"<pre><code>add_optimizer(\n    *,\n    optimizer_class: ENUM_Optimizers,\n    parameters: dict = {},\n    nepochs: int = 1,\n    name: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def add_optimizer(\n    self,\n    *,\n    optimizer_class: Optimizers,\n    parameters: dict = {},\n    nepochs: int = 1,\n    name: str,\n):\n    if len(self.optimizer_configurations) &gt; 0:\n        mpi.abort(\n            0,\n            f\"Multiple Optimizers in OptimizerAssembler are not currently supported\",\n        )\n\n    if nepochs &gt; 0:\n        self.composite_optimizer = None\n        self.optimizer_configurations.append(\n            (optimizer_class, parameters, nepochs, name)\n        )\n    else:\n        Logger.warning(\n            f'Optimizer \"{optimizer_class.value['name']}\" has not been added, # of active epochs is set to: {nepochs}'\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.assemble","title":"assemble","text":"<pre><code>assemble(params)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def assemble(\n    self,\n    params,\n):\n    self.assembled_optimizers = {}\n    epoch_activations = []\n    optimizers = []\n\n    params = list(params)\n    for (\n        optimizer_class,\n        parameters,\n        nepochs,\n        name,\n    ) in self.optimizer_configurations:\n\n        self.assembled_optimizers[name] = optimizer_wrapper(\n            optimizer_constructor=optimizer_class.value[\"class\"],\n            params=params,\n            kwargs=parameters,\n        )\n\n        epoch_activations.append(nepochs)\n        optimizers.append(self.assembled_optimizers[name])\n\n    self.composite_optimizer = CompositeOptimizer(\n        optimizers=optimizers,\n        epoch_activations=epoch_activations,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.clear_assembled_data","title":"clear_assembled_data","text":"<pre><code>clear_assembled_data()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def clear_assembled_data(\n    self,\n):\n    self.composite_optimizer = None\n    self.assembled_optimizers = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/optimizer_configuration/#AI4SurrogateModelling.src.training.optimizer.optimizer_configuration.OptimizerAssembler.get_optimizer_names","title":"get_optimizer_names","text":"<pre><code>get_optimizer_names()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/optimizer_configuration.py</code> <pre><code>def get_optimizer_names(\n    self,\n):\n    return list(self.assembled_optimizers.keys()) + [\"composite\"]\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/wrapper/","title":"Module <code>src.training.optimizer.wrapper</code>","text":""},{"location":"api/API%20Reference/src/training/optimizer/wrapper/#AI4SurrogateModelling.src.training.optimizer.wrapper","title":"AI4SurrogateModelling.src.training.optimizer.wrapper","text":""},{"location":"api/API%20Reference/src/training/optimizer/wrapper/#AI4SurrogateModelling.src.training.optimizer.wrapper.optimizer_wrapper","title":"optimizer_wrapper","text":"<pre><code>optimizer_wrapper(*, optimizer_constructor, params, kwargs)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/wrapper.py</code> <pre><code>def optimizer_wrapper(\n    *,\n    optimizer_constructor,\n    params,\n    kwargs,\n):\n    optimizer = ZeroRedundancyOptimizer(\n        params,\n        optimizer_class=optimizer_constructor,\n        **kwargs,\n    )\n\n    return optimizer\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/custom_optimizers/adam/","title":"Module <code>src.training.optimizer.custom_optimizers.adam</code>","text":""},{"location":"api/API%20Reference/src/training/optimizer/custom_optimizers/adam/#AI4SurrogateModelling.src.training.optimizer.custom_optimizers.adam","title":"AI4SurrogateModelling.src.training.optimizer.custom_optimizers.adam","text":""},{"location":"api/API%20Reference/src/training/optimizer/custom_optimizers/adam/#AI4SurrogateModelling.src.training.optimizer.custom_optimizers.adam.Adam","title":"Adam","text":"<pre><code>Adam(\n    params,\n    lr=0.0001,\n    betas=(0.9, 0.999),\n    eps=1e-08,\n    weight_decay=0,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>AI4SurrogateModelling/src/training/optimizer/custom_optimizers/adam.py</code> <pre><code>def __init__(\n    self,\n    params,\n    lr=1e-4,\n    betas=(0.9, 0.999),\n    eps=1e-08,\n    weight_decay=0,\n):\n\n    defaults = dict(\n        gamma=lr, betas=betas, eps=eps, weight_decay=weight_decay\n    )\n    super(Adam, self).__init__(params, defaults)\n    self.state[\"step\"] = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/optimizer/custom_optimizers/adam/#AI4SurrogateModelling.src.training.optimizer.custom_optimizers.adam.Adam.step","title":"step","text":"<pre><code>step(closure=None)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/optimizer/custom_optimizers/adam.py</code> <pre><code>def step(self, closure=None):\n    self.state[\"step\"] += 1\n    loss = None\n\n    if closure is not None:\n        loss = closure()\n\n    group_index = 0\n    for g in self.param_groups:\n        group_index += 1\n        # print('learning rate:', g['gamma'], 'betas:', g['betas'], 'eps:', g['eps'], 'weight decay:', g['weight_decay'])\n\n        param_index = 0\n        for p in g[\"params\"]:\n            param_index += 1\n\n            if p.grad is not None:\n                if len(self.state[p]) == 0:\n                    self.state[p][\"first_moment\"] = torch.zeros_like(p.data)\n                    self.state[p][\"second_moment\"] = torch.zeros_like(\n                        p.data\n                    )\n                print()\n\n                gt = p.grad.data\n                if g[\"weight_decay\"] != 0:\n                    gt += p.data * g[\"weight_decay\"]\n                    # gt = gt.add(g['weight_decay'], p.data)\n\n                beta1, beta2 = g[\"betas\"]\n                print(f\"Group: {group_index}, Parameter: {param_index}\")\n                print(f'Step: {self.state[\"step\"]}')\n                print(f'Epsilon: {g[\"eps\"]}')\n\n                print(f\"Beta1: {beta1}\")\n                print(f\"Beta2: {beta2}\")\n                print(f\"gt: {gt.min()} - {gt.max()}\")\n                first_moment = self.state[p][\"first_moment\"]\n                print(\n                    f\"first_moment: {first_moment.min()} - {first_moment.max()}\"\n                )\n                second_moment = self.state[p][\"second_moment\"]\n                print(\n                    f\"second_moment: {second_moment.min()} - {second_moment.max()}\"\n                )\n                expr1 = (1 - beta1) * gt\n                print(f\"(1 - beta1) * gt: {expr1.min()} - {expr1.max()}\")\n                expr2 = beta1 * first_moment\n                print(\n                    f\"first_moment * beta1: {expr2.min()} - {expr2.max()}\"\n                )\n\n                self.state[p][\"first_moment\"] = (\n                    first_moment * beta1 + (1 - beta1) * gt\n                )\n                self.state[p][\"second_moment\"] = second_moment * beta2 + (\n                    1 - beta2\n                ) * (gt * gt)\n                first_moment = self.state[p][\"first_moment\"]\n                print(\n                    f\"first_moment: {first_moment.min()} - {first_moment.max()}\"\n                )\n                second_moment = self.state[p][\"second_moment\"]\n                print(\n                    f\"second_moment: {second_moment.min()} - {second_moment.max()}\"\n                )\n\n                mt_hat = first_moment / (1 - beta1 ** self.state[\"step\"])\n                vt_hat = (\n                    second_moment / (1 - beta2 ** self.state[\"step\"])\n                ) ** 0.5\n                print(f\"mt_hat: {mt_hat.min()} - {mt_hat.max()}\")\n                print(f\"vt_hat: {vt_hat.min()} - {vt_hat.max()}\")\n\n                grad = g[\"gamma\"] * mt_hat / (g[\"eps\"] + vt_hat)\n                print(f\"grad: {grad.min()} - {grad.max()}\")\n\n                p.data -= grad\n                print(f\"data: {p.data.min()} - {p.data.max()}\")\n\n    return loss\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/","title":"Module <code>src.training.scheduler.scheduler_configuration</code>","text":""},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration","title":"AI4SurrogateModelling.src.training.scheduler.scheduler_configuration","text":""},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler","title":"CompositeScheduler","text":"<pre><code>CompositeScheduler(\n    *,\n    schedulers: list[_LRScheduler],\n    epoch_activations: list[int]\n)\n</code></pre> <p>               Bases: <code>_LRScheduler</code></p> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def __init__(\n    self,\n    *,\n    schedulers: list[optim.lr_scheduler._LRScheduler],\n    epoch_activations: list[int],\n):\n    if len(schedulers) == 0:\n        self.schedulers = None\n        self.current_scheduler_index = -1\n        self.lr_multipliers = [1]\n        return\n\n    self.schedulers = schedulers\n    self.epoch_index = 0\n    self.current_scheduler_index = 0\n\n    self.param_groups = schedulers[0].optimizer.param_groups\n    self.lr_multipliers = [1.0 for _ in self.param_groups]\n\n    epoch_activation_period = 0\n    self.epoch_activations = []\n    for epoch_activation in epoch_activations:\n        epoch_activation_period += epoch_activation\n        self.epoch_activations.append(epoch_activation_period)\n\n    super().__init__(optimizer=self.schedulers[0].optimizer)\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.current_scheduler_index","title":"current_scheduler_index  <code>instance-attribute</code>","text":"<pre><code>current_scheduler_index = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.epoch_activations","title":"epoch_activations  <code>instance-attribute</code>","text":"<pre><code>epoch_activations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.epoch_index","title":"epoch_index  <code>instance-attribute</code>","text":"<pre><code>epoch_index = 0\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.lr_multipliers","title":"lr_multipliers  <code>instance-attribute</code>","text":"<pre><code>lr_multipliers = [1.0 for _ in (param_groups)]\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.param_groups","title":"param_groups  <code>instance-attribute</code>","text":"<pre><code>param_groups = param_groups\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.schedulers","title":"schedulers  <code>instance-attribute</code>","text":"<pre><code>schedulers = schedulers\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.get_lr","title":"get_lr","text":"<pre><code>get_lr()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def get_lr(\n    self,\n):\n    if self.schedulers is None:\n        return [1]\n    return [\n        group[\"lr\"] * self.lr_multipliers[idx]\n        for idx, group in enumerate(self.param_groups)\n    ]\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.get_lr_multiplier","title":"get_lr_multiplier","text":"<pre><code>get_lr_multiplier()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def get_lr_multiplier(\n    self,\n):\n    return self.lr_multipliers[0]\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.load_scheduler","title":"load_scheduler  <code>staticmethod</code>","text":"<pre><code>load_scheduler(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>@staticmethod\ndef load_scheduler(\n    *,\n    fn: str,\n):\n    with open(fn, \"rb\") as f:\n        scheduler = dill.load(f)\n\n    return scheduler\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.save_scheduler","title":"save_scheduler","text":"<pre><code>save_scheduler(*, fn: str)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def save_scheduler(\n    self,\n    *,\n    fn: str,\n):\n    with open(fn, \"wb\") as f:\n        dill.dump(self, f)\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.CompositeScheduler.step","title":"step","text":"<pre><code>step(epoch=None)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def step(\n    self,\n    epoch=None,\n):\n    if self.schedulers is None:\n        return\n\n    current_scheduler = self.schedulers[self.current_scheduler_index]\n    if self.epoch_index == 0:\n        Logger.info(\n            f\"Epoch: {self.epoch_index} -&gt; using scheduler: {current_scheduler}\"\n        )\n\n    self.schedulers[self.current_scheduler_index].step(epoch)\n\n    self._next_epoch()\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler","title":"SchedulerAssembler","text":"<pre><code>SchedulerAssembler()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def __init__(\n    self,\n):\n    self.assembled_schedulers = {}\n    self.composite_scheduler = None\n    self.scheduler_configurations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.assembled_schedulers","title":"assembled_schedulers  <code>instance-attribute</code>","text":"<pre><code>assembled_schedulers = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.composite_scheduler","title":"composite_scheduler  <code>instance-attribute</code>","text":"<pre><code>composite_scheduler = None\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.scheduler_configurations","title":"scheduler_configurations  <code>instance-attribute</code>","text":"<pre><code>scheduler_configurations = []\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def __getitem__(\n    self,\n    item,\n):\n    if isinstance(item, str):\n        if item in self.assembled_schedulers.keys():\n            return self.assembled_schedulers[item]\n        else:\n            Logger.warning(\n                f'Unknown scheduler ID: \"{item}\", returning composite scheduler.'\n            )\n            return self.composite_scheduler\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.add_scheduler","title":"add_scheduler","text":"<pre><code>add_scheduler(\n    *,\n    scheduler_class: ENUM_Schedulers,\n    parameters: dict,\n    nepochs: int = 1,\n    name: str\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def add_scheduler(\n    self,\n    *,\n    scheduler_class: Schedulers,\n    parameters: dict,\n    nepochs: int = 1,\n    name: str,\n):\n    if len(self.scheduler_configurations) &gt; 0:\n        mpi.abort(\n            0,\n            f\"Multiple Schedulers in SchedulerAssembler are not currently supported\",\n        )\n\n    if nepochs &gt; 0:\n        self.composite_scheduler = None\n        self.scheduler_configurations.append(\n            (scheduler_class.value[\"class\"], parameters, nepochs, name)\n        )\n    else:\n        Logger.warning(\n            f'Scheduler \"{scheduler_class.value[\"name\"]}\" has not been added, # of active epochs is set to: {nepochs}'\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.assemble","title":"assemble","text":"<pre><code>assemble(optimizer)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def assemble(\n    self,\n    optimizer,\n):\n    self.assembled_schedulers = {}\n    epoch_activations = []\n    schedulers = []\n\n    for (\n        scheduler_class,\n        parameters,\n        nepochs,\n        name,\n    ) in self.scheduler_configurations:\n        parameters[\"optimizer\"] = optimizer\n        self.assembled_schedulers[name] = scheduler_class(**parameters)\n\n        epoch_activations.append(nepochs)\n        schedulers.append(self.assembled_schedulers[name])\n\n    self.composite_scheduler = CompositeScheduler(\n        schedulers=schedulers,\n        epoch_activations=epoch_activations,\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.clear_assembled_data","title":"clear_assembled_data","text":"<pre><code>clear_assembled_data()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def clear_assembled_data(\n    self,\n):\n    self.composite_scheduler = None\n    self.assembled_schedulers = {}\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.get_scheduler_names","title":"get_scheduler_names","text":"<pre><code>get_scheduler_names()\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def get_scheduler_names(\n    self,\n):\n    return list(self.assembled_schedulers.keys()) + [\"composite\"]\n</code></pre>"},{"location":"api/API%20Reference/src/training/scheduler/scheduler_configuration/#AI4SurrogateModelling.src.training.scheduler.scheduler_configuration.SchedulerAssembler.parse_scheduler_string","title":"parse_scheduler_string","text":"<pre><code>parse_scheduler_string(scheduler_string)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/training/scheduler/scheduler_configuration.py</code> <pre><code>def parse_scheduler_string(\n    self,\n    scheduler_string,\n):\n    if scheduler_string is None:\n        return None\n\n    code = scheduler_string.lower()\n    if code not in SchedulerAssembler.code_to_scheduler.keys():\n        mpi.abort(0, f'Unknown scheduler code: \"{code}\"')\n    return SchedulerAssembler.code_to_scheduler[code]\n</code></pre>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/","title":"Module <code>src.training.schedulers.lambdas.cyclical_lr</code>","text":""},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr","title":"AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr","text":"<p>Contains modules representing various lambda function based on cyclical behaviour.</p>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR","title":"CyclicalLR","text":"<pre><code>CyclicalLR(\n    *,\n    log_factor_low: float = -1,\n    log_factor_high: float = 1,\n    periods: list\n)\n</code></pre> <p>A basic cyclical learning rate scheduler utilizing user provided periods and learning rate ranges. Designed to be used by the LambdaLR scheduler provided by pytorch.</p> <p>Initializes the cyclical learning rate scheduler. The log10 learning  rate multiplier ranges between [log_factor_low, log_factor_high] with  nested cyclical behaviour with periods specified by the user.</p> <p>Parameters:</p> Name Type Description Default <code>periods</code> <code>list</code> <p>A list of integers specifying the periodical                behaviour of the scheduler.</p> required <code>log_factor_low</code> <code>float</code> <p>Minimal log10 learning rate                multiplier. Defaults to -1.</p> <code>-1</code> <code>log_factor_high</code> <code>float</code> <p>Maximal log10 learning rate                multiplier. Defaults to 1.</p> <code>1</code> Source code in <code>AI4SurrogateModelling/src/training/schedulers/lambdas/cyclical_lr.py</code> <pre><code>def __init__(\n    self,\n    *,\n    log_factor_low: float = -1,\n    log_factor_high: float = 1,\n    periods: list,\n):\n    \"\"\"Initializes the cyclical learning rate scheduler. The log10 learning \n    rate multiplier ranges between [log_factor_low, log_factor_high] with \n    nested cyclical behaviour with periods specified by the user.\n\n    Args:\n        periods (list): A list of integers specifying the periodical\\\n            behaviour of the scheduler.\n        log_factor_low (float, optional): Minimal log10 learning rate\\\n            multiplier. Defaults to -1.\n        log_factor_high (float, optional): Maximal log10 learning rate\\\n            multiplier. Defaults to 1.\n    \"\"\"\n    self.periods = periods\n    self.log_factor_low = log_factor_low / len(periods)\n    self.log_factor_high = log_factor_high / len(periods)\n</code></pre>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR.log_factor_high","title":"log_factor_high  <code>instance-attribute</code>","text":"<pre><code>log_factor_high = log_factor_high / len(periods)\n</code></pre>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR.log_factor_low","title":"log_factor_low  <code>instance-attribute</code>","text":"<pre><code>log_factor_low = log_factor_low / len(periods)\n</code></pre>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR.periods","title":"periods  <code>instance-attribute</code>","text":"<pre><code>periods = periods\n</code></pre>"},{"location":"api/API%20Reference/src/training/schedulers/lambdas/cyclical_lr/#AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR.__call__","title":"__call__","text":"<pre><code>__call__(step: int) -&gt; float\n</code></pre> <p>Given the training step, calculates the cyclical learning rate  multiplier based on the internal multiplier range and periodical behaviour.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Index of the epoch.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Learning rate multiplier in range                10**[log_factor_low, log_factor_high]</p> Source code in <code>AI4SurrogateModelling/src/training/schedulers/lambdas/cyclical_lr.py</code> <pre><code>def __call__(self, step: int) -&gt; float:\n    \"\"\"Given the training step, calculates the cyclical learning rate \n    multiplier based on the internal multiplier range and periodical behaviour.\n\n    Args:\n        step (int): Index of the epoch.\n\n    Returns:\n        float: Learning rate multiplier in range\\\n            10**[log_factor_low, log_factor_high]\n    \"\"\"\n    lr_scale = 1\n    for period in self.periods:\n        alpha = (\n            np.cos(step / period * 2 * np.pi) + 1\n        ) * 0.5  # in range [0, 1]\n        log_factor = (\n            self.log_factor_low * alpha + (1 - alpha) * self.log_factor_high\n        )\n        lr_scale *= 10**log_factor\n\n    return lr_scale\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainers/feed_forward/","title":"Module <code>src.training.trainers.feed_forward</code>","text":""},{"location":"api/API%20Reference/src/training/trainers/feed_forward/#AI4SurrogateModelling.src.training.trainers.feed_forward","title":"AI4SurrogateModelling.src.training.trainers.feed_forward","text":"<p>Contains modules and functions used for training simple feedforward networks.</p>"},{"location":"api/API%20Reference/src/training/trainers/feed_forward/#AI4SurrogateModelling.src.training.trainers.feed_forward.get_model_properties","title":"get_model_properties","text":"<pre><code>get_model_properties(\n    model: ModelBase,\n    loss: LossBase,\n    database,\n    dataloader_train,\n    dataloader_test,\n    dataloader_validation,\n) -&gt; dict\n</code></pre> <p>Collects diagnostics describing the trained model.</p> <p>Builds a dictionary that contains L1-error histograms, cumulative error curves, weight/bias distributions and gradient-based sensitivity matrices. All dataloaders are evaluated in <code>eval()</code> mode, so no gradients are tracked.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelBase</code> <p>Fully initialised model to analyse.</p> required <code>loss</code> <code>LossBase</code> <p>Loss used during training (for matching outputs to targets).</p> required <code>database</code> <code>DatabaseTabular</code> <p>Database object used for label/metadata lookups.</p> required <code>dataloader_train</code> <code>DictionaryDataLoader</code> <p>Training dataloader providing reference samples.</p> required <code>dataloader_test</code> <code>DictionaryDataLoader</code> <p>Test dataloader providing reference samples.</p> required <code>dataloader_validation</code> <code>DictionaryDataLoader</code> <p>Validation dataloader providing reference samples.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Nested data structure describing distributions/sensitivities.</p> Source code in <code>AI4SurrogateModelling/src/training/trainers/feed_forward.py</code> <pre><code>def get_model_properties(\n    model: ModelBase,\n    loss: LossBase,\n    database,\n    dataloader_train,\n    dataloader_test,\n    dataloader_validation,\n) -&gt; dict:\n    \"\"\"Collects diagnostics describing the trained model.\n\n    Builds a dictionary that contains L1-error histograms, cumulative error\n    curves, weight/bias distributions and gradient-based sensitivity matrices.\n    All dataloaders are evaluated in `eval()` mode, so no gradients are tracked.\n\n    Args:\n        model (ModelBase): Fully initialised model to analyse.\n        loss (LossBase): Loss used during training (for matching outputs to targets).\n        database (DatabaseTabular): Database object used for label/metadata lookups.\n        dataloader_train (DictionaryDataLoader): Training dataloader providing reference samples.\n        dataloader_test (DictionaryDataLoader): Test dataloader providing reference samples.\n        dataloader_validation (DictionaryDataLoader): Validation dataloader providing reference samples.\n\n    Returns:\n        dict: Nested data structure describing distributions/sensitivities.\n    \"\"\"\n    model.eval()\n    dtype = dataloader_train.dtype\n    keys = {}\n    labels = database.metadata_get_value(\"labels\")\n    nbins = 2 * 3 * 5 * 7 * 11\n    nbins_cummulative = 200\n    nsamples_random = 10000\n    nsample_sobol = 2**12\n    nlevels_morris = 4\n    nsamples_morris = 2**12\n    dataloader_random = database.get_dataloader_random(\n        n=nsamples_random,\n        batch_size=dataloader_train.batch_size,\n        dtype=dtype,\n    )\n    admissible_ranges = {\n        k: torch.tensor(v, dtype=dtype)\n        for k, v in database.get_data_ranges().items()\n    }\n\n    # metadata retrieval\n    for data in dataloader_train:\n        predictions = model(data_in=data)\n        prediction_keys = sorted(list(predictions.keys()))\n\n        for key_tgt in prediction_keys:\n            if key_tgt in loss.coefficients[\"outputs\"]:\n                keys_src = sorted(\n                    list(loss.coefficients[\"outputs\"][key_tgt].keys())\n                )\n                keys[key_tgt] = keys_src[0]\n        break\n\n    ####################### error distribution wrt the outputs #################\n    data_train = {}\n    data_test = {}\n    data_validation = {}\n    for data in dataloader_train:\n        predictions = model(data_in=data)\n\n        for prediction_key, expected_key in keys.items():\n            l1_loss = torch.abs(\n                data[expected_key] - predictions[prediction_key]\n            )\n\n            for idx, expected_label in enumerate(labels[expected_key]):\n                if expected_label not in data_train:\n                    data_train[expected_label] = []\n                    data_test[expected_label] = None\n                    data_validation[expected_label] = None\n                data_train[expected_label] += (\n                    l1_loss[:, idx].detach().numpy().flatten().tolist()\n                )\n\n    for data in dataloader_test:\n        predictions = model(data_in=data)\n\n        for prediction_key, expected_key in keys.items():\n            l1_loss = torch.abs(\n                data[expected_key] - predictions[prediction_key]\n            )\n\n            for idx, expected_label in enumerate(labels[expected_key]):\n                if data_test[expected_label] is None:\n                    data_test[expected_label] = []\n                data_test[expected_label] += (\n                    l1_loss[:, idx].detach().numpy().flatten().tolist()\n                )\n\n    for data in dataloader_validation:\n        predictions = model(data_in=data)\n\n        for prediction_key, expected_key in keys.items():\n            l1_loss = torch.abs(\n                data[expected_key] - predictions[prediction_key]\n            )\n\n            for idx, expected_label in enumerate(labels[expected_key]):\n                if data_validation[expected_label] is None:\n                    data_validation[expected_label] = []\n                data_validation[expected_label] += (\n                    l1_loss[:, idx].detach().numpy().flatten().tolist()\n                )\n\n    data_L1_distribution = {}\n    l1_error_distribution_min = {\n        expected_label: np.min(\n            [\n                np.min(data_train[expected_label] or float(\"inf\")),\n                np.min(data_test[expected_label] or float(\"inf\")),\n                np.min(data_validation[expected_label] or float(\"inf\")),\n            ]\n        )\n        for _, expected_key in keys.items()\n        for expected_label in labels[expected_key]\n    }\n    l1_error_distribution_max = {\n        expected_label: np.max(\n            [\n                np.max(data_train[expected_label] or -float(\"inf\")),\n                np.max(data_test[expected_label] or -float(\"inf\")),\n                np.max(data_validation[expected_label] or -float(\"inf\")),\n            ]\n        )\n        for _, expected_key in keys.items()\n        for expected_label in labels[expected_key]\n    }\n\n    for key in sorted(list(data_train.keys())):\n        data_L1_distribution[key] = {\n            \"histogram_plots\": {\n                \"title\": f'L1 Error distribution for output \"{key}\"',\n                \"xlabel\": \"L1 error\",\n                \"ylabel\": \"Portion [%]\",\n                \"data\": {},\n            },\n        }\n\n        if key in data_train:\n            data_ = np.array(data_train[key])\n            x_train, w_train = PT.calculate_histograms(\n                predefined_min=l1_error_distribution_min[key],\n                predefined_max=l1_error_distribution_max[key],\n                data=data_,\n                nbins=nbins,\n            )\n            x_train = [float(v) for v in x_train]\n            w_train = [float(v) for v in w_train]\n            data_L1_distribution[key][\"histogram_plots\"][\"data\"][\"Train\"] = {\n                \"x\": x_train,\n                \"w\": w_train,\n            }\n\n        if key in data_test:\n            if data_test[key] is None:\n                continue\n\n            data_ = np.array(data_test[key])\n            x_test, w_test = PT.calculate_histograms(\n                predefined_min=l1_error_distribution_min[key],\n                predefined_max=l1_error_distribution_max[key],\n                data=data_,\n                nbins=nbins,\n            )\n            x_test = [float(v) for v in x_test]\n            w_test = [float(v) for v in w_test]\n            data_L1_distribution[key][\"histogram_plots\"][\"data\"][\"Test\"] = {\n                \"x\": x_test,\n                \"w\": w_test,\n            }\n\n        if key in data_validation:\n            if data_validation[key] is None:\n                continue\n            data_ = np.array(data_validation[key])\n            x_validation, w_validation = PT.calculate_histograms(\n                predefined_min=l1_error_distribution_min[key],\n                predefined_max=l1_error_distribution_max[key],\n                data=data_,\n                nbins=nbins,\n            )\n            x_validation = [float(v) for v in x_validation]\n            w_validation = [float(v) for v in w_validation]\n            data_L1_distribution[key][\"histogram_plots\"][\"data\"][\n                \"Validation\"\n            ] = {\"x\": x_validation, \"w\": w_validation}\n\n    ####################### data_cummulative_error_curve #################\n    data_cummulative_error_curve = {}\n    for key in sorted(list(data_train.keys())):\n        data_cummulative_error_curve[key] = {\n            \"line_plots\": {\n                \"title\": f'Cummulative L1 error distribution for output \"{key}\"',\n                \"xlabel\": \"L1 error\",\n                \"ylabel\": \"Portion [%]\",\n                \"data\": {},\n            },\n        }\n\n        if key in data_train:\n            if data_train[key] is None:\n                continue\n\n            data_ = np.array(data_train[key]).flatten()\n            x_train, y_train = PT.calculate_cummulative_distribution(\n                data=data_,\n                nbins=nbins_cummulative,\n            )\n            x_train = [float(v) for v in x_train]\n            y_train = [float(v) for v in y_train]\n            data_cummulative_error_curve[key][\"line_plots\"][\"data\"][\"Train\"] = {\n                \"x\": x_train,\n                \"y\": y_train,\n            }\n\n        if key in data_test:\n            if data_test[key] is None:\n                continue\n\n            data_ = np.array(data_test[key]).flatten()\n            x_test, y_test = PT.calculate_cummulative_distribution(\n                data=data_,\n                nbins=nbins_cummulative,\n            )\n            x_test = [float(v) for v in x_test]\n            y_test = [float(v) for v in y_test]\n            data_cummulative_error_curve[key][\"line_plots\"][\"data\"][\"Test\"] = {\n                \"x\": x_test,\n                \"y\": y_test,\n            }\n\n        if key in data_validation:\n            if data_validation[key] is None:\n                continue\n\n            data_ = np.array(data_validation[key]).flatten()\n            x_validation, y_validation = PT.calculate_cummulative_distribution(\n                data=data_,\n                nbins=nbins_cummulative,\n            )\n            x_validation = [float(v) for v in x_validation]\n            y_validation = [float(v) for v in y_validation]\n            data_cummulative_error_curve[key][\"line_plots\"][\"data\"][\n                \"Validation\"\n            ] = {\"x\": x_validation, \"y\": y_validation}\n\n    ######################### Weights &amp; Biases ###################\n    data_weights_and_biases = {\n        \"histogram_plots\": {\n            \"title\": \"Distribution of Model's Weights &amp; Biases\",\n            \"xlabel\": \"Value\",\n            \"ylabel\": \"Portion [%]\",\n            \"data\": {},\n        },\n    }\n    weights_biases = model.state_dict()\n    weights_distribution_min = float(\"inf\")\n    weights_distribution_max = -weights_distribution_min\n    for _, v in weights_biases.items():\n        weights_distribution_min = np.min(\n            [weights_distribution_min, np.min(v.detach().numpy().flatten())]\n        )\n        weights_distribution_max = np.max(\n            [weights_distribution_max, np.max(v.detach().numpy().flatten())]\n        )\n\n    for k, v in weights_biases.items():\n        x, w = PT.calculate_histograms(\n            predefined_min=weights_distribution_min,\n            predefined_max=weights_distribution_max,\n            data=v.detach().numpy().flatten(),\n            nbins=nbins,\n        )\n        x = [float(v) for v in x]\n        w = [float(v) for v in w]\n        data_weights_and_biases[\"histogram_plots\"][\"data\"][k] = {\"x\": x, \"w\": w}\n\n    ####################### Sensitivity Analysis #################\n    sensitivity_sobol, row_labels_sobol, col_labels_sobol = sobol_sensitivity(\n        model=model,\n        labels=labels,\n        bounds=admissible_ranges,\n        nsamples=nsample_sobol,\n        dtype=dtype,\n    )\n    # sensitivity_morris, row_labels_morris, col_labels_morris = (\n    #     morris_sensitivity(\n    #         model=model,\n    #         labels=labels,\n    #         bounds=admissible_ranges,\n    #         nsamples=nsamples_morris,\n    #         dtype=dtype,\n    #         nlevels=nlevels_morris,\n    #     )\n    # )\n\n    sensitivity_grad_train, row_labels, col_labels = gradient_sensitivity(\n        dataloader=dataloader_train, model=model, labels=labels\n    )\n    sensitivity_grad_test, _, _ = gradient_sensitivity(\n        dataloader=dataloader_test, model=model, labels=labels\n    )\n    sensitivity_grad_validation, _, _ = gradient_sensitivity(\n        dataloader=dataloader_validation, model=model, labels=labels\n    )\n    sensitivity_grad_random, _, _ = gradient_sensitivity(\n        dataloader=dataloader_random, model=model, labels=labels\n    )\n    sensitivity_grad_data = None\n    data_sensitivity_analysis_gradients = None\n    if mpi.get_rank() == 0:\n\n        labels_sa_columns = []\n        sensitivity_grad_data = [[] for _ in range(len(row_labels))]\n        for col_idx in range(len(col_labels)):\n            if len(sensitivity_grad_train) &gt; 0:\n                labels_sa_columns.append(f\"{col_labels[col_idx]} (Train)\")\n                for row_idx in range(len(row_labels)):\n                    # print('TRAIN', sensitivity_grad_train[row_idx][col_idx])\n                    sensitivity_grad_data[row_idx] += [\n                        sensitivity_grad_train[row_idx][col_idx]\n                    ]\n\n            if len(sensitivity_grad_test) &gt; 0:\n                labels_sa_columns.append(f\"{col_labels[col_idx]} (Test)\")\n                for row_idx in range(len(row_labels)):\n                    # print('TEST', sensitivity_grad_test[row_idx][col_idx])\n                    sensitivity_grad_data[row_idx] += [\n                        sensitivity_grad_test[row_idx][col_idx]\n                    ]\n\n            if len(sensitivity_grad_validation) &gt; 0:\n                labels_sa_columns.append(f\"{col_labels[col_idx]} (Validation)\")\n                for row_idx in range(len(row_labels)):\n                    sensitivity_grad_data[row_idx] += [\n                        sensitivity_grad_validation[row_idx][col_idx]\n                    ]\n\n            if len(sensitivity_grad_random) &gt; 0:\n                labels_sa_columns.append(f\"{col_labels[col_idx]} (Random)\")\n                for row_idx in range(len(row_labels)):\n                    sensitivity_grad_data[row_idx] += [\n                        sensitivity_grad_random[row_idx][col_idx]\n                    ]\n\n        data_sensitivity_analysis_gradients = {\n            \"annotated_matrix\": {\n                \"title\": \"Local gradient based Sensitivity Analysis\",\n                \"labels_vertical\": row_labels,\n                \"labels_horizontal\": labels_sa_columns,\n                \"data\": sensitivity_grad_data,\n                \"decimals\": 3,\n                \"colorscale\": \"Viridis\",\n            },\n        }\n        data_sensitivity_analysis_sobol = {\n            \"annotated_matrix\": {\n                \"title\": \"Sensitivity Analysis (Sobol indices of degree 1)\",\n                \"labels_vertical\": row_labels_sobol,\n                \"labels_horizontal\": col_labels_sobol,\n                \"data\": sensitivity_sobol,\n                \"decimals\": 3,\n                \"colorscale\": \"Viridis\",\n            },\n        }\n        # data_sensitivity_analysis_morris = {\n        #     \"annotated_matrix\": {\n        #         \"title\": \"Sensitivity Analysis (Morris type)\",\n        #         \"labels_vertical\": row_labels_morris,\n        #         \"labels_horizontal\": col_labels_morris,\n        #         \"data\": sensitivity_morris,\n        #         \"decimals\": 3,\n        #         \"colorscale\": \"Viridis\",\n        #     },\n        # }\n\n    out = {\n        \"L1 error distribution\": data_L1_distribution,  # train, test, validation\n        \"Cummulative error curve\": data_cummulative_error_curve,  # train, test, validation\n        \"Weights &amp; Biases\": data_weights_and_biases,\n        \"Sensitivity Analysis/Gradients\": data_sensitivity_analysis_gradients,  # train, test, validation, random\n        # \"Sensitivity Analysis/Morris\": data_sensitivity_analysis_morris,  # train, test, validation, random\n        \"Sensitivity Analysis/Sobol-1\": data_sensitivity_analysis_sobol,  # train, test, validation, random\n    }\n\n    return out\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainers/feed_forward/#AI4SurrogateModelling.src.training.trainers.feed_forward.one_test_run","title":"one_test_run","text":"<pre><code>one_test_run(\n    *,\n    number_of_epochs: int,\n    dataloader_train: DictionaryDataLoader,\n    dataloader_test: DictionaryDataLoader,\n    dataloader_validation: DictionaryDataLoader,\n    optimizer: Optimizer,\n    scheduler,\n    model: ModelBase,\n    loss: LossBase,\n    train_manager: TrainingManager\n) -&gt; None\n</code></pre> <p>Runs a full training loop with metric logging and dashboards.</p> <p>Parameters:</p> Name Type Description Default <code>number_of_epochs</code> <code>int</code> <p>Number of epochs to train.</p> required <code>dataloader_train</code> <code>DictionaryDataLoader</code> <p>Training dataloader.</p> required <code>dataloader_test</code> <code>DictionaryDataLoader</code> <p>Test dataloader.</p> required <code>dataloader_validation</code> <code>DictionaryDataLoader</code> <p>Validation dataloader.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer applied to model parameters.</p> required <code>scheduler</code> <code>_LRScheduler</code> <p>Scheduler advanced after each epoch/batch sequence.</p> required <code>model</code> <code>ModelBase</code> <p>Model instance to train.</p> required <code>loss</code> <code>LossBase</code> <p>Loss instance used for optimisation and metric aggregation.</p> required <code>train_manager</code> <code>TrainingManager</code> <p>Persistent state tracking checkpoints and histories.</p> required Source code in <code>AI4SurrogateModelling/src/training/trainers/feed_forward.py</code> <pre><code>def one_test_run(\n    *,\n    number_of_epochs: int,\n    dataloader_train: DictionaryDataLoader,\n    dataloader_test: DictionaryDataLoader,\n    dataloader_validation: DictionaryDataLoader,\n    optimizer: torch.optim.Optimizer,\n    scheduler,\n    model: ModelBase,\n    loss: LossBase,\n    train_manager: TrainingManager,\n) -&gt; None:\n    \"\"\"Runs a full training loop with metric logging and dashboards.\n\n    Args:\n        number_of_epochs (int): Number of epochs to train.\n        dataloader_train (DictionaryDataLoader): Training dataloader.\n        dataloader_test (DictionaryDataLoader): Test dataloader.\n        dataloader_validation (DictionaryDataLoader): Validation dataloader.\n        optimizer (torch.optim.Optimizer): Optimizer applied to model parameters.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Scheduler advanced after each epoch/batch sequence.\n        model (ModelBase): Model instance to train.\n        loss (LossBase): Loss instance used for optimisation and metric aggregation.\n        train_manager (TrainingManager): Persistent state tracking checkpoints and histories.\n    \"\"\"\n    if number_of_epochs &lt;= 0:\n        return\n\n    Dashboard = TrainingDashboard()\n\n    one_run_progress = HistoryProgress()\n\n    output_frequency = 0.25\n\n    TimeMonitor.start(\"training-output\")\n    TimeMonitor.start(\"training-total\")\n    tcompute = train_manager.get_training_time()\n    epoch_index = train_manager.get_training_epoch()\n    for epoch in range(number_of_epochs):\n\n        TimeMonitor.start(\"training-compute\")\n        criterion_total_train, loss_total_train, individual_losses_train = (\n            _evaluate_batches(\n                dataloader=dataloader_train,\n                model=model,\n                optimizer=optimizer,\n                scheduler=scheduler,\n                loss=loss,\n            )\n        )\n        tcompute += TimeMonitor.get(\"training-compute\")[\"wall\"]\n        epoch_index += 1\n\n        TimeMonitor.start(\"training-compute\")\n        criterion_total_test, loss_total_test, individual_losses_test = (\n            _evaluate_batches(\n                dataloader=dataloader_test,\n                model=model,\n                loss=loss,\n            )\n        )\n        tcompute += TimeMonitor.get(\"training-compute\")[\"wall\"]\n\n        if len(dataloader_train) &gt; 0:\n            one_run_progress.add(\n                f\"criterion/train\", criterion_total_train, log=True\n            )\n            one_run_progress.add(\n                f\"loss-scaled/train\", loss_total_train, log=True\n            )\n            one_run_progress.add_transpose_last(\n                f\"loss-raw/train\", individual_losses_train, log=True\n            )\n\n        if len(dataloader_test) &gt; 0:\n            one_run_progress.add(\n                f\"criterion/test\", criterion_total_test, log=True\n            )\n            one_run_progress.add(f\"loss-scaled/test\", loss_total_test, log=True)\n            one_run_progress.add_transpose_last(\n                f\"loss-raw/test\", individual_losses_test, log=True\n            )\n\n        one_run_progress.add(\n            f\"learning-rate/learning_rate\", scheduler.get_last_lr()[0]\n        )\n        one_run_progress.add(f\"time\", tcompute)\n        one_run_progress.add(f\"epoch\", epoch_index)\n\n        train_manager.update_model_best(\n            model=model,\n            criterion=criterion_total_test.item(),\n        )\n\n        time_wall = TimeMonitor.get(\"training-output\")[\"wall\"]\n        if (\n            time_wall &gt;= output_frequency\n            or epoch == 0\n            or epoch == number_of_epochs - 1\n        ):\n            TimeMonitor.increment(\"training-output\", output_frequency)\n            (\n                criterion_total_validation,\n                loss_total_validation,\n                individual_losses_validation,\n            ) = _evaluate_batches(\n                dataloader=dataloader_validation,\n                model=model,\n                loss=loss,\n            )\n            if len(dataloader_validation) &gt; 0:\n                one_run_progress.add(\n                    f\"criterion/validation\",\n                    criterion_total_validation,\n                    log=True,\n                )\n                one_run_progress.add(\n                    f\"loss-scaled/validation\", loss_total_validation, log=True\n                )\n                one_run_progress.add_transpose_last(\n                    f\"loss-raw/validation\",\n                    individual_losses_validation,\n                    log=True,\n                )\n            Dashboard.set_cursor()\n\n            learning_rate = float(scheduler.get_last_lr()[0])\n            time_per_epoch = TimeMonitor.get(\"training-total\")[\"wall\"] / (\n                epoch + 1\n            )\n            progress_value = (epoch + 1) / number_of_epochs\n            time_remaining = time_per_epoch * (number_of_epochs - epoch - 1)\n\n            # one_run_progress.add(f'learning_rate', learning_rate)\n            # one_run_progress.add(f'time', TimeMonitor.get('training-total')['wall'])\n\n            Dashboard.refresh(\n                progress=progress_value,\n                time_per_epoch=time_per_epoch,\n                learning_rate=learning_rate,\n                time_remaining=time_remaining,\n                criterion_train=criterion_total_train,\n                criterion_test=criterion_total_test,\n                criterion_validation=criterion_total_validation,\n                loss_train=loss_total_train,\n                loss_test=loss_total_test,\n                loss_validation=loss_total_validation,\n                losses_train=individual_losses_train,\n                losses_test=individual_losses_test,\n                losses_validation=individual_losses_validation,\n            )\n\n    # TrainerManager.add_epoch(checkpoint_dir)\n    Dashboard.release_cursor()\n    # Logger.warning(f'YYY: {unparse_model(model).get_parameters()[:10]}')\n\n    train_manager.update_training_progress(\n        progress=one_run_progress,\n    )\n    train_manager.update_model_latest(model=model)\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainers/feed_forward/#AI4SurrogateModelling.src.training.trainers.feed_forward.refine","title":"refine","text":"<pre><code>refine(\n    *,\n    report_dir: str = None,\n    checkpoint_dir: str,\n    number_of_epochs: int,\n    batch_size: int,\n    dtype: dtype,\n    data_uid: str,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str,\n    reset: bool = False\n) -&gt; tuple\n</code></pre> <p>Constructs training components, runs one training cycle, and reports.</p> <p>Because the optimizer depends on model parameters and the scheduler depends on the optimizer, this function is marked lazy\u2014runtime state instantiates the components only when the function executes.</p> <p>Parameters:</p> Name Type Description Default <code>report_dir</code> <code>str | None</code> <p>Directory where the HTML training report is stored.</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>Directory that stores checkpoints/progress metadata.</p> required <code>number_of_epochs</code> <code>int</code> <p>Number of epochs to train in the single run.</p> required <code>batch_size</code> <code>int</code> <p>Batch size used by the dataloaders.</p> required <code>dtype</code> <code>dtype</code> <p>Torch dtype for model parameters and datasets.</p> required <code>data_uid</code> <code>str</code> <p>Runtime-state identifier for the database object.</p> required <code>model_uid</code> <code>str</code> <p>Identifier for the model to instantiate.</p> required <code>optimizer_uid</code> <code>str</code> <p>Identifier for the optimizer to instantiate.</p> required <code>scheduler_uid</code> <code>str</code> <p>Identifier for the scheduler to instantiate.</p> required <code>loss_uid</code> <code>str</code> <p>Identifier for the loss function.</p> required <code>reset</code> <code>bool</code> <p>If True, wipes existing checkpoints before training.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p><code>(criterion, best_model_train, best_model_validation)</code> as</p> <code>tuple</code> <p>produced by downstream routines.</p> Source code in <code>AI4SurrogateModelling/src/training/trainers/feed_forward.py</code> <pre><code>@mark_lazy_function\ndef refine(\n    *,\n    report_dir: str = None,\n    checkpoint_dir: str,\n    number_of_epochs: int,\n    batch_size: int,\n    dtype: torch.dtype,\n    data_uid: str,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str,\n    reset: bool = False,\n) -&gt; tuple:\n    \"\"\"Constructs training components, runs one training cycle, and reports.\n\n    Because the optimizer depends on model parameters and the scheduler depends\n    on the optimizer, this function is marked lazy\u2014runtime state instantiates\n    the components only when the function executes.\n\n    Args:\n        report_dir (str | None): Directory where the HTML training report is stored.\n        checkpoint_dir (str): Directory that stores checkpoints/progress metadata.\n        number_of_epochs (int): Number of epochs to train in the single run.\n        batch_size (int): Batch size used by the dataloaders.\n        dtype (torch.dtype): Torch dtype for model parameters and datasets.\n        data_uid (str): Runtime-state identifier for the database object.\n        model_uid (str): Identifier for the model to instantiate.\n        optimizer_uid (str): Identifier for the optimizer to instantiate.\n        scheduler_uid (str): Identifier for the scheduler to instantiate.\n        loss_uid (str): Identifier for the loss function.\n        reset (bool): If True, wipes existing checkpoints before training.\n\n    Returns:\n        tuple: `(criterion, best_model_train, best_model_validation)` as\n        produced by downstream routines.\n    \"\"\"\n    loss = runtime_state.construct_object(object_uid=loss_uid)\n\n    # lets initialize the database and load the dataloaders\n    database = runtime_state.construct_object(object_uid=data_uid)\n\n    data_require_gradient = loss.what_requires_gradients()\n    dataloader_train, dataloader_test, dataloader_validation = (\n        database.get_dataloaders(\n            batch_size=batch_size,\n            dtype=dtype,\n            data_require_gradient=data_require_gradient,\n        )\n    )\n\n    train_manager = TrainingManager(checkpoint_dir=checkpoint_dir, reset=reset)\n    model = train_manager.load_model_latest()\n\n    if model == None:\n        model = runtime_state.construct_object(\n            object_uid=model_uid,\n            reinit=True,\n        )\n\n    optimizer = runtime_state.construct_object(\n        object_uid=optimizer_uid,\n        reinit=True,\n        override_parameters={\"params\": model.parameters()},\n    )\n    scheduler = runtime_state.construct_object(\n        object_uid=scheduler_uid,\n        reinit=True,\n        override_parameters={\"optimizer\": optimizer},\n    )\n\n    one_test_run(\n        dataloader_train=dataloader_train,\n        dataloader_test=dataloader_test,\n        dataloader_validation=dataloader_validation,\n        loss=loss,\n        model=model,\n        number_of_epochs=number_of_epochs,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        train_manager=train_manager,\n    )\n\n    report_data = {\n        \"Training\": train_manager.get_progress_data(\n            xkey=\"epoch\"\n        ),  # losses, learning rates\n        \"Model\": get_model_properties(  # sensitivity analysis (histograms), error distribution wrt the outputs, final error cummulative distribution (curve)\n            model=model,\n            loss=loss,\n            database=database,\n            dataloader_train=dataloader_train,\n            dataloader_test=dataloader_test,\n            dataloader_validation=dataloader_validation,\n        ),\n    }\n\n    if report_dir is not None:\n        mpi.make_dir(report_dir)\n        create_report(fn=report_dir + \"/\" + \"training.html\", data=[report_data])\n</code></pre>"},{"location":"api/API%20Reference/src/training/trainers/feed_forward/#AI4SurrogateModelling.src.training.trainers.feed_forward.simple","title":"simple","text":"<pre><code>simple(\n    *,\n    checkpoint_dir: str,\n    number_of_inner_tests: int = 1,\n    number_of_epochs: int,\n    batch_size: int,\n    dtype: dtype,\n    data_uid: str,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str\n) -&gt; tuple\n</code></pre> <p>Runs one or more fresh training sessions for statistical evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory used to store checkpoints and histories.</p> required <code>number_of_inner_tests</code> <code>int</code> <p>How many fresh trainings to launch sequentially.</p> <code>1</code> <code>number_of_epochs</code> <code>int</code> <p>Number of epochs per training run.</p> required <code>batch_size</code> <code>int</code> <p>Size of batches fed to dataloaders.</p> required <code>dtype</code> <code>dtype</code> <p>Torch dtype used when creating dataloaders/models.</p> required <code>data_uid</code> <code>str</code> <p>Runtime-state identifier for the database to construct loaders.</p> required <code>model_uid</code> <code>str</code> <p>Identifier for the model factory.</p> required <code>optimizer_uid</code> <code>str</code> <p>Identifier for the optimizer factory.</p> required <code>scheduler_uid</code> <code>str</code> <p>Identifier for the scheduler factory.</p> required <code>loss_uid</code> <code>str</code> <p>Identifier for the loss function to instantiate.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>Whatever <code>runtime_state.construct_object</code> returns for downstream</p> <code>tuple</code> <p>consumption (historically criterion + model references).</p> Source code in <code>AI4SurrogateModelling/src/training/trainers/feed_forward.py</code> <pre><code>@mark_lazy_function\ndef simple(\n    *,\n    checkpoint_dir: str,\n    number_of_inner_tests: int = 1,\n    number_of_epochs: int,\n    batch_size: int,\n    dtype: torch.dtype,\n    data_uid: str,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str,\n) -&gt; tuple:\n    \"\"\"Runs one or more fresh training sessions for statistical evaluation.\n\n    Args:\n        checkpoint_dir (str): Directory used to store checkpoints and histories.\n        number_of_inner_tests (int): How many fresh trainings to launch sequentially.\n        number_of_epochs (int): Number of epochs per training run.\n        batch_size (int): Size of batches fed to dataloaders.\n        dtype (torch.dtype): Torch dtype used when creating dataloaders/models.\n        data_uid (str): Runtime-state identifier for the database to construct loaders.\n        model_uid (str): Identifier for the model factory.\n        optimizer_uid (str): Identifier for the optimizer factory.\n        scheduler_uid (str): Identifier for the scheduler factory.\n        loss_uid (str): Identifier for the loss function to instantiate.\n\n    Returns:\n        tuple: Whatever `runtime_state.construct_object` returns for downstream\n        consumption (historically criterion + model references).\n    \"\"\"\n    loss = runtime_state.construct_object(object_uid=loss_uid)\n\n    # lets initialize the database and load the dataloaders\n    database = runtime_state.construct_object(object_uid=data_uid)\n\n    data_require_gradient = loss.what_requires_gradients()\n    dataloader_train, dataloader_test, dataloader_validation = (\n        database.get_dataloaders(\n            batch_size=batch_size,\n            dtype=dtype,\n            data_require_gradient=data_require_gradient,\n        )\n    )\n\n    train_manager = TrainingManager(checkpoint_dir=checkpoint_dir)\n\n    for inner_test in range(number_of_inner_tests):\n        model = runtime_state.construct_object(\n            object_uid=model_uid,\n        )\n        # Logger.warning(model)\n        # continue\n        optimizer = runtime_state.construct_object(\n            object_uid=optimizer_uid,\n            override_parameters={\"params\": model.parameters()},\n        )\n        scheduler = runtime_state.construct_object(\n            object_uid=scheduler_uid,\n            override_parameters={\"optimizer\": optimizer},\n        )\n\n        one_test_run(\n            dataloader_train=dataloader_train,\n            dataloader_test=dataloader_test,\n            dataloader_validation=dataloader_validation,\n            loss=loss,\n            model=model,\n            number_of_epochs=number_of_epochs,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            train_manager=train_manager,\n        )\n</code></pre>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/gradient_sampler/","title":"Module <code>src.utilization.parameter_estimation.gradient_sampler</code>","text":""},{"location":"api/API%20Reference/src/utilization/parameter_estimation/gradient_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler","title":"AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler","text":""},{"location":"api/API%20Reference/src/utilization/parameter_estimation/gradient_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler.GradientParameterSampler","title":"GradientParameterSampler","text":"<p>Utility helpers for gradient-based parameter identification.</p> <p>These helpers take a trained model together with a collection of reference samples and search for admissible parameter vectors whose model outputs match the known data. Public entrypoints are intentionally thin wrappers around the private optimisation routine so they can be reused in different experiment scripts.</p>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/gradient_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler.GradientParameterSampler.estimate_parameters_tabular","title":"estimate_parameters_tabular  <code>staticmethod</code>","text":"<pre><code>estimate_parameters_tabular(\n    *,\n    ncandidates: int,\n    nepochs: int,\n    dtype: dtype,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str,\n    database_uid: str,\n    parameter_key: str,\n    known_values: dict = {},\n    report_dir: str\n)\n</code></pre> <p>Searches for parameter vectors that reproduce known tabular samples.</p> <p>The routine samples <code>ncandidates</code> random parameter vectors, clips them to the admissible bounds provided by the tabular database, and refines them jointly via <code>_refine_input_optimization</code>. Known input/output pairs are injected through <code>known_values</code>, while <code>parameter_key</code> selects which data block is considered optimisable. Once refined, the best candidates are exported to the requested report directory together with an HTML summary.</p> <p>Parameters:</p> Name Type Description Default <code>ncandidates</code> <code>int</code> <p>Number of parameter configurations to find.</p> required <code>nepochs</code> <code>int</code> <p>Maximal number of epochs in the optimization process.</p> required <code>dtype</code> <code>dtype</code> <p>Datatype of the parameters.</p> required <code>model_uid</code> <code>str</code> <p>Unique ID of the model to use for optimization.</p> required <code>optimizer_uid</code> <code>str</code> <p>Unique ID of the optimizer to be used.</p> required <code>scheduler_uid</code> <code>str</code> <p>Unique ID of the Scheduler to be used.</p> required <code>loss_uid</code> <code>str</code> <p>Unique ID of the loss to be used.</p> required <code>database_uid</code> <code>str</code> <p>Unique ID of the database containing training data.</p> required <code>parameter_key</code> <code>str</code> <p>Tells which part of the database corresponds to the parameters to be optimized.</p> required <code>known_values</code> <code>dict[str, list[float]]</code> <p>User prescribed known values to fill in/augment the search space.</p> <code>{}</code> <code>report_dir</code> <code>str</code> <p>Where to store the HTML report file.</p> required Source code in <code>AI4SurrogateModelling/src/utilization/parameter_estimation/gradient_sampler.py</code> <pre><code>@staticmethod\n@mark_lazy_function\ndef estimate_parameters_tabular(\n    *,\n    ncandidates: int,\n    nepochs: int,\n    dtype: torch.dtype,\n    model_uid: str,\n    optimizer_uid: str,\n    scheduler_uid: str,\n    loss_uid: str,\n    database_uid: str,\n    parameter_key: str,\n    known_values: dict = {},\n    report_dir: str,\n):\n    \"\"\"Searches for parameter vectors that reproduce known tabular samples.\n\n    The routine samples `ncandidates` random parameter vectors, clips them to\n    the admissible bounds provided by the tabular database, and refines them\n    jointly via `_refine_input_optimization`. Known input/output pairs are\n    injected through `known_values`, while `parameter_key` selects which data\n    block is considered optimisable. Once refined, the best candidates are\n    exported to the requested report directory together with an HTML summary.\n\n    Args:\n        ncandidates (int): Number of parameter configurations to find.\n        nepochs (int): Maximal number of epochs in the optimization process.\n        dtype (torch.dtype): Datatype of the parameters.\n        model_uid (str): Unique ID of the model to use for optimization.\n        optimizer_uid (str): Unique ID of the optimizer to be used.\n        scheduler_uid (str): Unique ID of the Scheduler to be used.\n        loss_uid (str): Unique ID of the loss to be used.\n        database_uid (str): Unique ID of the database containing training data.\n        parameter_key (str): Tells which part of the database corresponds to the parameters to be optimized.\n        known_values (dict[str, list[float]]): User prescribed known values to fill in/augment the search space.\n        report_dir (str): Where to store the HTML report file.\n    \"\"\"\n    database = runtime_state.construct_object(\n        object_uid=database_uid,\n    )  # instance of DatabaseTabular\n    database_keys = list(database.get_data_keys())\n    database_dimensions = database.metadata_get_value(\"dimensions\")\n\n    model = runtime_state.construct_object(\n        object_uid=model_uid,\n        reinit=True,\n    )\n\n    admissible_ranges = {\n        k: torch.tensor(v, dtype=dtype)\n        for k, v in database.get_data_ranges().items()\n    }\n\n    # known_value_labels = set(known_values.keys())\n    known_value_labels = sorted(list(known_values.keys()))\n\n    known_data_helper = {}\n    # specified keys need to be part of the database\n    for label in known_value_labels:\n        label_key = database.get_label_key(label)\n        if label_key is None:\n            msg = f\"Column: {label} is not present in the database.\"\n            Logger.warning(msg)\n            raise ValueError(msg)\n\n        label_index = database.get_label_index(label=label, key=label_key)\n        known_data_helper[label] = {\n            \"key\": label_key,\n            \"index\": label_index,\n            \"data\": np.array(known_values[label], dtype=np.float32).reshape(\n                -1, 1\n            ),\n        }\n\n    # normalize the known values\n    for label, helper in known_data_helper.items():\n        data, _ = database.transform_data(\n            data=helper[\"data\"], data_labels=[label]\n        )\n        data = torch.tensor(data, dtype=dtype)\n\n        data_lower = (\n            data\n            &lt; admissible_ranges[known_data_helper[label][\"key\"]][\n                known_data_helper[label][\"index\"], 0\n            ]\n            * 1.005\n        )\n        data_higher = (\n            data\n            &gt; admissible_ranges[known_data_helper[label][\"key\"]][\n                known_data_helper[label][\"index\"], 1\n            ]\n            * 1.005\n        )\n        if any(data_lower) or any(data_higher):\n            msg = f'Supplied values for column \"{label}\" are outside of the admissible range.'\n            Logger.warning(msg)\n            raise ValueError(msg)\n\n        helper[\"data\"] = data\n\n    # check for correctness of the arguments\n    nprescriptions = [len(v) for _, v in known_values.items()]\n    if np.min(nprescriptions) &lt; np.max(nprescriptions):\n        msg = \"Prescribed known values need to have the same number of\"\n        \"entries for each data label.\"\n        Logger.warning(msg)\n        raise ValueError(msg)\n    nevals = nprescriptions[0]\n\n    # construct random data for evaluation and clip them to admisible ranges\n    parameters = torch.nn.Parameter(\n        torch.randn(\n            (ncandidates, database_dimensions[parameter_key]), dtype=dtype\n        )\n    )\n    parameters = mpi.broadcast(parameters, root=0)\n    with torch.no_grad():\n        parameters = torch.clamp(\n            parameters,\n            min=admissible_ranges[parameter_key][:, 0],\n            max=admissible_ranges[parameter_key][:, 1],\n        )\n        parameters.requires_grad = True\n\n    non_parameter_known_data = {}\n    non_parameter_known_mask = {}\n    for key in database_keys:\n        if key != parameter_key:\n            non_parameter_known_data[key] = torch.zeros(\n                (nevals, database_dimensions[key]), dtype=dtype\n            )\n            non_parameter_known_mask[key] = (\n                non_parameter_known_data[key] &lt;= -1\n            )  # all false\n\n    for label, helper in known_data_helper.items():\n        key = helper[\"key\"]\n        index = helper[\"index\"]\n        values = helper[\"data\"]\n        if key != parameter_key:\n            non_parameter_known_data[key][:, index] = values[:, 0]\n            non_parameter_known_mask[key][:, index] = True\n\n    for key in database_keys:\n        if key != parameter_key:\n            if torch.any(non_parameter_known_mask[key] == False):\n                msg = f\"Some of the database values have not been properly set for the optimization process.\"\n                Logger.warning(msg)\n                raise ValueError\n\n    optimizer = runtime_state.construct_object(\n        object_uid=optimizer_uid,\n        reinit=True,\n        override_parameters={\"params\": [parameters]},\n    )\n    scheduler = runtime_state.construct_object(\n        object_uid=scheduler_uid,\n        reinit=True,\n        override_parameters={\"optimizer\": optimizer},\n    )\n    loss = runtime_state.construct_object(object_uid=loss_uid)\n\n    train_manager = TrainingManager(checkpoint_dir=None, reset=False)\n\n    parameters, losses, predictions = (\n        GradientParameterSampler._refine_input_optimization(\n            model=model,\n            train_manager=train_manager,\n            nevals=nevals,\n            parameter_key=parameter_key,\n            parameters=parameters,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            loss=loss,\n            admissible_ranges=admissible_ranges,\n            known_data=non_parameter_known_data,\n            nepochs=nepochs,\n        )\n    )\n    report_data = {\n        \"Parameter - Search\": train_manager.get_progress_data(\n            xkey=\"epoch\"\n        ),  # losses, learning rates\n    }\n    mpi.make_dir(report_dir)\n    create_report(\n        fn=report_dir + \"/\" + \"parameter_search.html\", data=[report_data]\n    )\n\n    # keep only a handful of good candidates\n    sort_indices = np.argsort(losses)\n    data = {}\n    data[parameter_key] = (parameters.detach().numpy())[sort_indices]\n\n    # denormalize parameters\n    data_normal = database.revert_transform_range_external(\n        keys=[parameter_key],\n        data=data,\n    )\n\n    predictions_keys = sorted(list(predictions.keys()))\n    data2export = data_normal[parameter_key]\n    known_tensor = torch.vstack([torch.hstack([torch.tensor(known_values[key]) for key in known_value_labels])  for _ in range(ncandidates)]).cpu().numpy()\n    prediction_tensor = []\n    for key in predictions_keys:\n        P = {key: predictions[key].detach().cpu().numpy()}\n        prediction_normal = database.revert_transform_range_external(\n            keys=[key],\n            data=P,\n        )[key].reshape(ncandidates, -1)\n        prediction_tensor.append(prediction_normal)\n    data2export = np.hstack([data2export, known_tensor] + prediction_tensor)\n\n    labels = database.metadata_get_value(\"labels\")\n    labels2export = labels[parameter_key]\n    for key in known_value_labels:\n        labels2export += [f'{key}{idx + 1}' for idx in range(nevals)]\n    for key in predictions_keys:\n        for idx in range(nevals):\n            for prediction_label in labels[key]:\n                labels2export += [f'Prediction {prediction_label}{idx + 1}']\n\n\n    export_results2csv(\n        data = data2export,\n        labels = labels2export,\n        delimiter=';',\n        fn=report_dir + \"/\" + \"parameters.csv\",\n    )\n    Logger.info(\n        f\"Generated parameter configurations with losses: {[losses[u] for u in sort_indices]}\"\n    )\n</code></pre>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/gradient_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler.export_results2csv","title":"export_results2csv","text":"<pre><code>export_results2csv(\n    *,\n    labels: list[str],\n    data: ndarray,\n    fn: str,\n    delimiter: str\n)\n</code></pre> <p>Takes the user provided data and labeling and exports it to a comma  separated file using the supplied delimitting character.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>Target filename.</p> required <code>delimiter</code> <code>str</code> <p>Delimiting character.</p> required <code>data</code> <code>ndarray</code> <p>Data array.</p> required <code>labels</code> <code>list[str]</code> <p>Labels.</p> required Source code in <code>AI4SurrogateModelling/src/utilization/parameter_estimation/gradient_sampler.py</code> <pre><code>def export_results2csv(\n    *,\n    labels: list[str],\n    data: np.ndarray,\n    fn: str,\n    delimiter: str,\n):\n    \"\"\"Takes the user provided data and labeling and exports it to a comma \n    separated file using the supplied delimitting character.\n\n    Args:\n        fn (str): Target filename.\n        delimiter (str): Delimiting character.\n        data (np.ndarray): Data array.\n        labels (list[str]): Labels.\n    \"\"\"\n\n    try:\n\n        nrows, ncols = data.shape[0], data.shape[1]\n\n        max_label_length = np.max([len(v) for v in labels]) + 11\n        label_formatting = \"{:&gt;\" + f\"{max_label_length}s\" + \"}{}\"\n        value_formatting = \"{:&gt;\" + f\"{max_label_length}.12f\" + \"}{}\"\n\n        if mpi.get_rank() == 0:\n            with open(fn, \"w\") as f:\n                for label in labels:\n                    f.write(\n                        label_formatting.format(label, delimiter)\n                    )\n                f.write(\"\\n\")\n\n                for j in range(nrows):\n                    for i in range(ncols):\n                        f.write(\n                            value_formatting.format(\n                                data[j, i], delimiter\n                            )\n                        )\n                    f.write(\"\\n\")\n\n        mpi.sync()\n        mpi.print(f\"\u2705The CSV file '{fn}' has been written successfully.\")\n    except Exception:\n        mpi.print(f\"\u274cThe writing of CSV file '{fn}' failed.\")\n</code></pre>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/","title":"Module <code>src.utilization.parameter_estimation.random_sampler</code>","text":""},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler","title":"AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler","text":""},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler.RandomParameterSampler","title":"RandomParameterSampler","text":""},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler.RandomParameterSampler.estimate_vertices_random","title":"estimate_vertices_random  <code>staticmethod</code>","text":"<pre><code>estimate_vertices_random(\n    *,\n    ntests: int,\n    dtype: ENUM_Dtypes,\n    model: ModelBase,\n    vertex_coordinates: ndarray,\n    vertex_labels_local2global_map,\n    **kwargs\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/utilization/parameter_estimation/random_sampler.py</code> <pre><code>@staticmethod\ndef estimate_vertices_random(\n    *,\n    ntests: int,\n    dtype: Dtypes,\n    model: Model,\n    vertex_coordinates: np.ndarray,\n    vertex_labels_local2global_map,\n    **kwargs,\n):\n\n    dim_in = unparse_model(model).get_shape_input()\n    dim_out = unparse_model(model).get_shape_output()\n\n    output_reindexing = vertex_labels_local2global_map[\"outputs\"]\n    input_reindexing = vertex_labels_local2global_map[\"inputs\"]\n\n    nvertices = vertex_coordinates.shape[0]\n    vertex_values_inputs = []\n    vertex_values_outputs = []\n    vertex_values_loss = []\n    # try to estimate the parameters for vertices\n    for vertex_index in range(nvertices):\n        u_coordinates = vertex_coordinates[vertex_index]\n\n        x = torch.randn((ntests, dim_in)).to(dtype.value)\n        y = torch.randn((ntests, dim_out)).to(dtype.value)\n        x = 2 * (x - x.min()) / (x.max() - x.min()) - 1\n        y = 2 * (y - y.min()) / (y.max() - y.min()) - 1\n\n        with torch.no_grad():\n            for local_index, global_index in output_reindexing:\n                y[:, global_index] = u_coordinates[local_index]\n            for local_index, global_index in input_reindexing:\n                x[:, global_index] = u_coordinates[local_index]\n\n        with torch.no_grad():\n            prediction = model(x)\n\n            prediction_error = (\n                torch.sum(torch.abs(prediction - y), dim=1).detach().numpy()\n            )\n\n            prediction_ordering = np.argsort(prediction_error)\n            optimal_index = prediction_ordering[0]\n            optimal_loss_local = prediction_error[optimal_index]\n\n            optimal_loss_values = mpi.allgather(optimal_loss_local)\n            optimal_mpi_process = np.argsort(optimal_loss_values)\n            optimal_loss = optimal_loss_values[optimal_mpi_process[0]]\n\n            optimal_x = x[optimal_index].detach().numpy()\n            optimal_y = y[optimal_index].detach().numpy()\n\n            optimal_x = mpi.broadcast(\n                data=optimal_x, root=optimal_mpi_process[0]\n            )\n            optimal_y = mpi.broadcast(\n                data=optimal_y, root=optimal_mpi_process[0]\n            )\n\n            vertex_values_inputs.append(optimal_x)\n            vertex_values_outputs.append(optimal_y)\n            vertex_values_loss.append(optimal_loss)\n    Logger.warning(f\"Losses: {vertex_values_loss}\")\n\n    return vertex_values_inputs, vertex_values_outputs, vertex_values_loss\n</code></pre>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler.RandomParameterSampler.sample_simplices","title":"sample_simplices  <code>staticmethod</code>","text":"<pre><code>sample_simplices(\n    *,\n    ntests: int,\n    dtype: ENUM_Dtypes,\n    model: ModelBase,\n    simplex_coordinates: ndarray,\n    simplex_vertex_labels_local2global_map,\n    nsamples: int\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/utilization/parameter_estimation/random_sampler.py</code> <pre><code>@staticmethod\ndef sample_simplices(\n    *,\n    ntests: int,\n    dtype: Dtypes,\n    model: Model,\n    simplex_coordinates: np.ndarray,\n    simplex_vertex_labels_local2global_map,\n    nsamples: int,\n):\n    if simplex_coordinates.shape[1] &gt; 2:\n        mpi.abort(\n            0,\n            \"Simplex Sampling is implemented only for simplices of degree 2\",\n        )\n\n    nsimplices = simplex_coordinates.shape[0]\n\n    dim_in = unparse_model(model).get_shape_input()\n    dim_out = unparse_model(model).get_shape_output()\n\n    output_reindexing = simplex_vertex_labels_local2global_map[\"outputs\"]\n    input_reindexing = simplex_vertex_labels_local2global_map[\"inputs\"]\n\n    curve_coordinates = []\n    total_len = 0\n    for vertex_index in range(1, nsimplices):\n        u = simplex_coordinates[vertex_index - 1]\n        v = simplex_coordinates[vertex_index]\n        simplex_length = np.linalg.norm(u - v)\n        total_len += simplex_length\n\n    length_fraction = total_len / (nsamples - 1)\n    curve_coordinates.append(simplex_coordinates[0])\n\n    current_length = 0\n    current_vertex_u = 0\n    current_vertex_v = 1\n    previous_line_length_limit = 0\n    current_line_length_limit = np.linalg.norm(\n        simplex_coordinates[current_vertex_u]\n        - simplex_coordinates[current_vertex_v]\n    )\n    current_line_length = current_line_length_limit\n    for _ in range(1, nsamples):\n        current_length += length_fraction\n\n        if current_length &gt; current_line_length_limit:\n            current_vertex_u += 1\n            current_vertex_v += 1\n            previous_line_length_limit = current_line_length_limit\n            current_line_length = np.linalg.norm(\n                simplex_coordinates[current_vertex_u]\n                - simplex_coordinates[current_vertex_v]\n            )\n            current_line_length_limit += current_line_length\n\n        distance_u = current_length - previous_line_length_limit\n        distance_v = current_line_length_limit - current_length\n        alpha_u = distance_v / current_line_length\n        alpha_v = distance_u / current_line_length\n\n        x = (\n            alpha_u * simplex_coordinates[current_vertex_u]\n            + alpha_v * simplex_coordinates[current_vertex_v]\n        )\n        curve_coordinates.append(x)\n\n    collected_inputs = []\n    collected_outputs = []\n    # try to estimate the parameters for vertices\n    for vertex_index in range(len(curve_coordinates)):\n        vertex_coordinates = curve_coordinates[vertex_index]\n\n        x = torch.randn((ntests, dim_in)).to(dtype.value)\n        y = torch.randn((ntests, dim_out)).to(dtype.value)\n        x = 2 * (x - x.min()) / (x.max() - x.min()) - 1\n        y = 2 * (y - y.min()) / (y.max() - y.min()) - 1\n\n        with torch.no_grad():\n            for local_index, global_index in output_reindexing:\n                y[:, global_index] = vertex_coordinates[local_index]\n            for local_index, global_index in input_reindexing:\n                x[:, global_index] = vertex_coordinates[local_index]\n\n        with torch.no_grad():\n            prediction = model(x)\n\n            prediction_error = (\n                torch.sum(torch.abs(prediction - y), dim=1).detach().numpy()\n            )\n\n            prediction_ordering = np.argsort(prediction_error)\n            optimal_index = prediction_ordering[0]\n            optimal_loss_local = prediction_error[optimal_index]\n\n            optimal_loss_values = mpi.allgather(optimal_loss_local)\n            optimal_mpi_process = np.argsort(optimal_loss_values)\n\n            optimal_x = x[optimal_index].detach().numpy()\n            optimal_y = y[optimal_index].detach().numpy()\n\n            optimal_x = mpi.broadcast(\n                data=optimal_x, root=optimal_mpi_process[0]\n            )\n            optimal_y = mpi.broadcast(\n                data=optimal_y, root=optimal_mpi_process[0]\n            )\n\n            collected_inputs.append(optimal_x)\n            collected_outputs.append(optimal_y)\n\n    return np.array(collected_inputs), np.array(collected_outputs)\n</code></pre>"},{"location":"api/API%20Reference/src/utilization/parameter_estimation/random_sampler/#AI4SurrogateModelling.src.utilization.parameter_estimation.random_sampler.RandomParameterSampler.sample_simplices_interpolation","title":"sample_simplices_interpolation  <code>staticmethod</code>","text":"<pre><code>sample_simplices_interpolation(\n    *,\n    ntests: int,\n    dtype: ENUM_Dtypes,\n    model: ModelBase,\n    simplex_coordinates: ndarray,\n    simplex_vertex_labels_local2global_map,\n    nsamples: int\n)\n</code></pre> Source code in <code>AI4SurrogateModelling/src/utilization/parameter_estimation/random_sampler.py</code> <pre><code>@staticmethod\ndef sample_simplices_interpolation(\n    *,\n    ntests: int,\n    dtype: Dtypes,\n    model: Model,\n    simplex_coordinates: np.ndarray,\n    simplex_vertex_labels_local2global_map,\n    nsamples: int,\n):\n    if simplex_coordinates.shape[1] &gt; 2:\n        mpi.abort(\n            0,\n            \"Simplex Sampling is implemented only for simplices of degree 2\",\n        )\n\n    nsimplices = simplex_coordinates.shape[0]\n\n    vertex_values_inputs, vertex_values_outputs, vertex_values_loss = (\n        RandomParameterSampler.estimate_vertices_random(\n            ntests=ntests,\n            dtype=dtype,\n            model=model,\n            vertex_coordinates=simplex_coordinates,\n            vertex_labels_local2global_map=simplex_vertex_labels_local2global_map,\n        )\n    )\n\n    total_len = 0\n    for vertex_index in range(1, nsimplices):\n        u = simplex_coordinates[vertex_index - 1]\n        v = simplex_coordinates[vertex_index]\n        simplex_length = np.linalg.norm(u - v)\n        total_len += simplex_length\n\n    length_fraction = total_len / (nsamples - 1)\n    collected_inputs = [vertex_values_inputs[0]]\n    collected_outputs = [vertex_values_outputs[0]]\n\n    current_length = 0\n    current_vertex_u = 0\n    current_vertex_v = 1\n    previous_line_length_limit = 0\n    current_line_length_limit = np.linalg.norm(\n        simplex_coordinates[current_vertex_u]\n        - simplex_coordinates[current_vertex_v]\n    )\n    current_line_length = current_line_length_limit\n    for _ in range(1, nsamples):\n        current_length += length_fraction\n\n        if current_length &gt; current_line_length_limit:\n            current_vertex_u += 1\n            current_vertex_v += 1\n            previous_line_length_limit = current_line_length_limit\n            current_line_length = np.linalg.norm(\n                simplex_coordinates[current_vertex_u]\n                - simplex_coordinates[current_vertex_v]\n            )\n            current_line_length_limit += current_line_length\n\n        distance_u = current_length - previous_line_length_limit\n        distance_v = current_line_length_limit - current_length\n        alpha_u = distance_v / current_line_length\n        alpha_v = distance_u / current_line_length\n\n        x = (\n            alpha_u * vertex_values_inputs[current_vertex_u]\n            + alpha_v * vertex_values_inputs[current_vertex_v]\n        )\n        y = (\n            alpha_u * vertex_values_outputs[current_vertex_u]\n            + alpha_v * vertex_values_outputs[current_vertex_v]\n        )\n        collected_inputs.append(x)\n        collected_outputs.append(y)\n\n    return np.array(collected_inputs), np.array(collected_outputs)\n</code></pre>"},{"location":"developer_docs/architecture.hidden/","title":"Structure of the Software","text":"<p>This software consists of three main components.</p> <ul> <li>Data management and analysis toolbox.</li> <li>Neural networks assembly and training toolbox.</li> <li>Neural network utilization toolbox.</li> </ul>"},{"location":"developer_docs/architecture.hidden/#data-toolbox","title":"Data toolbox","text":"<p>This part of the software aims to represent training data efficiently for parallel I/O, analysis, utilization and reporting. </p> <p>In order for database to be created, an importer needs to be used. Importers are tailored converters which take an existing data format (like CSV, PNG, ...) and trasnform it to the internal database format. Each importer has its specific attributes which need to verbosely documented so they can be used by the users (see an example in the user's documentation here).</p> <p>The internal database uses LMBD under the hood for fast and parallel file access and to limit then number of files on disk.</p> <p>Currently, labeled tabular databases are supported with the following principles in mind.</p>"},{"location":"developer_docs/architecture.hidden/#labeled-database","title":"Labeled Database","text":"<p>Serves as overarching class </p>"},{"location":"developer_docs/index_developer/","title":"Notes for Developers","text":""},{"location":"developer_docs/index_developer/#setup-your-local-machine","title":"Setup your local machine","text":"<p>We assume you have a local copy of the git repository obtained as described in the Installation guide</p>"},{"location":"developer_docs/index_developer/#git-guidelines","title":"Git Guidelines","text":"<p>We will aim to follow the approach to git development described nicely in this blog post.</p> <ul> <li> <p>Intialize the Git flow wrapper via the <code>git flow init</code> command.</p> <ul> <li>use <code>master</code> branch for production release.</li> <li>use <code>development</code> as a future release branch.</li> <li>use <code>feature/</code> as Feature branches prefix.</li> <li>use <code>release/</code> as Release branches prefix.</li> <li>use <code>hotfix/</code> as Hotfix branches prefix.</li> <li>use <code>support/</code> as Support branches prefix.</li> <li>use <code>bugfix/</code> as Bugfix branches prefix.</li> <li>use nothing for version prefix.</li> </ul> </li> </ul> <p>Afterwards, try to follow the suggestions below, in case of any issues or suggestions, contact Michal Krav\u010denko.</p>"},{"location":"developer_docs/index_developer/#general-structure-of-the-repo","title":"General Structure of the repo","text":"<ul> <li>The production code is in the branch <code>master</code>, this branch contains the latest version of whatever code is available to the users.</li> <li>The development is contained in the branch <code>development</code>.</li> </ul>"},{"location":"developer_docs/index_developer/#feature-development","title":"Feature Development","text":"<p>Feature branches are designed for development of new functionalities.</p> <ul> <li>When the developer wants to work on a feature titled <code>feature_x</code>, they should perform the following commands:<ul> <li><code>git flow feature start feature_x</code> (initializes the appropriate local branch and switches the developer to it).</li> <li>Then continue developing the feature, add changes via the <code>git add ...</code> command, commit the changes via the <code>git commit -m 'commit message'</code> command. The commit message should contain a brief description of the changes being commited and, if available, link to an issue in the repository via the <code>#N</code> command (where <code>N</code> is the number of the issue).</li> <li>Call <code>git flow feature publish</code> to push your feature to the remote repository.</li> <li>Once the feature is ready to be finished and closed, call the finishing script via <code>bash git_finish_feature.sh</code>. It is not a foolproof script which does the following:</li> <li>detects if the user is in the correct branch family: <code>feature/*</code></li> <li>if so, it pulls the up-to-date version of the branch and commits local changes</li> <li>then it increments the minor version by one and sets the patch version to zero</li> <li>then it commits the versioning changes and closes the feature by calling <code>git flow feature finish</code></li> <li>updates the remote development branch</li> </ul> </li> </ul>"},{"location":"developer_docs/index_developer/#hotfix-development","title":"Hotfix Development","text":"<p>Hotfixes are urgent bug fixes, the bugs should be pushed to the production branch and development branches as soon as possible.</p> <ul> <li>When working on a hotfix <code>hotfix_x</code>, they should perform the following commands:<ul> <li><code>git flow hotfix start hotfix_x</code> (initializes the appropriate local branch asn switches the developer to it).</li> <li>Then continue developing the hotfix, add changes via the <code>git add ...</code> command, commit the changes via the <code>git commit -m 'commit message'</code> command.</li> <li>Once the hotfix is ready to be finished and closed, call the finishing script via <code>bash git_finish_hotfix.sh</code>. It is not a foolproof script which does the following:</li> <li>detects if the user is in the correct branch family: <code>hotfix/*</code> or <code>development</code></li> <li>if so, it pulls the up-to-date version of the branch and commits local changes</li> <li>then it increments the patch version by one</li> <li>then it commits the versioning changes and closes the feature by calling either <code>git flow hotfix finish</code> or <code>git push</code> (if on development branch)</li> <li>updates the remote development branch</li> </ul> </li> </ul>"},{"location":"developer_docs/index_developer/#making-a-new-release","title":"Making a new Release","text":"<p>Release branch is designed for preparation of a new production release. Hotfixes are urgent bug fixes, the bugs should be pushed to the production branch and development branches as soon as possible.</p> <p>We supply a single sincript <code>git_publish_release.sh</code> which publishes a new release online. It performs the following steps:</p> <ul> <li>Ensures that you are on the <code>development</code> branch</li> <li>commits local changes and pulls the remote <code>development</code> branch</li> <li>initializes the new release branch via <code>git flow release start $VERSION</code>, where <code>$VERSION</code> is pulled from the source code</li> <li>finishes the new release branch via <code>git flow release finish $VERSION</code></li> <li>pushes the <code>master</code> and <code>development</code> branches to remote repository.</li> <li>pushes the new version tag to the remote repository</li> </ul>"},{"location":"developer_docs/index_developer/#code-quality","title":"Code Quality","text":"<p>Developers can call <code>./install/code_check.sh</code> script to automatically improve and analyze the quality of the produced code.</p>"},{"location":"installation_docs/index_installation/","title":"Downloading the repository","text":"<p>In order to download the git repository, follow these steps:</p> <ul> <li>If not already installed, install the Git versioning tool via <code>apt install -y git-all</code>.</li> <li>Choose an appropriate directory for the repository and navigate to it via the <code>cd</code> command.</li> <li>Download the git repository to your local storage via <code>git clone https://code.it4i.cz/kra568/ai-surrogate-modelling.git</code>.</li> <li>Navigate to the newly created directory via <code>cd ai-surrogate-modelling</code>.</li> <li>Setup your environment by calling the installation script via <code>sudo ./install/install.sh</code></li> <li>To update the Python packages required by the software in the virtual Python environment, call <code>./install/install_python.sh</code> (this script is also called from the command above)</li> </ul> <p>Now you have access to the source codes and can either start developing or install the package via pip.</p>"},{"location":"installation_docs/index_installation/#wheel-installation","title":"Wheel Installation","text":"<p>You have downloaded the git repository, but do not wish to work on development. For this reason, we include a simple installation script.</p> <p>Navigate to the project directory and run the command <code>sudo bash install/install_wheel.sh</code>. This installs a Python virtual environment <code>python_venv/.ai-for-surrogate-modelling</code> in your home directory, constructs the installation files and installs this software to the newly created virtual environment via pip.</p> <p>Once installed, you can start experimenting with the software.</p>"},{"location":"installation_docs/index_installation/#uninstall-the-wheel","title":"Uninstall the Wheel","text":"<p>To uninstall this software from the virtual environment, run the following series of commands from the terminal:</p> <pre><code>. $ai_for_surrogate_modelling_path/bin/activate\npip uninstall -y SHAIPER-AI-for-Surrogate-Modelling\n</code></pre>"},{"location":"user_docs/configuration_main/","title":"Main Configuration","text":"<p>The software uses a system of configuration files controlling the flow of the program. The aim is to lessen the hardships imposed by efficient parallel implementations, sofisticated reporting, database management, consistent training procedures and parallel I/O. Below we describe the basic structure of the configuration files. See Examples for more concrete tutorials.</p>"},{"location":"user_docs/configuration_main/#main-configuration-file","title":"Main Configuration File","text":"<p>So, you have run the software with the attribute <code>--config path/to/main/config.yml</code>, which points to the main configuration file. The main configuration YAML file consists of three components.</p>"},{"location":"user_docs/configuration_main/#main","title":"<code>main</code>","text":"<p>The top-level behaviour of the software is described inside the main section. <pre><code>main:\n    logging: info/debug                     # specifies the loggin level\n    logging_directory: path/to/logs/storage # where should the logs be stored\n    parallelization: distributed/isolated   # should all processes train a single model, or should each process do its own thing?\n    seed: int                               # sets all random seeds to this value, for reproducibility purposes\n</code></pre></p>"},{"location":"user_docs/configuration_main/#aliases","title":"<code>aliases</code>","text":"<p>Writing configuration files may contain very long and/or repeating values. To ease the writing of the configurations, the user can specify so-called aliases in the form of <code>alias_key: alias_value</code>. This tells the configuration parser, that all values inside the configuration files in the form <code>$alias_key$</code> should be replaced by the corresponding value <code>alias_value</code>.</p> <p>Aliases can be recursively defined, i.e. one alias can use other alias in its value. Aliases cannot use aliases in their keys. The software checks if the aliases are defined correctly, i.e. that all aliases are replaced by an alias value and that nested aliases do not form a cyclic alias definition. <pre><code>aliases:\n    alias_key_0: alias_value_0\n    alias_key_1: alias_value_1\n    alias_key_2: alias_value_2-$alias_key_1$\n</code></pre></p>"},{"location":"user_docs/configuration_main/#operations","title":"<code>operations</code>","text":"<p>The last component of the main configuration file contains the operations to be performed by the program. Each operation has its unique numerical suffix which specifies the order of operations. The configuration parser checks the correctness of the operational IDs.</p> <p><pre><code>operations:\n    operation_0: path/to/config_0.yml\n    operation_25: path/to/config_25.yml\n    operation_15: path/to/config_15.yml\n    operation_10: path/to/config_10.yml\n</code></pre> As you can see, the numerical suffices need not to form a sequence and do not need to be ordered in the configuration file. The existence of the files the operation points to is verified before any computation takes place.</p>"},{"location":"user_docs/configuration_operational/","title":"Operational Configuration","text":"<p>The main configuration file describes the top-level configuration, which contains operations linking to the operational configuration files.</p> <p>Each operational configuration file contains two basic sections: <code>objects</code> and <code>actions</code>.</p>"},{"location":"user_docs/configuration_operational/#objects","title":"<code>objects</code>","text":"<p>The object section contains the specification of objects used during the program for objects/actions specified in this operational file and all successive operational files.</p> <pre><code>objects:\n    object_UID_0:\n        constructor: path.to.class.definition_0\n        parameters: \n            parameter_name_0: parameter_value_0\n            parameter_name_1: parameter_value_1\n                        ...\n\n    object_UID_1:\n        constructor: path.to.class.definition_1\n        parameters: \n            parameter_name_0: parameter_value_0\n            parameter_name_1: parameter_value_1\n                        ...\n</code></pre> <p>This allows the user to specify any objects contained in the software. In the snippet above, the user specified two objects, each with a unique ID (the configuration parser checks the uniqueness requirement). Each objects needs to point to a method or a class in the code-base which returns the instance of the object initialized with the supplied parameters.</p> <p>So, the user can use any objects, but needs to know how to construct them. The objects can be referenced by their unique IDs in the configuration files. For example, an action can be done using an object, or an object can use another object in its constructor, etc.</p> <p>Object UIDs, paths to constructors, parameter names and values can use aliases specified in the main configuration file.</p>"},{"location":"user_docs/configuration_operational/#actions","title":"<code>actions</code>","text":"<p>Constructing objects and not using them is not very productive. Thus, the final building block of our configuration system is the system of <code>actions</code>. An action can reference an instance of an object, a static function or a global function. </p> <p>In the snippet below, you can see <code>action_0</code> referencing an instance of the object specified above with UID <code>object_UID_0</code> and calling its member function <code>class_function_name</code> with the supplied parameters (if any).</p> <p>The action <code>action_42</code> references either a static function or a global function with the supplied parameters.</p> <p>The system performs the actions in order with respect the the integer suffix. A check is performed whether the referenced functions are available in the code-base and whether the objects have been specified in the configuration files (i.e. the program knows how to construct them).</p> <pre><code>actions:\n  action_0:\n    object_uid: object_UID_0\n    fname: class_function_name\n    parameters: \n        parameter_name_0: parameter_value_0\n        parameter_name_1: parameter_value_1\n                    ...\n\n  action_42:\n    fname: path.to.function.call\n    parameters: \n        parameter_name_0: parameter_value_0\n        parameter_name_1: parameter_value_1\n                    ...\n</code></pre>"},{"location":"user_docs/index_user/","title":"Overview","text":"<p>This software has a single entry point, which parses the provided configuration file and performs the operations specified therein.</p> <p>If you are using the installed package vie pip, the following commands will initiate the program. <pre><code>. $ai_for_surrogate_modelling_path/bin/activate\nmpirun -n N ai4sm --config path/to/main/config.yml\n</code></pre></p> <p>or alternatively, when you are working directly in the code-base (e.g. developing the software): <pre><code>. $ai_for_surrogate_modelling_path/bin/activate\nmpirun -n N python -m AI4SurrogateModelling.entry_point --config path/to/main/config.yml\n</code></pre></p> <p>where <code>N</code> denotes the desired number of mpi processes and <code>path/to/main/config.yml</code> denotes the main configuration file.</p>"},{"location":"user_docs/examples/example_tabular/","title":"Example Tabular (Random)","text":"<p>Here you can find an overview of a complete example in which you learn:</p> <pre><code>- How to define random Tabular datasets with various data subgroups\n- How to process the dataset and export its various statistics\n- How to define a basic loss function\n- How to define optimizers\n- How to define schedulers\n- How to define and train a model\n- How to use a trained model in search of optimal parameter configurations\n</code></pre> <p>The configuration scripts for this example can located at <code>conf/examples/random</code>.</p> <p>To run this example, use the commands depicted in the Overview.</p>"},{"location":"user_docs/examples/example_tabular/#the-main-configuration-file","title":"The Main configuration file","text":"<p>(<code>conf/examples/random/run_config.yml</code>) This file defines a lot of aliases and the complete workflow in terms of the operational configuration files. The system of aliases was chosen to simplify the definition of the model size, size of the dataset and datatype of the datasets and models being used.</p> <pre><code>main:\n  logging: info # possible values: info/debug\n  logging_directory: logs/examples/random # default value: logs\n  parallelization: distributed # possible values: distributed/isolated\n  seed: 42\n\naliases:\n  project_path: AI4SurrogateModelling.src\n  dtype: float32\n\n  input_key_known: inputs_group_known\n  input_key_unknown: inputs_group_unknown\n  output_key: outputs_group\n\n  input_dim: 7\n  output_dim: 4\n  nrows: 10000\n\n  ConfigDirectory: conf/examples/random\n\n  DatabaseTabular: $project_path$.database.labeled.tabular.database_tabular.DatabaseTabular\n  Importer: $project_path$.importer.tabular.database_importer_tabular_random.ImporterTabularRandom\n\n  dim_layer_1: 256\n  dim_layer_2: 128\n  dim_layer_3: 64\n  dropout_layer_1: 0.25\n  dropout_layer_2: 0.25\n  dropout_layer_3: 0.25\n\n  Example_ID: random_example\n  DatabaseDirectory: DATABASE_EXAMPLES/$Example_ID$\n  results_dir: results/random_example\n\noperations:\n  operation_0: $ConfigDirectory$/data_create.yml\n  operation_1: $ConfigDirectory$/data_export.yml\n  operation_10: $ConfigDirectory$/define_models.yml\n  operation_20: $ConfigDirectory$/define_optimizers.yml\n  operation_30: $ConfigDirectory$/define_schedulers.yml\n  operation_40: $ConfigDirectory$/define_losses.yml\n  operation_51: $ConfigDirectory$/model_refine.yml\n  operation_60: $ConfigDirectory$/find_parameters.yml\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-create-database","title":"Operation - Create Database","text":"<p>(<code>conf/examples/random/data_create.yml</code>) The first operation performed is to define the dataset. For this we need to define two objects, the database itself and at least one importer. In our case, we use a tabular database and a random tabular importer (i.e. it fills the database with random data)</p> <p>The database object has UID <code>database</code> and the importer object has UID <code>random_importer</code> for future reference in other operational configuration files. The random importer is tasked to define tabular data with 11 columns, 7 of wchich are inputs and 4 of which are the outputs. The 7 inputs are further divided into two groups. Group <code>$input_key_known$</code> represents those inputs, for which we prescribe known values in the utilization step. Group <code>$input_key_unknown$</code> represents a subset of inputs the values of which we are tasked to find during the utilization step.</p> <p>After the object definition, several actions are defined.</p> <ul> <li><code>action_0</code> clears the whole database by calling a member function <code>clear_database()</code> of the <code>TabularDatabase</code> class.</li> <li><code>action 1</code> uses another member function of the TabularDatabase <code>add_data()</code>, which uses the supplied importers to fill the database.</li> <li><code>action_2</code> takes the data in the database and transforms each column in each data group to fit into the specified range.</li> <li><code>action_4</code> removes all columns which do not contain enough information, e.g. columns which are highly correlated to ther columns, or columns which do not contain enough variance in the data.</li> <li><code>action_5</code> defines a split of the database into three groups, <code>train</code> with 80% of rows, <code>test</code> with 20% of rows and <code>validation</code> with 0% of rows.</li> </ul> <p>In this example, the database is stored in the directory <code>DATABASE_EXAMPLES/random_example</code> is created.</p> <pre><code>objects:\n  database:\n    constructor: $DatabaseTabular$\n    parameters: \n      path_tgt: $DatabaseDirectory$\n\n  random_importer:\n    constructor: $Importer$\n    parameters:\n      nentries: $nrows$\n      data_column_groups:\n        $input_key_known$: [0, 1]\n        $input_key_unknown$: [2, 3, 5, 7, 9]\n        $output_key$: [4, 6, 8, 10]\n\nactions:\n  action_0:\n    object_uid: database\n    fname: clear_database\n\n  action_1:\n    object_uid: database\n    fname: add_data\n    parameters:\n      importers: [\n        random_importer,\n      ]\n\n  action_2:\n    object_uid: database\n    fname: transform_range\n    parameters:\n      keys: [$input_key_known$, $input_key_unknown$, $output_key$]\n      target_range: (-1, 1)\n\n  action_4:\n    object_uid: database\n    fname: cleanup_columns\n    parameters:\n      keys_to_consider: [$input_key_known$, $input_key_unknown$, $output_key$]\n      correlation_eps: 1e-6\n      variance_eps: 1e-6\n\n  action_5:\n    object_uid: database\n    fname: train_test_split\n    parameters:\n      train_ratio: 0.8\n      test_ratio: 0.2\n      validation_ratio: 0.0\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-database-export","title":"Operation - Database Export","text":"<p>Each database can be analyzed and results presented. One of the methods is to generate a standalone HTML file containing predefined information. </p> <p>This operation file defines one object <code>database</code>, located at the path identical to the <code>database</code> in the previous configuration file.</p> <p>There is only one action to be performed, to call the method <code>export_to_html</code> located deep in the source code hierarchy, which takes the supplied tabular database and exports various statistics to the specified file.</p> <p>In this example, a file at <code>results/random_example/reports/database.html</code> is created.</p> <pre><code>objects:\n  database:\n    constructor: $DatabaseTabular$\n    parameters: \n      path_tgt: $DatabaseDirectory$\n\nactions:\n  action_20:\n    fname: $project_path$.export.reports.tabular.ReporterTabular.export_to_html\n    parameters:\n      database: database\n      tgt_fn: $results_dir$/reports/database.html\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-model-definition","title":"Operation - Model Definition","text":"<p>This operational file showcases the possibilities of creating nested object dependencies and arbitrary dependent structures. This file defines several pytorch models, all of them combining into a single <code>final_model</code> to be trained and used utilization.</p> <pre><code>objects:\n\n  block_model_MLP_01: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_1$, $dim_layer_1$]\n      layer_activations: [['layer_norm_$dim_layer_1$', 'dropout_$dropout_layer_1$'], ['gelu']]\n      layer_biases: [False, True]\n      name: \"The first hidden MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$input_key_known$]\n\n  block_model_MLP_02: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_2$, $dim_layer_2$]\n      layer_activations: [['layer_norm_$dim_layer_2$', 'dropout_$dropout_layer_2$'], ['gelu']]\n      layer_biases: [False, True]\n      name: \"The second hidden MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$input_key_known$]\n\n  block_model_MLP_03: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_3$, $dim_layer_3$]\n      layer_activations: [['layer_norm_$dim_layer_3$', 'dropout_$dropout_layer_3$'], ['gelu']]\n      layer_biases: [False, True]\n      name: \"The third hidden MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$input_key_known$]\n\n  block_model_MLP_input: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$input_dim$, $dim_layer_1$]\n      layer_activations: [[None], ['gelu']]\n      layer_biases: [False, True]\n      name: \"An input MLP block\"\n      input_keys: [$input_key_known$, $input_key_unknown$]\n      output_keys: [$input_key_known$]\n\n  transfer_layer_00: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_1$, $dim_layer_2$]\n      layer_activations: [[None], ['gelu']]\n      layer_biases: [False, True]\n      name: \"A transfer MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$input_key_known$]\n\n  transfer_layer_01: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_2$, $dim_layer_3$]\n      layer_activations: [[None], ['gelu']]\n      layer_biases: [False, True]\n      name: \"A transfer MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$input_key_known$]\n\n  transfer_layer_output: \n    constructor: $project_path$.model.models.other.mlp.MultiLayerPerceptron\n    parameters:\n      dtype: $dtype$\n      layer_vertices: [$dim_layer_3$, $output_dim$]\n      layer_activations: [[None], ['tanh']]\n      layer_biases: [False, True]\n      name: \"A final output MLP block\"\n      input_keys: [$input_key_known$]\n      output_keys: [$output_key$]\n\n  residual_layer_00:\n    constructor: $project_path$.model.models.layers.residual.ResidualLayer\n    parameters:\n      dtype: $dtype$\n      model: block_model_MLP_01\n\n  residual_layer_01:\n    constructor: $project_path$.model.models.layers.residual.ResidualLayer\n    parameters:\n      dtype: $dtype$\n      model: block_model_MLP_02\n\n  residual_layer_02:\n    constructor: $project_path$.model.models.layers.residual.ResidualLayer\n    parameters:\n      dtype: $dtype$\n      model: block_model_MLP_03\n\n  final_model:\n    constructor: $project_path$.model.models.other.sequential.Sequential\n    parameters:\n      layers: [\n        block_model_MLP_input,\n        residual_layer_00,\n        transfer_layer_00,\n        residual_layer_01,\n        transfer_layer_01,\n        residual_layer_02,\n        transfer_layer_output\n      ]\n      name: \"A composite model for the vodik voltage prediction task\"\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-optimizer-definition","title":"Operation - Optimizer Definition","text":"<p>This file contains a definition of three built-in optimizers with some of its most relevant parameters exposed. Users can reference any built in optimizers or custom-built ones.</p> <pre><code>objects:\n  optimizer_adam:\n    constructor: torch.optim.Adam\n    parameters:\n      lr: 1e-3\n      betas: (0.8, 0.99)\n      eps: 1e-06\n      weight_decay: 0\n      amsgrad: False\n\n  optimizer_adamW:\n    constructor: torch.optim.AdamW\n    parameters:\n      lr: 1e-3\n      weight_decay: 1e-5\n\n  optimizer_SGD:\n    constructor: torch.optim.SGD\n    parameters:\n      lr: 1e-3\n      momentum: 0\n      dampening: 0\n      weight_decay: 0\n      nesterov: False\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-scheduler-definition","title":"Operation - Scheduler Definition","text":"<p>This file contains a definition of one learning rate scheduler, namely the <code>LambdaLR</code> scheduler. User can reference any built in and applicable callable object to be used as the lambda function.</p> <p>We offer one lambda function example located at <code>AI4SurrogateModelling.src.training.schedulers.lambdas.cyclical_lr.CyclicalLR</code> which serves as a cyclical learning rate manipulator.</p> <pre><code>objects:\n  lambda_scheduler_function_0:\n    constructor: $project_path$.training.schedulers.lambdas.cyclical_lr.CyclicalLR\n    parameters:\n      log_factor_low: -1\n      log_factor_high: 1\n      periods: [500, 1000, 2000]\n\n  scheduler_LAMBDA_0:\n    constructor: torch.optim.lr_scheduler.LambdaLR\n    parameters:\n      lr_lambda: lambda_scheduler_function_0\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-loss-definition","title":"Operation - Loss Definition","text":"<p>This file contains an example of a very basic loss object used in the training procedure. The loss object has UID <code>loss_0</code> and the object constructor is located at <code>AI4SurrogateModelling.src.training.loss.loss_base.LossBase</code>. The constructor parameters allow the user to define losses based on the </p> <ul> <li>weights &amp; biases of the model</li> <li>difference in data calculated by the model specified by a nested dictionary. In this example, the loss is calculated between <code>real['outputs_group']</code> and <code>predicted['outputs_group']</code></li> <li>partial derivatives of data calculated by the model with respect to other data, together with the prescription on the intervals in which the partial derivatives should lie.</li> </ul> <p>The loss function keeps track of all individual losses with coefficients &gt; 0 and provides a single criterion value to be used by a hyperparameter optimization procedure.</p> <pre><code>objects:\n  loss_0:\n    constructor: $project_path$.training.loss.loss_base.LossBase\n    parameters:\n      model_parameters:\n        mae: 0\n        mse: 0\n        rmae: 0\n        rmse: 0\n\n      outputs:\n        # a key contained in the data provided by the model\n        $output_key$: \n          # a key contained in the data provided by the dataloader\n          $output_key$:\n            mae: 0\n            mse: 1\n            rmae: 0\n            rmse: 0\n\n      # derivatives: \n      #   # a key contained in the data provided by the model\n      #   $output_key$:\n      #     # a key contained in the data provided by the dataloader\n      #     $input_key_known$: \n      #       # partial derivative of output with index 0 w.r.t. input with index 0\n      #       'out[0]/in[0]': \n      #         constraints: ['value &gt;= 0', 'value &lt;= 1'] # we want to enforce two constraints on the partial derivative\n      #         mae: 0 # MAE loss component coefficient\n      #         mse: 0 # MSE loss component coefficient\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-refine-model","title":"Operation - Refine Model","text":"<p>This file initializes the database to be used during training and performs a single action located at <code>AI4SurrogateModelling.src.training.trainers.feed_forward.refine</code>. This action initializes a model (if it does not exist) and trains it for the specified number of epochs using the supplied scheduler, optimizer and loss function.</p> <p>The training state and progress are stored inside the directory <code>results/random_example/training</code>. The standalone HTML report visualizing the training progress and best model properties is generated after the training finishes and is located in <code>results/random_example/reports/training.html</code></p> <pre><code>objects:\n  database:\n    constructor: $DatabaseTabular$\n    parameters: \n      path_tgt: $DatabaseDirectory$\n\nactions:\n  action_0:\n    # training function to be used\n    fname: $project_path$.training.trainers.feed_forward.refine\n\n    parameters:\n      checkpoint_dir: $results_dir$/training\n      number_of_epochs: 10\n      batch_size: 1024\n      dtype: $dtype$\n\n      data_uid: database\n      model_uid: final_model\n      optimizer_uid: optimizer_adam\n      scheduler_uid: scheduler_LAMBDA_0\n      reset: True\n      report_dir: $results_dir$/reports\n\n      loss_uid: loss_0\n</code></pre>"},{"location":"user_docs/examples/example_tabular/#operation-model-utilization","title":"Operation - Model Utilization","text":"<p>Finally, we take the best model obtained via the refinement process with UID <code>loaded_model</code>, the underlying database with UID <code>database</code>, a gradient optimizer, scheduler and loss and perform a single action: gradient based parameter optimization located at <code>AI4SurrogateModelling.src.utilization.parameter_estimation.gradient_sampler.GradientParameterSampler.estimate_parameters_tabular</code>.</p> <p>The optimization runs for the specified number of epochs and attempts to find the specified number of parameter configurations. The Parameters are kept inside the known value boundaries (inferred from the supplied database). The search space is restricted by a known set of input/output values. In this example, the inputs from the group <code>inputs_group_known</code> are to be found by the optimization, the inputs from group <code>inputs_group_unknown</code> and outputs <code>outputs_group</code> need to be specified by the user. </p> <p>The optimization attempts to find a <code>P</code> such that for all prescribed input values <code>X</code> and prescribed output values <code>Y</code>, the equality <code>model([P, X]) = Y</code> holds.</p> <pre><code>objects:\n  loaded_model:\n    constructor: $project_path$.model.model_base.ModelBase.load_model\n    parameters: \n      fn: '$results_dir$/training/model_best.bin'\n\n  database:\n    constructor: $DatabaseTabular$\n    parameters: \n      path_tgt: $DatabaseDirectory$\n\n  optimizer_adam_inverse:\n    constructor: torch.optim.Adam\n    parameters:\n      lr: 1e-3\n      betas: (0.8, 0.99)\n      eps: 1e-12\n      weight_decay: 0\n      amsgrad: False\n\n  lambda_scheduler_function_inverse:\n    constructor: $project_path$.training.schedulers.lambdas.cyclical_lr.CyclicalLR\n    parameters:\n      log_factor_low: -1\n      log_factor_high: 1\n      periods: [500, 1000, 2000]\n\n  scheduler_LAMBDA_inverse:\n    constructor: torch.optim.lr_scheduler.LambdaLR\n    parameters:\n      lr_lambda: lambda_scheduler_function_inverse\n\n  loss_inverse:\n    constructor: $project_path$.training.loss.loss_base.LossBase\n    parameters:\n      outputs:\n        $output_key$: \n          $output_key$:\n            mae: 0\n            mse: 1\n            rmae: 0\n            rmse: 0\n\nactions:\n  action_0:\n    fname: $project_path$.utilization.parameter_estimation.gradient_sampler.GradientParameterSampler.estimate_parameters_tabular\n\n    parameters:\n        ncandidates: 50\n        nepochs: 100\n        dtype: $dtype$\n\n        model_uid: loaded_model\n        optimizer_uid: optimizer_adam_inverse\n        scheduler_uid: scheduler_LAMBDA_inverse\n        loss_uid: loss_inverse\n\n        parameter_key: $input_key_unknown$ # assume the data is split so that one group contains the values to be optimized\n        database_uid: database\n\n        # some values may be known\n        known_values:\n            Column_000: [0.025, 0.025, 0.015]\n            Column_001: [0.005, 0.015, 0.026]\n            Column_004: [0.045, 0.039, 0.029]\n            Column_006: [0.045, 0.039, 0.029]\n            Column_008: [0.045, 0.039, 0.029]\n            Column_010: [0.045, 0.039, 0.029]\n        report_dir: '$results_dir$/reports'\n</code></pre>"},{"location":"user_docs/examples/example_tabular_2.hidden/","title":"Example Tabular","text":""}]}